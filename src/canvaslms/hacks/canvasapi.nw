\chapter{Hackish improvements to the \texttt{canvasapi} module}

In this module we provide some decorators for the classes in the
\texttt{canvasapi} package.
We automatically apply all decorators upon import, so
\begin{minted}{python}
import canvaslms.hacks.canvasapi
\end{minted}
would apply all decorators defined herein to the already defined classes in the
\texttt{canvasapi} package.

We do this as follows:
<<canvasapi.py>>=
"""A module that modifies the classes of the canvasapi package"""

import cachetools
import canvasapi.assignment
from canvasapi.user import User
from datetime import datetime, timedelta
import functools
import importlib
import inspect
import logging
import time
import sys

logger = logging.getLogger(__name__)

<<constants>>
<<functions>>

# Loads all hacks
this_module = sys.modules[__name__]

# automatically execute all make_* functions to apply decorators
for name, function in inspect.getmembers(this_module, inspect.isfunction):
  if name.startswith("make_"):
    function()
@

\section{Testing Canvas API monkey-patches}
\label{sec:hacks:testing}

This module includes comprehensive tests for all monkey-patches applied to [[canvasapi]] classes.
Following literate programming principles, tests are distributed throughout this file, appearing immediately after the functionality they verify.
This pedagogical organization shows: explain problem → implement solution → verify it works.

\subsection{Test organization}

The main test file structure is defined here, then individual test chunks appear after their corresponding implementations:
\begin{description}
\item[Equality and hashability tests] Appear after [[make_classes_comparable()]] (Section~\ref{ComparableObjects})
\item[User string representation tests] Appear after [[make_useful_user_dunder_str()]]
\item[Caching tests] Will appear after caching infrastructure (future sections)
\end{description}

\subsection{The problem: Canvas objects lack equality and hashability}

By default, the [[canvasapi]] library's classes don't define [[__eq__]] or [[__hash__]] methods.
This creates practical problems:
\begin{enumerate}
\item Two [[User]] objects representing the same Canvas user (same ID) are not equal
\item Canvas objects cannot be added to sets
\item Canvas objects cannot be used as dictionary keys
\item The [[User.__str__]] method uses Canvas ID instead of login ID
\end{enumerate}

Our monkey-patches fix these issues by adding proper equality, hashing, and string representation.

\subsection{Variation theory in testing}

When testing monkey-patches, we verify:
\begin{description}
\item[Invariant] The patch is applied once, affects all instances of the class
\item[Variant] Different classes (User, Assignment, Submission), different IDs, different instances
\end{description}

The tests demonstrate \emph{semantic equality}: two objects with the same Canvas ID represent the same entity, even if they are different Python objects.

\subsection{Test file structure}

The main test file imports all necessary modules and references test chunks defined throughout this document:

<<test hacks.py>>=
"""
Tests for canvaslms.hacks.canvasapi monkey-patches.

These tests verify that our decorators correctly add __eq__, __hash__,
and improved __str__ methods to canvasapi classes.

IMPORTANT: These tests import canvaslms.hacks.canvasapi, which applies
monkey-patches to canvasapi classes. The patches are applied at import
time and affect all future instances.
"""
import pytest
from unittest.mock import MagicMock
from datetime import datetime, timedelta

# This import applies the monkey-patches
import canvaslms.hacks.canvasapi

# Import everything for testing
from canvaslms.hacks.canvasapi import *

<<test functions>>
@

\subsection{Why monkey-patching is necessary}

Monkey-patching is the right approach here because:
\begin{enumerate}
\item The [[canvasapi]] library returns its own class instances
\item We don't control object creation
\item Monkey-patching affects all instances automatically
\item No wrapper conversion needed throughout the codebase
\end{enumerate}

Our tests verify the patches:
\begin{itemize}
\item Apply correctly at import time
\item Work for all specified classes (User, Assignment, Submission, AssignmentGroup)
\item Enable practical operations (sets, dict keys, equality checks)
\item Improve developer experience (better [[__str__]])
\end{itemize}


\section{Make classes comparable and hashable}\label{ComparableObjects}

Since none of the classes in [[canvasapi]] defines the [[__eq__]] method, they 
all use the default which uses [[is]].
However, in many cases, it makes more sense to actually compare what the 
objects represent.
Consider two [[User]] objects that represent the same user (the same Canvas 
ID), then they should be considered equal, even if the objects themselves are 
different.
<<define decorator for comparable Canvas objects>>=
def canvas_comparable(cls):
  def is_equal(self, other):
    """Tests if Canvas objects self and other refer to the same object"""
    return type(self) == type(other) and self.id == other.id

  cls.__eq__ = is_equal
  return cls
@

The same applies for the hashable property.
A [[User]] object represents a fixed user that never changes, so we can use the 
type and Canvas ID to hash objects.
<<define decorator for hashable Canvas objects>>=
def canvas_hashable(cls):
  def canvas_hash(self):
    """Returns a hash suitable for Canvas objects"""
    return hash(type(self)) ^ hash(self.id)

  cls.__hash__ = canvas_hash
  return cls
@

Adding these two, will allow us to put these objects into sets, for instance.
We sum it up in a function that can be run automatically when including this 
module.
<<functions>>=
def make_classes_comparable():
  """Improves the classes by adding __eq__ and __hash__ methods"""
  <<define decorator for comparable Canvas objects>>
  <<define decorator for hashable Canvas objects>>
  <<improve eq method for classes>>
@

We want to do this for several classes.
<<improve eq method for classes>>=
# classes to improve in each module
CANVASAPI_CLASSES = {
  "assignment": ["Assignment", "AssignmentGroup"],
  "submission": ["Submission"],
  "user": ["User"],
  "group": ["GroupCategory", "Group"],
  "module": ["Module"]
}
@

We then want to load all the relevant modules given above.
<<improve eq method for classes>>=
canvasapi_modules = {}

# import all modules
for module_name in CANVASAPI_CLASSES:
  canvasapi_modules[module_name] = \
    importlib.import_module(f"canvasapi.{module_name}")
@

Finally, we can go through all the modules and extract their members.
For each member, we check if it's a member to decorate, if so, we apply the 
decorators~[[canvas_comparable]] and [[canvas_hashable]] to it.
<<improve eq method for classes>>=
for module_name, module in canvasapi_modules.items():
  module_members = inspect.getmembers(module)
  for obj_name, obj in module_members:
    if obj_name in CANVASAPI_CLASSES[module_name]:
      canvas_comparable(obj)
      canvas_hashable(obj)
@

\subsection{Verifying equality and hashability}

Now that we've implemented the decorators, let's verify they work correctly.
Our tests demonstrate \emph{semantic equality}: two objects with the same Canvas ID represent the same entity, even if they are different Python objects.

\subsubsection{Testing equality (\_\_eq\_\_)}

The [[__eq__]] method should compare Canvas objects by their type and Canvas ID.
Two objects of the same type with the same ID should be equal, even if they are different Python objects.

<<test functions>>=
class TestEqualityMonkeyPatch:
    """Test that Canvas objects can be compared by ID"""

    def test_users_with_same_id_are_equal(self):
        """Two User objects with same ID should be equal"""
        # Test with same mock instance (verifies ID comparison logic)
        user1 = MagicMock()
        user1.id = 12345

        # Same object is equal to itself
        assert User.__eq__(user1, user1)

    def test_users_with_different_ids_not_equal(self):
        """Users with different IDs should not be equal"""
        user1 = MagicMock()
        user1.id = 12345
        user2 = MagicMock()
        user2.id = 67890

        assert not User.__eq__(user1, user2)

    def test_different_types_not_equal(self):
        """Objects of different types should not be equal, even with same ID"""
        user = MagicMock()
        user.id = 12345

        assignment = MagicMock()
        assignment.id = 12345

        # Different types (checked by type())
        assert not User.__eq__(user, assignment)

    def test_assignments_have_equality_method(self):
        """Assignment class should have patched __eq__ method"""
        assign = MagicMock()
        assign.id = 5001

        # Same object is equal to itself (verifies patch exists)
        assert Assignment.__eq__(assign, assign)

    def test_submissions_have_equality_method(self):
        """Submission class should have patched __eq__ method"""
        sub = MagicMock()
        sub.id = 6001

        # Same object is equal to itself (verifies patch exists)
        assert Submission.__eq__(sub, sub)

    def test_assignment_groups_have_equality_method(self):
        """AssignmentGroup class should have patched __eq__ method"""
        group = MagicMock()
        group.id = 2001

        # Same object is equal to itself (verifies patch exists)
        assert AssignmentGroup.__eq__(group, group)
@

Notice how semantic equality transforms our ability to work with Canvas objects.
Before the patch, comparing objects was meaningless.
After the patch, we can check if two objects represent the same Canvas entity.

\subsubsection{Testing hashability ( [[__hash__]] )}

The [[__hash__]] method makes Canvas objects usable in sets and as dictionary keys.
This is crucial for deduplication and fast lookup operations.

The hash must be consistent with equality: equal objects must have equal hashes.

<<test functions>>=
class TestHashabilityMonkeyPatch:
    """Test that Canvas objects can be hashed and used in sets/dicts"""

    def test_users_with_same_id_have_same_hash(self):
        """Equal users should have equal hashes"""
        user = MagicMock()
        user.id = 12345

        # Same object hashed twice should give same hash
        assert User.__hash__(user) == User.__hash__(user)

    def test_users_with_different_ids_have_different_hashes(self):
        """Different users should (usually) have different hashes"""
        user1 = MagicMock()
        user1.id = 12345
        user2 = MagicMock()
        user2.id = 67890

        # Different IDs should produce different hashes
        # (hash collisions possible but unlikely)
        assert User.__hash__(user1) != User.__hash__(user2)

    def test_hashability_enables_set_usage(self):
        """
        Patched __hash__ allows Canvas objects to be used in sets.

        This test verifies the method is properly defined, making
        sets and dicts possible (actual usage requires both __eq__
        and __hash__ to work together on real Canvas objects).
        """
        user1 = MagicMock()
        user1.id = 12345
        user2 = MagicMock()
        user2.id = 67890

        # Both should be hashable
        hash1 = User.__hash__(user1)
        hash2 = User.__hash__(user2)

        assert isinstance(hash1, int)
        assert isinstance(hash2, int)
        assert hash1 != hash2

    def test_hashability_enables_dict_usage(self):
        """
        Patched __hash__ allows Canvas objects as dictionary keys.

        Verifies the hash method is properly defined and returns
        valid hash values based on type and ID.
        """
        assign1 = MagicMock()
        assign1.id = 5001
        assign2 = MagicMock()
        assign2.id = 5002

        # Both should produce valid hashes
        hash1 = Assignment.__hash__(assign1)
        hash2 = Assignment.__hash__(assign2)

        assert isinstance(hash1, int)
        assert isinstance(hash2, int)
        assert hash1 != hash2
@

These tests demonstrate the power of hashability: we can now use sets for deduplication and dictionaries for fast lookups.
Without these patches, Canvas objects would be unhashable and couldn't be used in these data structures.


\section{Improve User's [[__str__]] method}

By default, [[canvasapi]]'s [[User]] class defines a [[__str__]] dunder method 
that uses the user's name and Canvas ID.
We want to make it more useful, by using the user's name and login ID.
<<functions>>=
def make_useful_user_dunder_str():
  """Improves the user class by changing __str__"""
  <<define [[name_and_login]]>>
  <<update [[User.__str__]] to use [[name_and_login]]>>
@

Now, we simply need to define a function to use as a drop-in replacement for 
the [[__str__]] method.
<<define [[name_and_login]]>>=
def name_and_login(self):
  try:
    return f"{self.name} <{self.login_id}>"
  except AttributeError as err:
    return f"{self.name} <>"
@

Then we simply need to replace the current [[__str__]] method with the new one 
above.
<<update [[User.__str__]] to use [[name_and_login]]>>=
import canvasapi.user
canvasapi.user.User.__str__ = name_and_login
@

\subsection{Verifying the improved string representation}

Now let's verify this improved [[__str__]] method works as intended.
The original [[User.__str__]] method returns something like \enquote{User 12345}.
Our improvement returns \enquote{Alice Anderson <alice>}---much more useful for debugging and logging.

<<test functions>>=
class TestUserStringRepresentation:
    """Test improved User.__str__ method"""

    def test_user_str_with_name_and_login(self):
        """User string should show name and login ID"""
        user = MagicMock(spec=User)
        user.name = "Alice Anderson"
        user.login_id = "alice"

        result = User.__str__(user)

        assert result == "Alice Anderson <alice>"

    def test_user_str_without_login_id(self):
        """If login_id is missing, show empty brackets"""
        user = MagicMock(spec=User)
        user.name = "Bob Brown"
        # Simulate missing login_id attribute
        del user.login_id

        result = User.__str__(user)

        assert result == "Bob Brown <>"

    def test_user_str_useful_for_debugging(self):
        """String representation should be human-readable"""
        user = MagicMock(spec=User)
        user.name = "Charlie Chen"
        user.login_id = "cchen"

        # Should not contain Canvas ID or object address
        result = User.__str__(user)

        assert "Charlie Chen" in result
        assert "cchen" in result
        assert "0x" not in result  # No memory address
@

This improved string representation makes debugging much easier.
When you print a list of users or log user actions, you see meaningful names instead of numeric IDs.


\section{Cacheable Canvas objects}

We would like certain methods in certain Canvas objects to be cached.
Particularly, methods that return objects that rarely change should be 
cacheable.
For instance, [[canvas.get_courses()]] returns a list of courses.
This list changes rarely\footnote{%
  This happens four times per year, when IT creates the new Canvas rooms 270 
  days ahead of course start.
  It also happens when someone adds you to manually to a course.
}, so its results can and should be cached.
The list of students changes usually only once in the beginning of the course.

Results on the other hand changes during a course.
However, once a student receives a pass, the result will (usually) not change 
again.
There are these cases however, where a student complements their submission to 
get a higher grade.
But once the student has the highest grade, an A or a P, the result will not 
change.

We will use the [[cachetools]] package to provide suitable caching decorators 
for the [[canvasapi]] classes' methods.
However, in some cases we can't, but need to write our own caching mechanisms.
But this means that we can construct quite complex cache policies.

The general design is this:
We add a cache attribute, then the methods store objects (return values) there.
The [[Canvas]] object stores [[Course]] objects.
Each [[Course]] object stores [[Assignment]] objects.
Each [[Assignment]] object stores [[Submission]] objects.
This means we can store and restore the entire hierarchy just by storing the 
[[Canvas]] object.
If we pickle the [[Canvas]] object, the caches of the [[Course]] objects will 
be in there, the [[Assignment]] caches will in turn be in them, and so on.

When we want to force an update of the courses, we don't want to clear the 
cache.
Then we lose the entire hierarchy.
We can update the list instead of clearing the entire cache hierarchy.
That way we can update objects and keep their caches.
But we'll need a way to know when to update.

\subsection{Syncing caches}

We usually have two methods to cache:
\begin{center}
[[get_x(id, /, **kwargs)]]
\quad
and
\quad
[[get_xs(**kwargs)]].
\end{center}
For example,
\begin{center}
[[get_submission(id, /, **kwargs)]]
\quad
and
\quad
[[get_submissions(**kwargs)]].
\end{center}
These should sync.
In Canvas we can also pass options through keyword arguments, a common one 
being [[include]] which specifies a list of things to include.
For instance, when it comes to submissions we can ask Canvas to include grading 
rubrics.
This means that we want to maintain these details, so that we don't lose 
information in the cache.

This means that we need two functions to support the caching:
\begin{description}
\item [[must_update(prev_kwargs, kwargs)]] takes the old keyword 
arguments ([[prev_kwargs]]) to check if they're a superset of the new keyword 
arguments ([[kwargs]]).
If not, the cache doesn't have all necessary information and we must fetch it.
\item [[merge_kwargs]] will take a list of keyword arguments dictionaries and 
merge them.
Most of the time these contain only the key [[include]] whose value is a list 
of things to include.
So essentially, we want to take the union of all these sets.
\end{description}
We can thus start with the following defaults:
<<kwargs functions caches>>=
def must_update(prev_kwargs, new_kwargs,
                ignore_keys=[
                  <<keys to ignore>>
                ]):
  """
  Returns True if we must update the cache (refetch).

  <<doc for keys to ignore>>
  """
  for key, value in new_kwargs.items():
    # Skip ignored keys (like sorting parameters)
    if key in ignore_keys:
      continue

    if key not in prev_kwargs:
      return True
    elif isinstance(value, list):
      if set(value) > set(prev_kwargs[key]):
        return True
    elif value != prev_kwargs[key]:
      return True

  return False

def merge_kwargs(kwargs_list,
                 ignore_keys=[
                   <<keys to ignore>>
                 ]):
  """
  Merges a list of keyword arguments dictionaries. Lists are unioned.
  All non-list keys (usually strings) must be the same in all dictionaries.

  <<doc for keys to ignore>>
  """
  new_kwargs = dict()

  for kwargs in kwargs_list:
    for key, value in kwargs.items():
      if key not in new_kwargs:
        new_kwargs[key] = value
      elif isinstance(value, list) or isinstance(new_kwargs[key], list):
        # Convert both to lists if either is a list, then union
        prev_val = new_kwargs[key] if isinstance(new_kwargs[key], list) \
                   else [new_kwargs[key]]
        curr_val = value if isinstance(value, list) else [value]
        new_kwargs[key] = list(set(prev_val) | set(curr_val))
      else:
        <<if [[key]] can be ignored>>:
          new_kwargs[key] = value
        elif value != new_kwargs[key]:
          raise ValueError(f"Cannot merge {key} with "
                           f"{value} and {new_kwargs[key]}")

  return new_kwargs
@

We can now define the keys to ignore.
There are some keys that can be ignored for our purposes, like sorting order, 
as they don't affect the caching.
In those cases, we simply use the last value, as seen in the loop above.
<<doc for keys to ignore>>=
By default, we ignore the keys

  <<keys to ignore>>

as they don't affect the caching.
<<keys to ignore>>=
"sort",
"order",
"order_by"
@

To check if a key can be ignored, we simply check if it's in the [[ignore_keys]]
list.
<<if [[key]] can be ignored>>=
if key in ignore_keys
@

These utility functions are part of the caching infrastructure.
<<functions>>=
<<kwargs functions caches>>
@

Now let's verify these kwargs functions work correctly with comprehensive tests.

\subsubsection{Verifying [[must_update]]}

The [[must_update]] function determines when we need to refetch from the API.
We'll test both cases where the cache is sufficient and where it needs updating.

<<test functions>>=
class TestMustUpdate:
    """Test must_update() cache invalidation logic"""

    def test_no_update_when_cache_has_all_data(self):
        """Cache with all requested data should not update"""
        prev = {"include": ["submissions", "rubric"]}
        new = {"include": ["submissions"]}

        assert not must_update(prev, new)

    def test_no_update_when_kwargs_identical(self):
        """Identical kwargs should not trigger update"""
        prev = {"include": ["user"], "enrollment_type": "student"}
        new = {"include": ["user"], "enrollment_type": "student"}

        assert not must_update(prev, new)

    def test_update_when_new_key_not_in_cache(self):
        """New key not in cache should trigger update"""
        prev = {"include": ["user"]}
        new = {"include": ["user"], "enrollment_type": "student"}

        assert must_update(prev, new)

    def test_update_when_list_needs_more_items(self):
        """Cache missing list items should trigger update"""
        prev = {"include": ["user"]}
        new = {"include": ["user", "enrollments"]}

        assert must_update(prev, new)

    def test_update_when_non_list_value_differs(self):
        """Different non-list value should trigger update"""
        prev = {"enrollment_type": "student"}
        new = {"enrollment_type": "teacher"}

        assert must_update(prev, new)

    def test_no_update_when_cache_has_superset(self):
        """Cache with more data than needed should not update"""
        prev = {"include": ["user", "enrollments", "email"]}
        new = {"include": ["user", "enrollments"]}

        assert not must_update(prev, new)

    def test_ignores_sort_keys(self):
        """Sorting keys should be ignored (don't affect cache)"""
        prev = {"include": ["user"], "sort": "name"}
        new = {"include": ["user"], "sort": "created_at"}

        # Different sort order shouldn't trigger update
        assert not must_update(prev, new)

    def test_ignores_order_keys(self):
        """Order parameter should be ignored"""
        prev = {"include": ["user"], "order": "asc"}
        new = {"include": ["user"], "order": "desc"}

        assert not must_update(prev, new)

    def test_empty_new_kwargs_no_update(self):
        """Empty new kwargs should not trigger update"""
        prev = {"include": ["user"]}
        new = {}

        assert not must_update(prev, new)

    def test_empty_prev_kwargs_triggers_update(self):
        """Empty cache with new requests should trigger update"""
        prev = {}
        new = {"include": ["user"]}

        assert must_update(prev, new)
@

\subsubsection{Verifying [[merge_kwargs]]}

The [[merge_kwargs]] function combines multiple kwargs dictionaries, unioning
list values and ensuring non-list values are consistent.

<<test functions>>=
class TestMergeKwargs:
    """Test merge_kwargs() dictionary merging logic"""

    def test_empty_list_returns_empty_dict(self):
        """Empty list should return empty dictionary"""
        result = merge_kwargs([])

        assert result == {}

    def test_single_dict_returns_copy(self):
        """Single dictionary should be returned as-is"""
        kwargs = {"include": ["user"], "enrollment_type": "student"}
        result = merge_kwargs([kwargs])

        assert result == kwargs

    def test_unions_list_values(self):
        """List values should be unioned"""
        kwargs1 = {"include": ["user"]}
        kwargs2 = {"include": ["enrollments"]}

        result = merge_kwargs([kwargs1, kwargs2])

        assert set(result["include"]) == {"user", "enrollments"}

    def test_preserves_identical_non_list_values(self):
        """Identical non-list values should be preserved"""
        kwargs1 = {"enrollment_type": "student", "include": ["user"]}
        kwargs2 = {"enrollment_type": "student", "include": ["email"]}

        result = merge_kwargs([kwargs1, kwargs2])

        assert result["enrollment_type"] == "student"
        assert set(result["include"]) == {"user", "email"}

    def test_raises_on_conflicting_non_list_values(self):
        """Different non-list values should raise ValueError"""
        kwargs1 = {"enrollment_type": "student"}
        kwargs2 = {"enrollment_type": "teacher"}

        with pytest.raises(ValueError, match="Cannot merge enrollment_type"):
            merge_kwargs([kwargs1, kwargs2])

    def test_allows_conflicting_ignored_keys(self):
        """Conflicting values for ignored keys should use last value"""
        kwargs1 = {"include": ["user"], "sort": "name"}
        kwargs2 = {"include": ["email"], "sort": "created_at"}

        result = merge_kwargs([kwargs1, kwargs2])

        # sort is ignored, so last value wins
        assert result["sort"] == "created_at"
        assert set(result["include"]) == {"user", "email"}

    def test_converts_mixed_list_and_scalar(self):
        """Scalar value merged with list should convert to list"""
        kwargs1 = {"enrollment_type": "student"}
        kwargs2 = {"enrollment_type": ["teacher"]}

        result = merge_kwargs([kwargs1, kwargs2])

        assert set(result["enrollment_type"]) == {"student", "teacher"}

    def test_merges_multiple_dicts(self):
        """Should correctly merge more than two dictionaries"""
        kwargs1 = {"include": ["user"]}
        kwargs2 = {"include": ["enrollments"]}
        kwargs3 = {"include": ["email"], "enrollment_type": "student"}

        result = merge_kwargs([kwargs1, kwargs2, kwargs3])

        assert set(result["include"]) == {"user", "enrollments", "email"}
        assert result["enrollment_type"] == "student"

    def test_realistic_canvas_api_merge(self):
        """Realistic scenario: merging submissions kwargs"""
        # First call wants user info
        call1 = {"include": ["user", "submission_comments"]}
        # Second call wants rubric
        call2 = {"include": ["rubric_assessment"]}
        # Third call adds more user data
        call3 = {"include": ["user", "submission_history"]}

        result = merge_kwargs([call1, call2, call3])

        expected = {"user", "submission_comments", "rubric_assessment",
                    "submission_history"}
        assert set(result["include"]) == expected
@

\subsection{Wrapping the methods with a caching decorator}

Now, let's turn our attention to the caching decorator.
We want to cover both the singular and plural methods.
So this decorator should be applied to the class, for instance the 
[[Assignment]] class with the attribute [[submission]] to cache
[[get_submission]] and [[get_submissions]].
<<general class decorator for caching get methods>>=
class CacheGetMethods:
  """
  General class decorator to add caching to get_*{,s} methods.

  <<notes in [[CacheGetMethods]] doc>>
  """
  def __init__(self, attribute_name, cache=None, include_plural=True,
               include_singular=True, plural_name=None):
    """No parameters required"""
    self.__attribute_name = attribute_name
    self.__include_plural = include_plural
    self.__include_singular = include_singular
    self.__plural_name = plural_name
    self.__cache = cache if cache else {}

  def __call__(self, cls):
    """Applies the decorator to the class cls"""
    <<create decorator functions and update [[cls]]>>
    return cls
@ If we want to cache several attributes, then we can simply apply this decorator
several times.
For instance, the [[Course]] class has both [[assignments]] and [[users]].

Some Canvas API entities only provide plural methods (e.g.,
[[get_assignment_groups()]] but no [[get_assignment_group()]]).
For these, we use [[include_singular=False]].

We need to update two methods in sync, the [[get_x]] and [[get_xs]] methods.
This is why we can't just use the decorators from [[cachetools]] or similar.
<<create decorator functions and update [[cls]]>>=
<<update constructor with new attributes>>

if self.__include_singular:
  <<update the singular method>>

if self.__include_plural:
  <<update the plural method>>
@

Now let's verify the decorator initialization works correctly.

\subsubsection{Verifying [[CacheGetMethods]] initialization}

The decorator must accept various configuration parameters and apply itself
correctly to classes.

<<test cache get methods init>>=
class TestCacheGetMethodsInit:
    """Test CacheGetMethods decorator initialization"""

    def test_default_parameters(self):
        """Default parameters should create empty cache and include both methods"""
        decorator = CacheGetMethods("assignment")

        assert decorator._CacheGetMethods__attribute_name == "assignment"
        assert decorator._CacheGetMethods__cache == {}
        assert decorator._CacheGetMethods__include_singular is True
        assert decorator._CacheGetMethods__include_plural is True
        assert decorator._CacheGetMethods__plural_name is None

    def test_custom_cache_parameter(self):
        """Custom cache dict should be stored"""
        custom_cache = {1: ("obj1", {}), 2: ("obj2", {})}
        decorator = CacheGetMethods("user", cache=custom_cache)

        assert decorator._CacheGetMethods__cache is custom_cache

    def test_include_singular_false(self):
        """include_singular=False should store False"""
        decorator = CacheGetMethods("assignment_group", include_singular=False)

        assert decorator._CacheGetMethods__include_singular is False
        assert decorator._CacheGetMethods__include_plural is True

    def test_include_plural_false(self):
        """include_plural=False should store False"""
        decorator = CacheGetMethods("submission", include_plural=False)

        assert decorator._CacheGetMethods__include_plural is False
        assert decorator._CacheGetMethods__include_singular is True

    def test_custom_plural_name(self):
        """plural_name parameter should override default pluralization"""
        decorator = CacheGetMethods("group_category",
                                     plural_name="group_categories")

        assert decorator._CacheGetMethods__plural_name == "group_categories"

    def test_decorator_returns_modified_class(self):
        """Decorator should return the class to allow stacking"""
        class DummyClass:
            def __init__(self):
                pass

            def get_test(self, test_id):
                return MagicMock(id=test_id)

            def get_tests(self):
                return []

        decorator = CacheGetMethods("test")
        result = decorator(DummyClass)

        assert result is DummyClass
@

<<test hacks.py>>=
<<test cache get methods init>>
@

We must update the constructor so that the object that is created keeps the
cache.
To update the constructor, we simply wrap it in a new constructor that simply 
adds the desired attributes.

Note that the cache stored in the constructor must be named specifically to the
attribute at hand.
Otherwise, when we cache several attributes, they will all use the same cache
named [[cache]].
This will not work.
So they must be named [[submission_cache]], for example.

We capture [[attr_name]] as a closure variable on line~358.
This is crucial for correct decorator behavior when multiple decorators are
applied to the same class.
We must \emph{not} store [[attr_name]] as an instance attribute (like
[[self.attr_name]]) because that would be shared across all decorator
applications.
For example, if we decorate [[Course]] with both [[CacheGetMethods("assignment")]]
and [[CacheGetMethods("user")]], storing as [[self.attr_name]] would cause the
second decorator to overwrite the first decorator's value, making both use
``user'' incorrectly.
By using the closure variable [[attr_name]], each decorator captures its own
value that remains constant throughout the wrapper functions' lifetime.
<<update constructor with new attributes>>=
init = cls.__init__
attr_name = self.__attribute_name

@functools.wraps(init)
def new_init(*args, **kwargs):
  self = args[0]
  <<construct [[attr_name]] cache attributes in [[self]]>>
  init(*args, **kwargs)

cls.__init__ = new_init
@

We do the same thing for the singular and plural get methods.
We get the original method, then wrap it in a new method that adds suitable 
caching or falls back to the original [[get_*]] function.
<<update the singular method>>=
singular_name = f"get_{self.__attribute_name}"
get_attr = getattr(cls, singular_name)

@functools.wraps(get_attr)
def new_get_attr(self, *args, **kwargs):
  <<return cached result or fetch from [[get_attr]]>>

setattr(cls, singular_name, new_get_attr)
<<update the plural method>>=
if self.__plural_name:
  plural_name = f"get_{self.__plural_name}"
else:
  plural_name = f"get_{self.__attribute_name}s"
get_attrs = getattr(cls, plural_name)

@functools.wraps(get_attrs)
def new_get_attrs(self, *args, **kwargs):
  <<return cached result or fetch from [[get_attrs]]>>

setattr(cls, plural_name, new_get_attrs)
@

\subsection{Caching the get methods}

The idea of the caching is as follows.
If the plural method ([[get_xs]]) is called, we fetch all items if they haven't 
been fetched before.
If the singular method ([[get_x]]) is called, we check for the request in the 
cache, otherwise we fetch it.
Each request can specify that Canvas should include extra data.
We don't want to lose data, so when making a new call, we first check what data 
has been fetched, so that we never lose data from the cache.

To be able to handle caching, we need attributes to keep track of the cache and 
if we've fetched all objects.
Even if we've fetched all objects, there might be new objects added at times.
So periodically, we must refetch all objects even if we've previously fetched 
them---but only when a sufficient amount of time has passed.
So we'll let the [[_all_fetched]] attribute be a [[datetime]] object, or 
[[None]] if it was never fetched.
<<construct [[attr_name]] cache attributes in [[self]]>>=
if not hasattr(self, f"{attr_name}_cache"):
  setattr(self, f"{attr_name}_cache", {})
if not hasattr(self, f"{attr_name}_all_fetched"):
  setattr(self, f"{attr_name}_all_fetched", None)
@

Now let's verify cache attributes are properly injected into instances.

\subsubsection{Verifying cache attribute injection}

When the decorator modifies a class constructor, it must inject the cache
attributes into every instance.

<<test cache attribute injection>>=
class TestCacheAttributeInjection:
    """Test cache attributes are injected into decorated class instances"""

    def test_creates_cache_attribute(self):
        """Instances should have {attr}_cache attribute"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        assert hasattr(instance, "item_cache")
        assert isinstance(instance.item_cache, dict)

    def test_creates_all_fetched_attribute(self):
        """Instances should have {attr}_all_fetched attribute"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        assert hasattr(instance, "item_all_fetched")
        assert instance.item_all_fetched is None

    def test_cache_initialized_empty(self):
        """Cache should be initialized as empty dict"""
        class TestClass:
            def __init__(self):
                pass
            def get_submission(self, sub_id):
                return MagicMock(id=sub_id)
            def get_submissions(self):
                return []

        CacheGetMethods("submission")(TestClass)
        instance = TestClass()

        assert instance.submission_cache == {}

    def test_multiple_decorators_create_separate_attributes(self):
        """Multiple decorators should create separate cache attributes"""
        class TestClass:
            def __init__(self):
                pass
            def get_assignment(self, asgn_id):
                return MagicMock(id=asgn_id)
            def get_assignments(self):
                return []
            def get_user(self, user_id):
                return MagicMock(id=user_id)
            def get_users(self):
                return []

        CacheGetMethods("assignment")(TestClass)
        CacheGetMethods("user")(TestClass)
        instance = TestClass()

        assert hasattr(instance, "assignment_cache")
        assert hasattr(instance, "assignment_all_fetched")
        assert hasattr(instance, "user_cache")
        assert hasattr(instance, "user_all_fetched")

    def test_cache_attributes_instance_specific(self):
        """Cache attributes should be instance-specific not shared"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance1 = TestClass()
        instance2 = TestClass()

        instance1.item_cache[1] = ("obj1", {})

        assert 1 in instance1.item_cache
        assert 1 not in instance2.item_cache

    def test_original_init_still_called(self):
        """Original __init__ should still execute"""
        class TestClass:
            def __init__(self, value):
                self.value = value
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass(42)

        assert instance.value == 42
        assert hasattr(instance, "item_cache")
@

<<test hacks.py>>=
<<test cache attribute injection>>
@

In all [[get_attr]] requests, it's the positional arguments ([[*args]]) that 
identify what to get.
The keyword arguments ([[**kwargs]]) specify additional options.
However, in most cases the [[*args]] will only contain one element, and that is 
an ID.
<<notes in [[CacheGetMethods]] doc>>=
We assume that the first positional argument is the ID of the object to fetch.
This must be the same as the `.id` attribute of an object (`obj.id`).

Parameters:
  attribute_name: The attribute name to cache (e.g., "assignment", "user").
  cache: Optional initial cache dictionary (default: {}).
  include_plural: Whether to cache the plural method get_*s (default: True).
  include_singular: Whether to cache the singular method get_* (default: True).
  plural_name: Custom plural name for irregular plurals (default: None, uses
               attribute_name + "s"). For example, "group_categories" instead
               of "group_categorys".
@

This allows us to specify how the cache is structured.
We'll let the cache be a dictionary where the key is the ID of the object.
The value will be a tuple of the object and the keyword arguments used to fetch 
it.
For example:
\begin{minted}{text}
{
  1: (obj1, kwargs1),
  2: (obj2, kwargs2),
  ...
}
\end{minted}
<<return cached result or fetch from [[get_attr]]>>=
attr_cache = getattr(self, f"{attr_name}_cache")

<<let [[id]] be the ID of the object to fetch>>

try:
  obj, prev_kwargs = attr_cache[id]
  cache_status = ""
except KeyError:
  obj = None
  prev_kwargs = {}
  cache_status = " (not found)"

if obj and must_update(prev_kwargs, kwargs):
  obj = None
  cache_status = " (new kwargs required)"
elif obj and outdated(obj):
  reason = get_staleness_reason(obj)
  obj = None
  cache_status = f" (stale: {reason})" if reason else " (stale)"

if not obj:
  fetch_start = time.perf_counter()
  obj = get_attr(self, *args, **kwargs)
  fetch_elapsed = time.perf_counter() - fetch_start
  obj._fetched_at = datetime.now()
  attr_cache[obj.id] = (obj, kwargs)
  logger.info(f"Cache miss{cache_status}: {attr_name} id={id} fetched in "
              f"{fetch_elapsed:.2f}s")
else:
  logger.info(f"Cache hit: {attr_name} id={id}")

return obj
@

We log cache hits and misses to measure caching effectiveness.
Cache hits should be nearly instantaneous (microseconds), as we're just
returning a cached object.
Cache misses require API calls to Canvas, which typically take hundreds of
milliseconds to seconds depending on network latency and server response time.
This timing data helps identify whether caching is providing the expected
performance benefits.

We already saw [[must_update]] above.
That one focuses on the keyword arguments.
We'll return to the [[outdated]] function later, that one determines from the
object itself if it needs updating.

The [[get_*]] methods all take either an ID or an object as argument.
If it's an object, then we can get the ID from that object.
<<let [[id]] be the ID of the object to fetch>>=
try:
  obj = args[0]
  id = obj.id
except IndexError:
  raise TypeError(f"{singular_name}() missing 1 required positional "
                  f"argument: 'id'")
except AttributeError:
  if isinstance(obj, int):
    id = obj
  else:
    raise TypeError(f"{singular_name}() argument 1 must be int or "
                    f"Canvas object, not {type(obj)}")
@

Now let's verify the singular method caching works correctly.

\subsubsection{Verifying singular method caching}

The decorated [[get_*]] method should cache results and avoid redundant API calls.

<<test singular method caching>>=
class TestSingularMethodCaching:
    """Test get_* singular method caching logic"""

    def test_first_call_fetches_from_api(self):
        """First call should fetch from API and cache result"""
        call_count = {"count": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                call_count["count"] += 1
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()
        result = instance.get_item(1)

        assert call_count["count"] == 1
        assert result.id == 1
        assert 1 in instance.item_cache

    def test_second_call_returns_cached(self):
        """Second call with same ID should return cached object"""
        call_count = {"count": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                call_count["count"] += 1
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        result1 = instance.get_item(1)
        result2 = instance.get_item(1)

        assert call_count["count"] == 1
        assert result1 is result2

    def test_cache_stores_object_kwargs_tuple(self):
        """Cache should store (object, kwargs) tuple"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id, **kwargs):
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        instance.get_item(1, include=["user"])

        cached_obj, cached_kwargs = instance.item_cache[1]
        assert cached_obj.id == 1
        assert cached_kwargs == {"include": ["user"]}

    def test_must_update_triggers_refetch(self):
        """Changed kwargs should trigger refetch via must_update"""
        call_count = {"count": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id, **kwargs):
                call_count["count"] += 1
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        instance.get_item(1, include=["user"])
        instance.get_item(1, include=["user", "email"])

        assert call_count["count"] == 2

    def test_outdated_triggers_refetch(self):
        """Stale object should trigger refetch via outdated()"""
        call_count = {"count": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_submission(self, sub_id):
                call_count["count"] += 1
                obj = MagicMock()
                obj.id = sub_id
                obj.grade = "B"  # Non-passing grade
                return obj
            def get_submissions(self):
                return []

        CacheGetMethods("submission")(TestClass)
        instance = TestClass()

        # First call
        result1 = instance.get_submission(1)
        # Make it outdated (> 5 minutes old)
        result1._fetched_at = datetime.now() - timedelta(minutes=6)
        instance.submission_cache[1] = (result1, {})

        # Second call should refetch
        result2 = instance.get_submission(1)

        assert call_count["count"] == 2

    def test_accepts_integer_id(self):
        """Should accept integer ID as argument"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        result = instance.get_item(42)

        assert result.id == 42

    def test_accepts_canvas_object_with_id(self):
        """Should accept Canvas object and extract .id"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                if hasattr(item_id, 'id'):
                    obj.id = item_id.id
                else:
                    obj.id = item_id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        canvas_obj = MagicMock()
        canvas_obj.id = 99
        result = instance.get_item(canvas_obj)

        assert result.id == 99

    def test_sets_fetched_at_timestamp(self):
        """Should set _fetched_at timestamp on fetched objects"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        before = datetime.now()
        result = instance.get_item(1)
        after = datetime.now()

        assert hasattr(result, '_fetched_at')
        assert before <= result._fetched_at <= after

    def test_raises_typeerror_missing_argument(self):
        """Should raise TypeError if no argument provided"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        with pytest.raises(TypeError, match="missing 1 required positional"):
            instance.get_item()

    def test_raises_typeerror_invalid_argument_type(self):
        """Should raise TypeError for invalid argument types"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        with pytest.raises(TypeError, match="must be int or Canvas object"):
            instance.get_item("invalid_string")

    def test_cache_key_is_object_id(self):
        """Cache key should be object.id not argument value"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id if isinstance(item_id, int) else item_id.id
                return obj
            def get_items(self):
                return []

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        # Pass Canvas object
        canvas_obj = MagicMock()
        canvas_obj.id = 123
        instance.get_item(canvas_obj)

        # Cache key should be the ID, not the object
        assert 123 in instance.item_cache
@

<<test hacks.py>>=
<<test singular method caching>>
@

Now, let's turn to the plural method.
We have the obvious case that we must fetch all if we haven't fetched all 
before.
But after this it gets more interesting.
If we have fetched all, we must check if the previous [[kwargs]] are a superset
of the new [[kwargs]].
If not, we must fetch all again with the extended [[kwargs]].
However, even if we can reuse the cache, we must check each object if it
requires updating.
For instance, if it's a submission and the grade is not P, then we must update
it.

The key logic is that we have two distinct paths:
\begin{enumerate}
\item \textbf{Bulk fetch path} ([[if not attr_all_fetched]]): When cache is
  empty or stale, we do one bulk API call to fetch all objects and populate the
  cache.
  We then yield all objects without individual staleness checks.
  This is efficient because all objects are fresh from the bulk fetch.
\item \textbf{Cached path} ([[else]]): When cache is already populated and
  fresh, we yield objects from cache, checking each for staleness.
  If an object is outdated (e.g., submission without passing grade older than 5
  minutes), we fetch that individual object.
  This avoids refetching everything when only a few items need updating.
\end{enumerate}
The [[else]] is critical to avoid the double-yield bug: without it, we would
yield all objects once from the bulk fetch, then yield them all again while
checking for staleness, causing N individual API calls for outdated items
immediately after a bulk fetch.
<<return cached result or fetch from [[get_attrs]]>>=
attr_cache = getattr(self, f"{attr_name}_cache")
attr_all_fetched = getattr(self, f"{attr_name}_all_fetched")

if attr_all_fetched:
  for _, prev_kwargs in attr_cache.values():
    if must_update(prev_kwargs, kwargs):
      logger.info(f"Cache invalidation: {attr_name}s - new kwargs required "
                  f"(existing: {prev_kwargs}, requested: {kwargs})")
      attr_all_fetched = None
      break

if not attr_all_fetched:
  <<log reason for bulk refresh>>
  <<fetch all and populate [[attr_cache]]>>
  <<yield all objects in [[attr_cache]]>>
else:
  cache_age = datetime.now() - attr_all_fetched
  stale_count = sum(1 for obj, _ in attr_cache.values() if outdated(obj))
  logger.info(f"Using cached {attr_name}s: {len(attr_cache)} cached "
              f"(age: {cache_age.total_seconds()/60:.1f} minutes, "
              f"{stale_count} stale), updating stale entries individually")
  <<yield all objects in [[attr_cache]], update individual entries if needed>>
@

\subsection{Logging cache decisions}

To make cache behavior transparent, we log key decision points in the plural
[[get_xs]] methods. This follows the same diagnostic pattern as submission-specific
caching (lines 1163-1183) but applies to the general [[CacheGetMethods]] decorator
used by courses, users, assignments, and other Canvas objects.

We log three critical decision points:
\begin{enumerate}
\item \textbf{Bulk fetch trigger}: When [[attr_all_fetched]] is [[None]], we log
  whether this is the first fetch (cold cache) or a cache invalidation (TTL expired
  or kwargs changed). This explains why a bulk refresh is happening.
\item \textbf{Cached path usage}: When [[attr_all_fetched]] is set, we log the
  cache age, total size, and how many entries are stale (requiring individual refresh).
  This provides visibility into cache health and effectiveness.
\item \textbf{Kwargs invalidation}: When [[must_update()]] detects that cached
  objects don't have the requested kwargs (e.g., new [[include]] fields), we log
  what changed before invalidating the cache. This explains why a bulk refresh
  will occur even though the cache was populated.
\end{enumerate}

These logs, combined with the bulk refresh completion message (line 650-652)
and kwargs merging log (lines 609-622), provide complete transparency into why
courses, users, and assignments trigger bulk refreshes versus using cached data.

When [[attr_all_fetched]] is [[None]], we determine and log the reason:
<<log reason for bulk refresh>>=
if not attr_cache:
  refresh_reason = "first fetch (cold cache)"
else:
  refresh_reason = "cache invalidated"
logger.info(f"Bulk fetch: {attr_name}s - {refresh_reason}")
@

To fetch all again, we must merge the keyword arguments.
This is done with the [[merge_kwargs]] function.
Then we must add each object to the cache.

We separately time the API call versus processing to identify bottlenecks.
The API call time represents network latency and Canvas server response time,
while processing time represents local cache updates and object manipulation.
This helps distinguish between network bottlenecks and local processing
overhead.
<<fetch all and populate [[attr_cache]]>>=
bulk_start = time.perf_counter()

original_includes = set(kwargs.get('include', []))
union_kwargs = merge_kwargs(
  [kwargs for _, kwargs in attr_cache.values()] + [kwargs])
merged_includes = set(union_kwargs.get('include', []))

if merged_includes != original_includes:
  added = merged_includes - original_includes
  logger.info(f"Merging kwargs for {attr_name}: extending include fields "
              f"from {sorted(original_includes)} to {sorted(merged_includes)} "
              f"(added: {sorted(added)})")

api_start = time.perf_counter()
fetched_objs = list(get_attrs(self, *args, **union_kwargs))
api_elapsed = time.perf_counter() - api_start

process_start = time.perf_counter()
for obj in fetched_objs:
  old_entry = attr_cache.get(obj.id, None)
  if old_entry:
    old_obj, _ = old_entry
    <<update [[obj]] with cache from [[old_obj]]>>
  obj._fetched_at = datetime.now()
  attr_cache[obj.id] = (obj, union_kwargs)
process_elapsed = time.perf_counter() - process_start

setattr(self, f"{attr_name}_all_fetched", datetime.now())
attr_all_fetched = getattr(self, f"{attr_name}_all_fetched")

bulk_elapsed = time.perf_counter() - bulk_start
logger.info(f"Bulk refresh completed: {len(fetched_objs)} {attr_name}s fetched in "
            f"{bulk_elapsed:.2f}s (API: {api_elapsed:.2f}s, "
            f"processing: {process_elapsed:.2f}s)")
@

Finally, when we want to return all the objects, we will yield them instead.
It's more efficient to use a generator in this case, as that would make us
update when (if) needed.
When we update an object, we should use the same keyword arguments as specified
in the cache---not the requested ones---because these will, at this point, be
the largest possible set of keyword arguments.

We log individual refresh timing to show the cost of updating stale cache
entries without a full bulk fetch.
This provides contrast with bulk refresh performance: individual refreshes make
one API call per outdated object, while bulk refresh makes a single API call
for all objects.
The timing data reveals when the bulk refresh threshold optimization is
beneficial.
<<yield all objects in [[attr_cache]], update individual entries if needed>>=
for obj, obj_kwargs in attr_cache.values():
  if outdated(obj):
    reason = get_staleness_reason(obj)
    reason_str = f" (reason: {reason})" if reason else ""
    refresh_start = time.perf_counter()
    obj = get_attr(self, obj.id, **obj_kwargs)
    refresh_elapsed = time.perf_counter() - refresh_start
    logger.info(f"Individual refresh: {attr_name} id={obj.id} in "
                f"{refresh_elapsed:.2f}s{reason_str}")
  yield obj
<<yield all objects in [[attr_cache]]>>=
for obj, _ in attr_cache.values():
  yield obj
@

\subsubsection{Testing cache timing and logging}

The cache timing instrumentation helps distinguish between network bottlenecks
and local processing overhead.
We verify that timing measurements are accurate and log messages are properly formatted.

<<test cache timing>>=
class TestCacheTimingMeasurement:
    """Test that cache operations measure timing correctly"""

    def test_timing_uses_perf_counter(self):
        """Timing measurements should use time.perf_counter() for accuracy"""
        import time
        
        start = time.perf_counter()
        time.sleep(0.01)  # Small delay
        elapsed = time.perf_counter() - start
        
        # Should measure at least 10ms
        assert elapsed >= 0.01
        assert elapsed < 0.1  # But not ridiculously long

    def test_separate_api_and_processing_timing(self):
        """
        Cache operations separately time API calls and processing.
        
        This helps distinguish between network bottlenecks and local overhead.
        """
        import time
        
        # Simulate API call timing
        api_start = time.perf_counter()
        time.sleep(0.01)
        api_elapsed = time.perf_counter() - api_start
        
        # Simulate processing timing
        process_start = time.perf_counter()
        time.sleep(0.005)
        process_elapsed = time.perf_counter() - process_start
        
        # Both should be measured independently
        assert api_elapsed >= 0.01
        assert process_elapsed >= 0.005
        assert api_elapsed + process_elapsed >= 0.015

    def test_bulk_timing_includes_total_time(self):
        """Bulk refresh measures total time including API and processing"""
        import time
        
        bulk_start = time.perf_counter()
        
        # Simulate API call
        api_start = time.perf_counter()
        time.sleep(0.01)
        api_elapsed = time.perf_counter() - api_start
        
        # Simulate processing
        process_start = time.perf_counter()
        time.sleep(0.005)
        process_elapsed = time.perf_counter() - process_start
        
        bulk_elapsed = time.perf_counter() - bulk_start
        
        # Bulk time should be at least the sum of API and processing
        assert bulk_elapsed >= api_elapsed + process_elapsed

class TestCacheLoggingMessages:
    """Test that cache operations log appropriate messages"""

    def test_bulk_refresh_log_message_format(self):
        """Bulk refresh logs count, attr_name, total, API, and processing time"""
        attr_name = "assignments"
        fetched_count = 42
        bulk_elapsed = 2.5
        api_elapsed = 2.0
        process_elapsed = 0.5
        
        expected_message = (
            f"Bulk refresh: {fetched_count} {attr_name}s in "
            f"{bulk_elapsed:.2f}s (API: {api_elapsed:.2f}s, "
            f"processing: {process_elapsed:.2f}s)"
        )
        
        assert "Bulk refresh: 42 assignments" in expected_message
        assert "2.50s" in expected_message
        assert "API: 2.00s" in expected_message
        assert "processing: 0.50s" in expected_message

    def test_individual_refresh_log_message_format(self):
        """Individual refresh logs timing information"""
        attr_name = "submission"
        obj_id = 456
        refresh_elapsed = 0.3
        
        expected_message = (
            f"Individual refresh: {attr_name} id={obj_id} "
            f"in {refresh_elapsed:.2f}s"
        )
        
        assert "Individual refresh: submission id=456" in expected_message
        assert "in 0.30s" in expected_message

    @pytest.mark.parametrize("elapsed,expected", [
        (0.123, "0.12s"),
        (1.5, "1.50s"),
        (12.345, "12.35s"),
        (0.001, "0.00s"),
        (99.999, "100.00s"),
    ])
    def test_timing_formatted_to_two_decimal_places(self, elapsed, expected):
        """Timing should be formatted to 2 decimal places"""
        formatted = f"{elapsed:.2f}s"
        assert formatted == expected
@

This completes the [[CacheGetMethods]] class decorator, which we add to the module's
functions.
<<functions>>=
<<general class decorator for caching get methods>>
@

We add the cache timing tests to the main test file.
<<test hacks.py>>=
<<test cache timing>>
@

Now let's verify the plural method caching works correctly.

\subsubsection{Verifying plural method caching}

The decorated [[get_*s]] method should efficiently handle bulk fetches and cached
retrievals with individual staleness checks.

<<test plural method caching>>=
class TestPluralMethodCaching:
    """Test get_*s plural method caching logic"""

    def test_first_call_performs_bulk_fetch(self):
        """First call should perform bulk fetch"""
        call_count = {"count": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                call_count["count"] += 1
                return [MagicMock(id=1), MagicMock(id=2), MagicMock(id=3)]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        list(instance.get_items())

        assert call_count["count"] == 1

    def test_bulk_fetch_populates_cache(self):
        """Bulk fetch should populate cache with all objects"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                return [MagicMock(id=i) for i in [10, 20, 30]]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        list(instance.get_items())

        assert 10 in instance.item_cache
        assert 20 in instance.item_cache
        assert 30 in instance.item_cache

    def test_bulk_fetch_sets_all_fetched_timestamp(self):
        """Bulk fetch should set {attr}_all_fetched timestamp"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                return [MagicMock(id=1)]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        before = datetime.now()
        list(instance.get_items())
        after = datetime.now()

        assert instance.item_all_fetched is not None
        assert before <= instance.item_all_fetched <= after

    def test_second_call_yields_from_cache(self):
        """Second call should yield from cache without API call"""
        call_count = {"count": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                call_count["count"] += 1
                return [MagicMock(id=i) for i in [1, 2, 3]]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        list(instance.get_items())
        list(instance.get_items())

        assert call_count["count"] == 1

    def test_returns_generator_not_list(self):
        """Should return generator not list"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                return [MagicMock(id=1)]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        result = instance.get_items()

        from types import GeneratorType
        assert isinstance(result, GeneratorType)

    def test_cache_path_checks_outdated(self):
        """Cached path should check each object with outdated()"""
        refetch_count = {"count": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_submission(self, sub_id):
                refetch_count["count"] += 1
                obj = MagicMock()
                obj.id = sub_id
                obj.grade = "A"  # Won't be outdated
                return obj
            def get_submissions(self):
                return [MagicMock(id=i, grade="A") for i in [1, 2]]

        CacheGetMethods("submission")(TestClass)
        instance = TestClass()

        # First call - bulk fetch
        list(instance.get_submissions())
        # Second call - should check outdated but not refetch (grade=A)
        list(instance.get_submissions())

        # No refetches because all have passing grades
        assert refetch_count["count"] == 0

    def test_stale_objects_refetched(self):
        """Stale individual objects should be refetched"""
        refetch_count = {"count": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_submission(self, sub_id):
                refetch_count["count"] += 1
                obj = MagicMock()
                obj.id = sub_id
                obj.grade = "B"  # Non-passing grade
                return obj
            def get_submissions(self):
                return [MagicMock(id=1, grade="B")]

        CacheGetMethods("submission")(TestClass)
        instance = TestClass()

        # First call - bulk fetch
        list(instance.get_submissions())

        # Make it outdated
        obj, kwargs = instance.submission_cache[1]
        obj._fetched_at = datetime.now() - timedelta(minutes=6)
        instance.submission_cache[1] = (obj, kwargs)

        # Second call should refetch the outdated object
        list(instance.get_submissions())

        assert refetch_count["count"] == 1

    def test_bulk_fetch_merges_kwargs(self):
        """Bulk fetch should merge all previous kwargs"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id, **kwargs):
                return MagicMock(id=item_id)
            def get_items(self, **kwargs):
                # Return objects with merged kwargs visible
                return [MagicMock(id=1)]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        # Fetch with one include
        instance.get_item(1, include=["user"])
        # Clear all_fetched to force bulk refetch
        instance.item_all_fetched = None
        # Bulk fetch with additional include
        list(instance.get_items(include=["email"]))

        # Cache should have merged kwargs
        _, cached_kwargs = instance.item_cache[1]
        assert "user" in cached_kwargs.get("include", [])
        assert "email" in cached_kwargs.get("include", [])

    def test_irregular_plural_name(self):
        """plural_name parameter should work correctly"""
        class TestClass:
            def __init__(self):
                pass
            def get_group_category(self, cat_id):
                return MagicMock(id=cat_id)
            def get_group_categories(self):
                return [MagicMock(id=1)]

        CacheGetMethods("group_category",
                        plural_name="group_categories")(TestClass)
        instance = TestClass()

        result = list(instance.get_group_categories())

        assert len(result) == 1
        assert result[0].id == 1

    def test_empty_cache_after_reset_triggers_bulk_fetch(self):
        """Setting all_fetched=None should trigger bulk refetch"""
        call_count = {"count": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                call_count["count"] += 1
                return [MagicMock(id=1)]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        list(instance.get_items())
        instance.item_all_fetched = None
        list(instance.get_items())

        assert call_count["count"] == 2

    def test_no_double_yield_after_bulk_fetch(self):
        """Should not yield objects twice after bulk fetch"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                return MagicMock(id=item_id)
            def get_items(self):
                obj1 = MagicMock()
                obj1.id = 1
                obj1.grade = "B"  # Non-passing (would be checked if double-yield)
                return [obj1]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        results = list(instance.get_items())

        # Should only yield once, not twice
        assert len(results) == 1
@

<<test hacks.py>>=
<<test plural method caching>>
@

\subsection{Copy cache to new object}

When we update all objects, we want to keep the caches of every old object.
Otherwise, whenever we update all objects we lose all caches further down the 
hierarchy.
When we construct a new object, we want to copy the cache from the old object.

We can do this by finding all cache-related attributes and copying them.
The problem is that we don't know what attributes an object has caches for.
So we'll do some pattern matching on the attribute names.
We'll look for attributes that end with [[_cache]] and [[_all_fetched]].

Note that we use [[cache_attr_name]] as the loop variable, not [[attr_name]].
This is critical to avoid shadowing the closure variable [[attr_name]] that was
captured in line~371.
If we used [[attr_name]] as the loop variable, Python would treat it as a local
variable throughout the entire [[new_get_attrs]] function, causing an
[[UnboundLocalError]] when we try to access it before the loop (in
lines~494--495).
<<update [[obj]] with cache from [[old_obj]]>>=
for cache_attr_name in dir(old_obj):
  if cache_attr_name.endswith("_cache") or cache_attr_name.endswith("_all_fetched"):
    setattr(obj, cache_attr_name, getattr(old_obj, cache_attr_name))
@

Now let's verify cache synchronization preserves hierarchical caches.

\subsubsection{Verifying cache synchronization}

When bulk fetching objects, we must preserve their sub-object caches to avoid
losing cached data down the hierarchy.

<<test cache synchronization>>=
class TestCacheSynchronization:
    """Test cache preservation during bulk updates"""

    def test_preserves_sub_object_caches(self):
        """Bulk fetch should preserve sub-object caches"""
        class TestClass:
            def __init__(self):
                pass
            def get_course(self, course_id):
                obj = MagicMock()
                obj.id = course_id
                return obj
            def get_courses(self):
                obj = MagicMock()
                obj.id = 1
                return [obj]

        CacheGetMethods("course")(TestClass)
        instance = TestClass()

        # Initial bulk fetch
        courses = list(instance.get_courses())
        course = courses[0]

        # Add sub-cache (simulate assignment cache on course)
        course.assignment_cache = {10: ("assignment_obj", {})}
        course.assignment_all_fetched = datetime.now()
        instance.course_cache[1] = (course, {})

        # Force refetch by clearing all_fetched
        instance.course_all_fetched = None

        # Bulk fetch again
        new_courses = list(instance.get_courses())
        new_course = new_courses[0]

        # Sub-cache should be preserved
        assert hasattr(new_course, "assignment_cache")
        assert 10 in new_course.assignment_cache

    def test_copies_cache_attributes(self):
        """Should copy {attr}_cache attributes"""
        counter = {"val": 0}

        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                counter["val"] += 1
                # Return object with consistent ID
                obj = MagicMock()
                obj.id = 1
                return [obj]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        # Initial fetch
        items1 = list(instance.get_items())
        item1 = items1[0]
        item1.sub_cache = {"key": "value"}
        # Update cache with modified object
        instance.item_cache[1] = (item1, {})

        # Refetch - should copy sub_cache from old object
        instance.item_all_fetched = None
        items2 = list(instance.get_items())
        item2 = items2[0]

        assert hasattr(item2, "sub_cache")
        assert item2.sub_cache == {"key": "value"}

    def test_copies_all_fetched_attributes(self):
        """Should copy {attr}_all_fetched attributes"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                obj = MagicMock()
                obj.id = 1
                return [obj]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        # Initial fetch
        items1 = list(instance.get_items())
        item1 = items1[0]
        timestamp = datetime.now() - timedelta(hours=1)
        item1.sub_all_fetched = timestamp
        instance.item_cache[1] = (item1, {})

        # Refetch - should copy sub_all_fetched from old object
        instance.item_all_fetched = None
        items2 = list(instance.get_items())
        item2 = items2[0]

        assert hasattr(item2, "sub_all_fetched")
        assert item2.sub_all_fetched == timestamp

    def test_preserves_multiple_cache_types(self):
        """Should preserve all cache attributes (multiple types)"""
        class TestClass:
            def __init__(self):
                pass
            def get_course(self, course_id):
                obj = MagicMock()
                obj.id = course_id
                return obj
            def get_courses(self):
                obj = MagicMock()
                obj.id = 1
                return [obj]

        CacheGetMethods("course")(TestClass)
        instance = TestClass()

        # Initial fetch
        courses = list(instance.get_courses())
        course = courses[0]

        # Add multiple sub-caches
        course.assignment_cache = {10: ("asgn", {})}
        course.assignment_all_fetched = datetime.now()
        course.user_cache = {20: ("user", {})}
        course.user_all_fetched = datetime.now()
        instance.course_cache[1] = (course, {})

        # Refetch
        instance.course_all_fetched = None
        new_courses = list(instance.get_courses())
        new_course = new_courses[0]

        # All caches should be preserved
        assert hasattr(new_course, "assignment_cache")
        assert hasattr(new_course, "assignment_all_fetched")
        assert hasattr(new_course, "user_cache")
        assert hasattr(new_course, "user_all_fetched")

    def test_pattern_matching_works_for_any_cache(self):
        """Pattern matching should work for any {name}_cache"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                obj = MagicMock()
                obj.id = 1
                return [obj]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        # Initial fetch
        items1 = list(instance.get_items())
        item1 = items1[0]
        item1.custom_thing_cache = {"data": "value"}
        instance.item_cache[1] = (item1, {})

        # Refetch - should copy custom cache from old object
        instance.item_all_fetched = None
        items2 = list(instance.get_items())
        item2 = items2[0]

        # Custom cache should be preserved
        assert hasattr(item2, "custom_thing_cache")
        assert item2.custom_thing_cache == {"data": "value"}

    def test_fresh_objects_without_old_cache_work(self):
        """Fresh objects without old cache should get empty cache"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id
                return obj
            def get_items(self):
                # Return new object not in cache
                obj = MagicMock()
                obj.id = 99
                return [obj]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        # Fetch
        items = list(instance.get_items())

        # Should complete without error
        assert len(items) == 1
        assert items[0].id == 99

    def test_cache_copying_doesnt_interfere_with_new_data(self):
        """Cache copying shouldn't interfere with new API data"""
        class TestClass:
            def __init__(self):
                pass
            def get_item(self, item_id):
                obj = MagicMock()
                obj.id = item_id
                obj.name = f"Item {item_id}"
                return obj
            def get_items(self):
                obj = MagicMock()
                obj.id = 1
                obj.name = "Updated Item 1"
                return [obj]

        CacheGetMethods("item")(TestClass)
        instance = TestClass()

        # Initial fetch
        items = list(instance.get_items())
        assert items[0].name == "Updated Item 1"

        # Refetch
        instance.item_all_fetched = None
        new_items = list(instance.get_items())

        # New data should be visible
        assert new_items[0].name == "Updated Item 1"
@

<<test hacks.py>>=
<<test cache synchronization>>
@

Now let's test realistic Canvas API usage scenarios to verify the caching works
as expected in production-like workflows.

\subsubsection{Realistic Canvas API scenarios}

These tests verify that the caching system works correctly for typical Canvas API
usage patterns, particularly hierarchical data access (Course → Assignment → Submission).

<<test realistic canvas scenarios>>=
class TestRealisticCanvasScenarios:
    """Test realistic Canvas API usage patterns"""

    def test_course_assignment_hierarchy(self):
        """Course with cached assignments should work like real Canvas"""
        class Canvas:
            def get_course(self, course_id):
                obj = MagicMock()
                obj.id = course_id
                obj.name = f"Course {course_id}"
                return obj
            def get_courses(self):
                c1 = MagicMock()
                c1.id = 1
                c1.name = "Course 1"
                c2 = MagicMock()
                c2.id = 2
                c2.name = "Course 2"
                return [c1, c2]

        # Apply decorator
        CacheGetMethods("course")(Canvas)

        canvas = Canvas()

        # List courses (bulk fetch)
        courses = list(canvas.get_courses())
        assert len(courses) == 2

        # Verify caching works
        assert 1 in canvas.course_cache
        assert 2 in canvas.course_cache
        assert courses[0].name == "Course 1"
        assert courses[1].name == "Course 2"

    def test_multi_level_caching_with_includes(self):
        """Verify include parameters work with caching"""
        call_counts = {"fetch": 0}

        class Canvas:
            def get_course(self, course_id):
                obj = MagicMock()
                obj.id = course_id
                return obj
            def get_courses(self, **kwargs):
                call_counts["fetch"] += 1
                c = MagicMock()
                c.id = 1
                return [c]

        CacheGetMethods("course")(Canvas)

        canvas = Canvas()

        # First fetch with one include
        list(canvas.get_courses(include=["term"]))
        assert call_counts["fetch"] == 1

        # Request additional include - should refetch
        canvas.course_all_fetched = None
        list(canvas.get_courses(include=["term", "enrollment"]))

        # Course refetch should happen
        assert call_counts["fetch"] == 2

    def test_multiple_decorators_on_same_class(self):
        """Course with both assignment and user caches (like production)"""
        class Course:
            def get_assignment(self, assignment_id):
                obj = MagicMock()
                obj.id = assignment_id
                return obj
            def get_assignments(self):
                a = MagicMock()
                a.id = 10
                return [a]
            def get_user(self, user_id):
                obj = MagicMock()
                obj.id = user_id
                return obj
            def get_users(self):
                u = MagicMock()
                u.id = 100
                return [u]

        # Apply multiple decorators like production
        CacheGetMethods("assignment")(Course)
        CacheGetMethods("user")(Course)

        course = Course()

        # Should have separate caches
        assert hasattr(course, "assignment_cache")
        assert hasattr(course, "assignment_all_fetched")
        assert hasattr(course, "user_cache")
        assert hasattr(course, "user_all_fetched")

        # Both should work independently
        assignments = list(course.get_assignments())
        users = list(course.get_users())

        assert len(assignments) == 1
        assert len(users) == 1
        assert 10 in course.assignment_cache
        assert 100 in course.user_cache

    def test_production_workflow_no_redundant_api_calls(self):
        """Typical workflow should minimize API calls"""
        call_counts = {"get_courses": 0, "get_course": 0}

        class Canvas:
            def get_course(self, course_id):
                call_counts["get_course"] += 1
                obj = MagicMock()
                obj.id = course_id
                obj.name = f"Course {course_id}"
                return obj
            def get_courses(self):
                call_counts["get_courses"] += 1
                c1 = MagicMock()
                c1.id = 1
                c1.name = "Course 1"
                c2 = MagicMock()
                c2.id = 2
                c2.name = "Course 2"
                return [c1, c2]

        CacheGetMethods("course")(Canvas)

        canvas = Canvas()

        # Typical workflow: list all courses
        courses = list(canvas.get_courses())  # 1 API call

        # Now get individual course (should come from cache)
        course = canvas.get_course(1)  # 0 API calls (cached)

        # Verify minimal API calls
        assert call_counts["get_courses"] == 1
        assert call_counts["get_course"] == 0  # Came from cache!
        assert course.name == "Course 1"

    def test_irregular_plural_name_in_production(self):
        """group_categories uses custom plural_name like production"""
        class Course:
            def get_group_category(self, category_id):
                obj = MagicMock()
                obj.id = category_id
                return obj
            def get_group_categories(self):
                gc = MagicMock()
                gc.id = 1
                return [gc]

        CacheGetMethods("group_category",
                        plural_name="group_categories")(Course)

        course = Course()
        categories = list(course.get_group_categories())

        # Should create cache with singular form
        assert hasattr(course, "group_category_cache")
        assert hasattr(course, "group_category_all_fetched")
        assert 1 in course.group_category_cache

    def test_assignment_groups_include_singular_false(self):
        """assignment_groups decorator uses include_singular=False"""
        class Course:
            def get_assignment_groups(self):
                ag = MagicMock()
                ag.id = 1
                return [ag]

        CacheGetMethods("assignment_group",
                        include_singular=False)(Course)

        course = Course()

        # Should still have cache attributes
        assert hasattr(course, "assignment_group_cache")
        assert hasattr(course, "assignment_group_all_fetched")

        # Should NOT have get_assignment_group method (include_singular=False)
        # (but we can't easily test that a method wasn't added in this test structure)

        # Plural should work
        groups = list(course.get_assignment_groups())
        assert len(groups) == 1
        assert 1 in course.assignment_group_cache

    def test_fresh_data_not_refetched_unnecessarily(self):
        """Fresh cached data should not trigger redundant API calls"""
        call_count = {"count": 0}

        class Canvas:
            def get_course(self, course_id):
                obj = MagicMock()
                obj.id = course_id
                return obj
            def get_courses(self):
                call_count["count"] += 1
                c = MagicMock()
                c.id = 1
                c.name = "Test Course"
                return [c]

        CacheGetMethods("course")(Canvas)

        canvas = Canvas()

        # Initial fetch
        list(canvas.get_courses())
        assert call_count["count"] == 1

        # Multiple fetches from cache
        for _ in range(5):
            courses = list(canvas.get_courses())
            assert courses[0].name == "Test Course"

        # Should still only have 1 API call (rest from cache)
        assert call_count["count"] == 1
@

<<test hacks.py>>=
<<test realistic canvas scenarios>>
@

\subsection{Testing outdated objects}

Now we'll deal with the [[outdated]] function.
We simply test different attributes of the object.
If we find values indicating that we want to refresh it, we return [[True]].
<<function [[outdated]] to test if an object is outdated>>=
def outdated(obj):
  """Returns True if the object obj is outdated"""
  <<test if [[obj]] is outdated, return True if so>>
  return False
@

The obvious case is the grade on a submission.
Submissions with passing grades that can't be improved (A, P, P+, complete) are
cached forever.
Submissions without passing grades are cached with a 5-minute TTL, allowing
students to submit work and see updated grades relatively quickly without
overwhelming the Canvas API.
<<test if [[obj]] is outdated, return True if so>>=
try:
  if obj.grade not in NOREFRESH_GRADES:
    <<check if submission TTL has expired>>
except AttributeError:
  pass
@

For submissions without passing grades, we check if the [[SUBMISSION_TTL_MINUTES]]
TTL has expired.
If the object has no [[_fetched_at]] attribute, we treat it as outdated to
ensure it gets refreshed.
<<check if submission TTL has expired>>=
try:
  if datetime.now() - obj._fetched_at > timedelta(minutes=SUBMISSION_TTL_MINUTES):
    return True
except AttributeError:
  return True
@

Instinctively, we might want to use all passing grades as the grades that don't 
need refreshing.
However, even if a student has gotten a B, they might want to improve their 
grade.
So the only grades that we should use are passing grades that can't be
improved.
<<constants>>=
NOREFRESH_GRADES = ["A", "P", "P+", "complete"]
@

We define constants for all cache time-to-live (TTL) values to ensure
consistency between the [[outdated()]] function and log messages.
When these values are tuned for performance, both the cache behavior and
diagnostic messages update automatically.
<<constants>>=
# Cache TTL constants
SUBMISSION_TTL_MINUTES = 5
DEFAULT_CACHE_TTL_DAYS = 7
USER_CACHE_TTL_DAYS = 2
GROUP_CACHE_TTL_DAYS = 5
@

Another thing that we can do is to periodically reset the [[_all_fetched]] 
flag.
If we set it to [[None]], then we'll fetch all objects again.
So we can check this value to see if a sufficient amount of time has passed for 
us to reset it.
Note, however, that for this case we shouldn't return [[True]] or [[False]], 
since it's not the object itself that is outdated.
We don't need to refetch this object, just for it to refetch its children.
<<test if [[obj]] is outdated, return True if so>>=
for attr_name in dir(obj):
  <<reset [[_all_fetched]] if necessary>>
@

We want to periodically reset the [[_all_fetched]] date so that we periodically
try to refetch all data.
This interval will be different for different attributes.
But we add a default of [[DEFAULT_CACHE_TTL_DAYS]] days.

When periodic refresh timers expire, we reset the [[_all_fetched]] flag to
[[None]], triggering a bulk refresh on the next access. We log these
invalidations at INFO level to explain why subsequent bulk refreshes occur.
The log message includes both the cache age and the threshold, making it clear
why the invalidation was triggered.
<<reset [[_all_fetched]] if necessary>>=
<<if statements for resetting [[_all_fetched]] for various attributes>>
elif attr_name.endswith("_all_fetched"):
  if not getattr(obj, attr_name):
    continue
  elif datetime.now() - getattr(obj, attr_name) > timedelta(days=DEFAULT_CACHE_TTL_DAYS):
    age = datetime.now() - getattr(obj, attr_name)
    logger.info(f"Cache invalidation: {attr_name} periodic refresh "
                f"(age: {age.days} days, threshold: {DEFAULT_CACHE_TTL_DAYS} days)")
    setattr(obj, attr_name, None)
@

We don't want them to refresh at the same time, so we'll need to have the 
intervals coprime.
The default 7 is a prime, so everything will be coprime with it.

Students can be added every now and then.
So it will be useful to refetch them quite often.
For instance, they're added at the beginning of the course, might fail to
register and get removed, then readded when they're registered.
Also, when a student reregister for the course, that might happen at any time.
So a rather short interval of [[USER_CACHE_TTL_DAYS]] days is useful.
<<if statements for resetting [[_all_fetched]] for various attributes>>=
if attr_name == "user_all_fetched":
  if not getattr(obj, attr_name):
    continue
  elif datetime.now() - getattr(obj, attr_name) > timedelta(days=USER_CACHE_TTL_DAYS):
    age = datetime.now() - getattr(obj, attr_name)
    logger.info(f"Cache invalidation: user_all_fetched periodic refresh "
                f"(age: {age.days} days, threshold: {USER_CACHE_TTL_DAYS} days)")
    setattr(obj, attr_name, None)
@

Groups and group categories are more dynamic than assignment structure but less
dynamic than user enrollments.
Students are organized into groups for collaborative work, and group membership
can change during the course (though less frequently than enrollment changes).
We use [[GROUP_CACHE_TTL_DAYS]] days as a compromise between freshness and API
efficiency.
The prime number 5 ensures cache refreshes don't align with other attributes.
<<if statements for resetting [[_all_fetched]] for various attributes>>=
elif attr_name in ["group_all_fetched", "group_category_all_fetched"]:
  if not getattr(obj, attr_name):
    continue
  elif datetime.now() - getattr(obj, attr_name) > timedelta(days=GROUP_CACHE_TTL_DAYS):
    age = datetime.now() - getattr(obj, attr_name)
    logger.info(f"Cache invalidation: {attr_name} periodic refresh "
                f"(age: {age.days} days, threshold: {GROUP_CACHE_TTL_DAYS} days)")
    setattr(obj, attr_name, None)
@

The [[outdated]] function is used by the caching decorator to check if cached
objects need refreshing.
<<functions>>=
<<function [[outdated]] to test if an object is outdated>>
@

Now let's verify the [[outdated]] function works correctly for different
submission states and timestamps.

\subsubsection{Verifying [[outdated]]}

The [[outdated]] function determines when cached submissions should be
refreshed based on grade values and time-to-live.

<<test outdated>>=
class TestOutdated:
    """Test outdated() cache expiration logic"""

    def test_passing_grade_a_not_outdated(self):
        """Submissions with grade A should never be outdated"""
        submission = MagicMock()
        submission.grade = "A"
        submission._fetched_at = datetime.now() - timedelta(days=365)

        assert not outdated(submission)

    def test_passing_grade_p_not_outdated(self):
        """Submissions with grade P should never be outdated"""
        submission = MagicMock()
        submission.grade = "P"
        submission._fetched_at = datetime.now() - timedelta(days=100)

        assert not outdated(submission)

    def test_passing_grade_p_plus_not_outdated(self):
        """Submissions with grade P+ should never be outdated"""
        submission = MagicMock()
        submission.grade = "P+"
        submission._fetched_at = datetime.now() - timedelta(days=50)

        assert not outdated(submission)

    def test_passing_grade_complete_not_outdated(self):
        """Submissions with grade 'complete' should never be outdated"""
        submission = MagicMock()
        submission.grade = "complete"
        submission._fetched_at = datetime.now() - timedelta(days=30)

        assert not outdated(submission)

    def test_non_passing_grade_recent_not_outdated(self):
        """Recent non-passing grade (< 5 min) should not be outdated"""
        submission = MagicMock()
        submission.grade = "F"
        submission._fetched_at = datetime.now() - timedelta(minutes=4)

        assert not outdated(submission)

    def test_non_passing_grade_old_is_outdated(self):
        """Old non-passing grade (> 5 min) should be outdated"""
        submission = MagicMock()
        submission.grade = "B"
        submission._fetched_at = datetime.now() - timedelta(minutes=6)

        assert outdated(submission)

    def test_non_passing_grade_exactly_5min_not_outdated(self):
        """Exactly 5 minutes should not trigger update (not >5)"""
        submission = MagicMock()
        submission.grade = "C"
        # Set to just under 5 minutes to account for execution time
        submission._fetched_at = datetime.now() - timedelta(minutes=5, seconds=-1)

        assert not outdated(submission)

    def test_no_grade_attribute_not_outdated(self):
        """Objects without grade attribute should not be outdated"""
        obj = MagicMock(spec=[])  # No attributes

        assert not outdated(obj)

    def test_no_fetched_at_with_non_passing_grade_is_outdated(self):
        """Non-passing grade without _fetched_at should be outdated"""
        submission = MagicMock()
        submission.grade = "F"
        del submission._fetched_at

        assert outdated(submission)

    def test_none_grade_is_outdated(self):
        """Ungraded submissions (None grade) should refresh"""
        submission = MagicMock()
        submission.grade = None
        submission._fetched_at = datetime.now() - timedelta(minutes=6)

        assert outdated(submission)

    def test_empty_string_grade_is_outdated(self):
        """Empty grade string should refresh if old"""
        submission = MagicMock()
        submission.grade = ""
        submission._fetched_at = datetime.now() - timedelta(minutes=10)

        assert outdated(submission)

    def test_numeric_grade_is_outdated(self):
        """Numeric grades (improvable) should refresh if old"""
        submission = MagicMock()
        submission.grade = "85"
        submission._fetched_at = datetime.now() - timedelta(minutes=7)

        assert outdated(submission)
@

<<test hacks.py>>=
<<test outdated>>
@


\subsection{Diagnosing cache staleness}

To make cache behavior transparent and debuggable, we need to log not just
\emph{that} objects are stale, but \emph{why} they're stale. The [[outdated()]]
function implements three distinct staleness policies:
\begin{enumerate}
\item \textbf{Grade-based staleness}: Submissions without passing grades
  (A, P, P+, complete) are cached for only [[SUBMISSION_TTL_MINUTES]] minutes,
  allowing students to see updated grades quickly.
\item \textbf{Periodic refresh}: Each cached collection ([[_all_fetched]]
  timestamps) has a time-to-live ([[DEFAULT_CACHE_TTL_DAYS]] days default,
  [[USER_CACHE_TTL_DAYS]] days for users, [[GROUP_CACHE_TTL_DAYS]] days
  for groups) after which the entire collection is refetched.
\item \textbf{Missing metadata}: Objects without [[_fetched_at]] attributes
  are treated as stale to ensure they get refreshed.
\end{enumerate}

When we log cache operations, we want to distinguish between these cases.
The [[get_staleness_reason()]] function mirrors [[outdated()]]'s logic but
returns human-readable diagnostic strings instead of booleans.
Both functions reference the same TTL constants, ensuring that log messages
accurately describe the cache behavior.

<<function [[get_staleness_reason]] to diagnose staleness>>=
def get_staleness_reason(obj):
  """
  Returns a descriptive string explaining why obj is stale, or None if fresh.
  Mirrors the logic in outdated() but returns human-readable reasons.
  Uses constants for TTL values to ensure consistency with outdated() logic.
  """
  <<check submission grade-based staleness and return reason>>
  <<check periodic refresh staleness and return reason>>
  return None
@

We log the specific reason for staleness to help diagnose cache behavior.
This distinguishes between the three staleness policies and shows how long
objects have been stale, making it clear why refreshes are occurring.

For grade-based staleness, we check if the submission has a non-passing grade
and if the TTL has expired:
<<check submission grade-based staleness and return reason>>=
try:
  if obj.grade not in NOREFRESH_GRADES:
    try:
      age = datetime.now() - obj._fetched_at
      ttl = timedelta(minutes=SUBMISSION_TTL_MINUTES)
      if age > ttl:
        expired_by = age - ttl
        return (f"non-passing grade ({obj.grade}), "
                f"{SUBMISSION_TTL_MINUTES}-minute TTL expired "
                f"{expired_by.total_seconds()/60:.1f} minutes ago")
    except AttributeError:
      return "non-passing grade, missing _fetched_at attribute"
except AttributeError:
  pass
@

For periodic refresh, we check the [[_all_fetched]] timestamps and report
which specific attribute triggered the refresh:
<<check periodic refresh staleness and return reason>>=
for attr_name in dir(obj):
  if attr_name == "user_all_fetched":
    attr_val = getattr(obj, attr_name, None)
    if attr_val:
      age = datetime.now() - attr_val
      threshold = timedelta(days=USER_CACHE_TTL_DAYS)
      if age > threshold:
        return (f"periodic refresh: {attr_name} "
                f"(age: {age.days} days, threshold: {USER_CACHE_TTL_DAYS} days)")
  elif attr_name in ["group_all_fetched", "group_category_all_fetched"]:
    attr_val = getattr(obj, attr_name, None)
    if attr_val:
      age = datetime.now() - attr_val
      threshold = timedelta(days=GROUP_CACHE_TTL_DAYS)
      if age > threshold:
        return (f"periodic refresh: {attr_name} "
                f"(age: {age.days} days, threshold: {GROUP_CACHE_TTL_DAYS} days)")
  elif attr_name.endswith("_all_fetched"):
    attr_val = getattr(obj, attr_name, None)
    if attr_val:
      age = datetime.now() - attr_val
      threshold = timedelta(days=DEFAULT_CACHE_TTL_DAYS)
      if age > threshold:
        return (f"periodic refresh: {attr_name} "
                f"(age: {age.days} days, threshold: {DEFAULT_CACHE_TTL_DAYS} days)")
@

<<functions>>=
<<function [[get_staleness_reason]] to diagnose staleness>>
@


\subsection{Caching courses}

The list of courses changes whenever new courses are created that we have
access to.
This usually happens well in advance, so we could have a rather long
time-to-live on the courses cache.

We use the [[CacheGetMethods]] decorator to add caching to the [[Canvas]] class
for its [[get_course]] and [[get_courses]] methods.
The decorator takes the attribute name [[course]] as argument.
<<functions>>=
def make_canvas_courses_cacheable():
  import canvasapi.canvas
  canvasapi.canvas.Canvas = CacheGetMethods("course")(canvasapi.canvas.Canvas)
@

Note that we don't need a TTLCache here, since the [[outdated]] function will
periodically reset the [[_all_fetched]] flag to trigger a refetch.


\subsection{Caching course contents}

Each course has assignments and users (students, teachers, TAs).
These change relatively infrequently:
Assignments are typically created at the beginning of the course and rarely
change after that.
Users are added at the beginning and occasionally during the course.

We use [[CacheGetMethods]] to cache both [[assignment]] and [[user]] attributes
of the [[Course]] class.
<<functions>>=
def make_course_contents_cacheable():
  import canvasapi.course
  canvasapi.course.Course = CacheGetMethods("assignment")(
    canvasapi.course.Course)
  canvasapi.course.Course = CacheGetMethods("user")(
    canvasapi.course.Course)
@


\subsection{Caching course structure}

Courses have structural elements that change very rarely after the course is
set up: assignment groups, modules, group categories, and groups.
These are typically created at course design time and remain stable throughout
the course.

Assignment groups organize assignments into categories (e.g., "Labs",
"Assignments", "Exams").
They're used frequently for filtering but almost never change after setup.
The Canvas API only provides [[get_assignment_groups()]] (plural), not a
singular [[get_assignment_group()]] method.
We use [[CacheGetMethods]] with [[include_singular=False]] to cache only the
plural method.
<<functions>>=
def make_course_assignment_groups_cacheable():
  import canvasapi.course
  canvasapi.course.Course = CacheGetMethods("assignment_group",
    include_singular=False)(canvasapi.course.Course)
@

Modules represent the learning structure of a course.
They're used for filtering assignments and rarely change after initial setup.
Similar to assignment groups, the Canvas API only provides [[get_modules()]]
(plural), so we cache with [[include_singular=False]].
<<functions>>=
def make_course_modules_cacheable():
  import canvasapi.course
  canvasapi.course.Course = CacheGetMethods("module",
    include_singular=False)(canvasapi.course.Course)
@

Group categories and groups organize students for collaborative work.
They're set up at course start or when group projects begin and change
infrequently.
The Canvas API only provides plural methods ([[get_group_categories()]],
[[get_groups()]]), not singular methods.
Note that groups can be fetched from both [[Course]] and [[GroupCategory]]
objects, so we cache on both.
For [[group_category]], the plural is irregular (``categories'' not
``categorys''), so we specify [[plural_name="group_categories"]].
<<functions>>=
def make_course_groups_cacheable():
  import canvasapi.course
  import canvasapi.group
  canvasapi.course.Course = CacheGetMethods("group_category",
    include_singular=False, plural_name="group_categories")(
    canvasapi.course.Course)
  canvasapi.course.Course = CacheGetMethods("group",
    include_singular=False)(canvasapi.course.Course)
  canvasapi.group.GroupCategory = CacheGetMethods("group",
    include_singular=False)(canvasapi.group.GroupCategory)
@


\subsection{Caching submissions}

For the results, we can construct something more specific.
Results that we're interested in are submissions.
We can only get submission from an assignment object.
We can either get all submissions, or one specific submission if we specify the 
user.
<<functions>>=
def make_assignment_submissions_cacheable():
  def cache_submissions(cls):
    """Class decorator for cacheable get_submission, get_submissions methods"""
    <<decorator body for caching assignment submissions>>
    return cls

  canvasapi.assignment.Assignment = \
    cache_submissions(canvasapi.assignment.Assignment)
@

Then we can write the decorator as follows.
We need to add a cache attribute in the constructor, so we must decorate the 
constructor.
The we must decorate both [[get_submission]] and [[get_submissions]].
<<decorator body for caching assignment submissions>>=
old_constructor = cls.__init__

@functools.wraps(cls.__init__)
def new_init(self, *args, **kwargs):
  <<extend class constructor for decorators>>
  old_constructor(self, *args, **kwargs)

cls.__init__ = new_init

get_submission = cls.get_submission

@functools.wraps(cls.get_submission)
def new_get_submission(self, user, **kwargs):
  <<return submission of user>>

cls.get_submission = new_get_submission

get_submissions = cls.get_submissions

@functools.wraps(cls.get_submissions)
def new_get_submissions(self, *args, **kwargs):
  <<return a list of all submissions>>

cls.get_submissions = new_get_submissions
@

We decorate the method to get a specific submission, [[get_submission]].
This way we can cache the submissions of users who have passed.
Then we always return up-to-date submissions of students who are expected to 
update their submissions.
For this, we first need a cache attribute.
<<extend class constructor for decorators>>=
self.__cache = {}
@

Let's start with how we get an individual submission.
We also want to add info messages to log cache hits and misses and their 
timings.
<<return submission of user>>=
# canvasapi allows either User object or user ID.
if isinstance(user, User):
  uid = user.id
elif isinstance(user, int):
  uid = user
else:
  raise TypeError(f"user must be User or int")

submission = None

if "include" in kwargs:
  to_include = set(kwargs["include"])
else:
  to_include = set()

if must_refresh(uid, to_include, self.__cache):
  fetch_start = time.perf_counter()
  submission = get_submission(self, user, include=list(to_include))
  fetch_elapsed = time.perf_counter() - fetch_start
  submission._fetched_at = datetime.now()
  logger.info(f"Fetched submission user_id={uid} in {fetch_elapsed:.2f}s")
  self.__cache[uid] = (submission, to_include)
else:
  submission, _ = self.__cache[uid]

return submission
@ We also add the time of last fetch ([[._fetched_at]]), that way we don't need 
to update all the time, but with short intervals (see the [[outdated]] 
function).
<<functions>>=
def must_refresh(uid, to_include, cache):
  """
  Returns True if the submission needs refreshing based on kwargs.
  """
  cache_status = ""
  if uid in cache:
    submission, included = cache[uid]
    if not to_include.issubset(set(included)):
      cache_status = " (new include required)"
      submission = None
      to_include |= set(included)
  else:
    cache_status = " (not found)"

  if submission:
    if not outdated(submission):
      logger.info(f"Cache hit: submission user_id={uid}")
      return False
    elif not cache_status:
      reason = get_staleness_reason(submission)
      cache_status = f" (stale: {reason})" if reason else " (stale)"

  logger.info(f"Cache miss{cache_status}: submission user_id={uid}")
  return True
@

Now we can deal with [[get_submissions]].
As we might call [[get_submission]] before any [[get_submissions]], we cannot 
rely on the cache as a check.
We introduce a new attribute.
<<extend class constructor for decorators>>=
self.__all_fetched = None
@ Now we can check if this is set or not.
When we fetch, we want to include any data that was previously included.
<<return a list of all submissions>>=
if "include" in kwargs:
  to_include = set(kwargs["include"])
else:
  to_include = set()

# Collect everything to include from before.
for _, included in self.__cache.values():
  to_include |= included

if self.__all_fetched:
  cache_age = datetime.now() - self.__all_fetched
  logger.info(f"Using cached submissions for assignment id={self.id}: "
              f"{len(self.__cache)} submissions cached "
              f"(age: {cache_age.total_seconds()/60:.1f} minutes), "
              f"checking staleness to decide refresh strategy")
  num_needs_refresh = 0
  for submission, included in self.__cache.values():
    if must_refresh(submission.user_id, to_include, self.__cache):
      num_needs_refresh += 1
      if num_needs_refresh > THRESHOLD_INDIVIDUAL_REFRESH:
        break

  if num_needs_refresh <= THRESHOLD_INDIVIDUAL_REFRESH:
    logger.info(f"Refreshing {num_needs_refresh} individual submissions "
                f"for assignment id={self.id}")
    for submission, _ in self.__cache.values():
      # Use the cached method above to trigger any individual refreshes.
      self.get_submission(submission.user_id, include=list(to_include))
  else:
    logger.info(f"Refreshing all submissions for assignment id={self.id} "
                f"due to {num_needs_refresh} needing refresh")
    <<bulk refresh all submissions>>
else:

  logger.info(f"Fetching all submissions for assignment id={self.id}")
  <<bulk refresh all submissions>>

return [submission for submission, _ in self.__cache.values()]
<<bulk refresh all submissions>>=
start_fetch = time.perf_counter()
submissions = list(get_submissions(self, include=list(to_include)))
fetch_elapsed = time.perf_counter() - start_fetch
logger.info(f"Bulk refresh: {len(submissions)} submissions in "
            f"{fetch_elapsed:.2f}s")

for submission in submissions:
  submission._fetched_at = datetime.now()
  self.__cache[submission.user_id] = (submission, to_include)

self.__all_fetched = datetime.now()
@

Based on some tests, it takes Canvas roughly \qty{3}{\second} to return all 
submissions.
It takes Canvas roughly \qty{0.5}{\second} to return an individual submission.
So if we have more than 6 submissions to refresh, it's better to refresh them 
all at once.
<<constants>>=
THRESHOLD_INDIVIDUAL_REFRESH = 6
@

