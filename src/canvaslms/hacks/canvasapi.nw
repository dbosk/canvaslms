\chapter{Hackish improvements to the \texttt{canvasapi} module}

In this module we provide some decorators for the classes in the 
\texttt{canvasapi} package.
We automatically apply all decorators upon import, so
\begin{minted}{python}
import canvaslms.hacks.canvasapi
\end{minted}
would apply all decorators defined herein to the already defined classes in the 
\texttt{canvasapi} package.

We do this as follows:
<<canvasapi.py>>=
"""A module that modifies the classes of the canvasapi package"""

import cachetools
import canvasapi.assignment
from canvasapi.user import User
from datetime import datetime, timedelta
import functools
import importlib
import inspect
import logging
import os
import time
import sys

logger = logging.getLogger(__name__)

<<constants>>
<<functions>>

# Loads all hacks
this_module = sys.modules[__name__]

# automatically execute all make_* functions to apply decorators
for name, function in inspect.getmembers(this_module, inspect.isfunction):
  if name.startswith("make_"):
    function()
@


\section{Make classes comparable and hashable}\label{ComparableObjects}

Since none of the classes in [[canvasapi]] defines the [[__eq__]] method, they 
all use the default which uses [[is]].
However, in many cases, it makes more sense to actually compare what the 
objects represent.
Consider two [[User]] objects that represent the same user (the same Canvas 
ID), then they should be considered equal, even if the objects themselves are 
different.
<<define decorator for comparable Canvas objects>>=
def canvas_comparable(cls):
  def is_equal(self, other):
    """Tests if Canvas objects self and other refer to the same object"""
    return type(self) == type(other) and self.id == other.id

  cls.__eq__ = is_equal
  return cls
@

The same applies for the hashable property.
A [[User]] object represents a fixed user that never changes, so we can use the 
type and Canvas ID to hash objects.
<<define decorator for hashable Canvas objects>>=
def canvas_hashable(cls):
  def canvas_hash(self):
    """Returns a hash suitable for Canvas objects"""
    return hash(type(self)) ^ hash(self.id)

  cls.__hash__ = canvas_hash
  return cls
@

Adding these two, will allow us to put these objects into sets, for instance.
We sum it up in a function that can be run automatically when including this 
module.
<<functions>>=
def make_classes_comparable():
  """Improves the classes by adding __eq__ and __hash__ methods"""
  <<define decorator for comparable Canvas objects>>
  <<define decorator for hashable Canvas objects>>
  <<improve eq method for classes>>
@

We want to do this for several classes.
<<improve eq method for classes>>=
# classes to improve in each module
CANVASAPI_CLASSES = {
  "assignment": ["Assignment", "AssignmentGroup"],
  "submission": ["Submission"],
  "user": ["User"],
  "group": ["GroupCategory", "Group"],
  "module": ["Module"]
}
@

We then want to load all the relevant modules given above.
<<improve eq method for classes>>=
canvasapi_modules = {}

# import all modules
for module_name in CANVASAPI_CLASSES:
  canvasapi_modules[module_name] = \
    importlib.import_module(f"canvasapi.{module_name}")
@

Finally, we can go through all the modules and extract their members.
For each member, we check if it's a member to decorate, if so, we apply the 
decorators~[[canvas_comparable]] and [[canvas_hashable]] to it.
<<improve eq method for classes>>=
for module_name, module in canvasapi_modules.items():
  module_members = inspect.getmembers(module)
  for obj_name, obj in module_members:
    if obj_name in CANVASAPI_CLASSES[module_name]:
      canvas_comparable(obj)
      canvas_hashable(obj)
@


\section{Improve User's [[__str__]] method}

By default, [[canvasapi]]'s [[User]] class defines a [[__str__]] dunder method 
that uses the user's name and Canvas ID.
We want to make it more useful, by using the user's name and login ID.
<<functions>>=
def make_useful_user_dunder_str():
  """Improves the user class by changing __str__"""
  <<define [[name_and_login]]>>
  <<update [[User.__str__]] to use [[name_and_login]]>>
@

Now, we simply need to define a function to use as a drop-in replacement for 
the [[__str__]] method.
<<define [[name_and_login]]>>=
def name_and_login(self):
  try:
    return f"{self.name} <{self.login_id}>"
  except AttributeError as err:
    return f"{self.name} <>"
@

Then we simply need to replace the current [[__str__]] method with the new one 
above.
<<update [[User.__str__]] to use [[name_and_login]]>>=
import canvasapi.user
canvasapi.user.User.__str__ = name_and_login
@


\section{Cacheable Canvas objects}

We would like certain methods in certain Canvas objects to be cached.
Particularly, methods that return objects that rarely change should be 
cacheable.
For instance, [[canvas.get_courses()]] returns a list of courses.
This list changes rarely\footnote{%
  This happens four times per year, when IT creates the new Canvas rooms 270 
  days ahead of course start.
  It also happens when someone adds you to manually to a course.
}, so its results can and should be cached.
The list of students changes usually only once in the beginning of the course.

Results on the other hand changes during a course.
However, once a student receives a pass, the result will (usually) not change 
again.
There are these cases however, where a student complements their submission to 
get a higher grade.
But once the student has the highest grade, an A or a P, the result will not 
change.

We will use the [[cachetools]] package to provide suitable caching decorators 
for the [[canvasapi]] classes' methods.
However, in some cases we can't, but need to write our own caching mechanisms.
But this means that we can construct quite complex cache policies.

The general design is this:
We add a cache attribute, then the methods store objects (return values) there.
The [[Canvas]] object stores [[Course]] objects.
Each [[Course]] object stores [[Assignment]] objects.
Each [[Assignment]] object stores [[Submission]] objects.
This means we can store and restore the entire hierarchy just by storing the 
[[Canvas]] object.
If we pickle the [[Canvas]] object, the caches of the [[Course]] objects will 
be in there, the [[Assignment]] caches will in turn be in them, and so on.

When we want to force an update of the courses, we don't want to clear the 
cache.
Then we lose the entire hierarchy.
We can update the list instead of clearing the entire cache hierarchy.
That way we can update objects and keep their caches.
But we'll need a way to know when to update.

\subsection{Syncing caches}

We usually have two methods to cache:
\begin{center}
[[get_x(id, /, **kwargs)]]
\quad
and
\quad
[[get_xs(**kwargs)]].
\end{center}
For example,
\begin{center}
[[get_submission(id, /, **kwargs)]]
\quad
and
\quad
[[get_submissions(**kwargs)]].
\end{center}
These should sync.
In Canvas we can also pass options through keyword arguments, a common one 
being [[include]] which specifies a list of things to include.
For instance, when it comes to submissions we can ask Canvas to include grading 
rubrics.
This means that we want to maintain these details, so that we don't lose 
information in the cache.

This means that we need two functions to support the caching:
\begin{description}
\item [[must_update(prev_kwargs, kwargs)]] takes the old keyword 
arguments ([[prev_kwargs]]) to check if they're a superset of the new keyword 
arguments ([[kwargs]]).
If not, the cache doesn't have all necessary information and we must fetch it.
\item [[merge_kwargs]] will take a list of keyword arguments dictionaries and 
merge them.
Most of the time these contain only the key [[include]] whose value is a list 
of things to include.
So essentially, we want to take the union of all these sets.
\end{description}
We can thus start with the following defaults:
<<kwargs functions caches>>=
def must_update(prev_kwargs, new_kwargs,
                ignore_keys=[
                  <<keys to ignore>>
                ]):
  """
  Returns True if we must update the cache (refetch).

  <<doc for keys to ignore>>
  """
  for key, value in new_kwargs.items():
    if key not in prev_kwargs:
      return True
    elif isinstance(value, list):
      if set(value) > set(prev_kwargs[key]):
        return True
    elif value != prev_kwargs[key]:
      return True

  return False

def merge_kwargs(kwargs_list,
                 ignore_keys=[
                   <<keys to ignore>>
                 ]):
  """
  Merges a list of keyword arguments dictionaries. Lists are unioned.
  All non-list keys (usually strings) must be the same in all dictionaries.

  <<doc for keys to ignore>>
  """
  new_kwargs = dict()

  for kwargs in kwargs_list:
    for key, value in kwargs.items():
      if key not in new_kwargs:
        new_kwargs[key] = value
      elif isinstance(value, list) or isinstance(new_kwargs[key], list):
        # Convert both to lists if either is a list, then union
        prev_val = new_kwargs[key] if isinstance(new_kwargs[key], list) \
                   else [new_kwargs[key]]
        curr_val = value if isinstance(value, list) else [value]
        new_kwargs[key] = list(set(prev_val) | set(curr_val))
      else:
        <<if [[key]] can be ignored>>:
          new_kwargs[key] = value
        elif value != new_kwargs[key]:
          raise ValueError(f"Cannot merge {key} with "
                           f"{value} and {new_kwargs[key]}")

  return new_kwargs
@

We can now define the keys to ignore.
There are some keys that can be ignored for our purposes, like sorting order, 
as they don't affect the caching.
In those cases, we simply use the last value, as seen in the loop above.
<<doc for keys to ignore>>=
By default, we ignore the keys

  <<keys to ignore>>

as they don't affect the caching.
<<keys to ignore>>=
"sort",
"order",
"order_by"
@

To check if a key can be ignored, we simply check if it's in the [[ignore_keys]]
list.
<<if [[key]] can be ignored>>=
if key in ignore_keys
@

These utility functions are part of the caching infrastructure.
<<functions>>=
<<kwargs functions caches>>
@

\subsection{Wrapping the methods with a caching decorator}

Now, let's turn our attention to the caching decorator.
We want to cover both the singular and plural methods.
So this decorator should be applied to the class, for instance the 
[[Assignment]] class with the attribute [[submission]] to cache
[[get_submission]] and [[get_submissions]].
<<general class decorator for caching get methods>>=
class CacheGetMethods:
  """
  General class decorator to add caching to get_*{,s} methods.

  <<notes in [[CacheGetMethods]] doc>>
  """
  def __init__(self, attribute_name, cache=None, include_plural=True,
               include_singular=True, plural_name=None):
    """No parameters required"""
    self.__attribute_name = attribute_name
    self.__include_plural = include_plural
    self.__include_singular = include_singular
    self.__plural_name = plural_name
    self.__cache = cache if cache else {}

  def __call__(self, cls):
    """Applies the decorator to the class cls"""
    <<create decorator functions and update [[cls]]>>
    return cls
@ If we want to cache several attributes, then we can simply apply this decorator
several times.
For instance, the [[Course]] class has both [[assignments]] and [[users]].

Some Canvas API entities only provide plural methods (e.g.,
[[get_assignment_groups()]] but no [[get_assignment_group()]]).
For these, we use [[include_singular=False]].

We need to update two methods in sync, the [[get_x]] and [[get_xs]] methods.
This is why we can't just use the decorators from [[cachetools]] or similar.
<<create decorator functions and update [[cls]]>>=
<<update constructor with new attributes>>

if self.__include_singular:
  <<update the singular method>>

if self.__include_plural:
  <<update the plural method>>
@

We must update the constructor so that the object that is created keeps the 
cache.
To update the constructor, we simply wrap it in a new constructor that simply 
adds the desired attributes.

Note that the cache stored in the constructor must be named specifically to the
attribute at hand.
Otherwise, when we cache several attributes, they will all use the same cache
named [[cache]].
This will not work.
So they must be named [[submission_cache]], for example.

We capture [[attr_name]] as a closure variable on line~358.
This is crucial for correct decorator behavior when multiple decorators are
applied to the same class.
We must \emph{not} store [[attr_name]] as an instance attribute (like
[[self.attr_name]]) because that would be shared across all decorator
applications.
For example, if we decorate [[Course]] with both [[CacheGetMethods("assignment")]]
and [[CacheGetMethods("user")]], storing as [[self.attr_name]] would cause the
second decorator to overwrite the first decorator's value, making both use
``user'' incorrectly.
By using the closure variable [[attr_name]], each decorator captures its own
value that remains constant throughout the wrapper functions' lifetime.
<<update constructor with new attributes>>=
init = cls.__init__
attr_name = self.__attribute_name

@functools.wraps(init)
def new_init(*args, **kwargs):
  self = args[0]
  <<construct [[attr_name]] cache attributes in [[self]]>>
  init(*args, **kwargs)

cls.__init__ = new_init
@

We do the same thing for the singular and plural get methods.
We get the original method, then wrap it in a new method that adds suitable 
caching or falls back to the original [[get_*]] function.
<<update the singular method>>=
singular_name = f"get_{self.__attribute_name}"
get_attr = getattr(cls, singular_name)

@functools.wraps(get_attr)
def new_get_attr(self, *args, **kwargs):
  <<return cached result or fetch from [[get_attr]]>>

setattr(cls, singular_name, new_get_attr)
<<update the plural method>>=
if self.__plural_name:
  plural_name = f"get_{self.__plural_name}"
else:
  plural_name = f"get_{self.__attribute_name}s"
get_attrs = getattr(cls, plural_name)

@functools.wraps(get_attrs)
def new_get_attrs(self, *args, **kwargs):
  <<return cached result or fetch from [[get_attrs]]>>

setattr(cls, plural_name, new_get_attrs)
@

\subsection{Caching the get methods}

The idea of the caching is as follows.
If the plural method ([[get_xs]]) is called, we fetch all items if they haven't 
been fetched before.
If the singular method ([[get_x]]) is called, we check for the request in the 
cache, otherwise we fetch it.
Each request can specify that Canvas should include extra data.
We don't want to lose data, so when making a new call, we first check what data 
has been fetched, so that we never lose data from the cache.

To be able to handle caching, we need attributes to keep track of the cache and 
if we've fetched all objects.
Even if we've fetched all objects, there might be new objects added at times.
So periodically, we must refetch all objects even if we've previously fetched 
them---but only when a sufficient amount of time has passed.
So we'll let the [[_all_fetched]] attribute be a [[datetime]] object, or 
[[None]] if it was never fetched.
<<construct [[attr_name]] cache attributes in [[self]]>>=
if not hasattr(self, f"{attr_name}_cache"):
  setattr(self, f"{attr_name}_cache", {})
if not hasattr(self, f"{attr_name}_all_fetched"):
  setattr(self, f"{attr_name}_all_fetched", None)
@

In all [[get_attr]] requests, it's the positional arguments ([[*args]]) that 
identify what to get.
The keyword arguments ([[**kwargs]]) specify additional options.
However, in most cases the [[*args]] will only contain one element, and that is 
an ID.
<<notes in [[CacheGetMethods]] doc>>=
We assume that the first positional argument is the ID of the object to fetch.
This must be the same as the `.id` attribute of an object (`obj.id`).

Parameters:
  attribute_name: The attribute name to cache (e.g., "assignment", "user").
  cache: Optional initial cache dictionary (default: {}).
  include_plural: Whether to cache the plural method get_*s (default: True).
  include_singular: Whether to cache the singular method get_* (default: True).
  plural_name: Custom plural name for irregular plurals (default: None, uses
               attribute_name + "s"). For example, "group_categories" instead
               of "group_categorys".
@

This allows us to specify how the cache is structured.
We'll let the cache be a dictionary where the key is the ID of the object.
The value will be a tuple of the object and the keyword arguments used to fetch 
it.
For example:
\begin{minted}{text}
{
  1: (obj1, kwargs1),
  2: (obj2, kwargs2),
  ...
}
\end{minted}
<<return cached result or fetch from [[get_attr]]>>=
import time

attr_cache = getattr(self, f"{attr_name}_cache")

<<let [[id]] be the ID of the object to fetch>>

try:
  obj, prev_kwargs = attr_cache[id]
  cache_status = ""
except KeyError:
  obj = None
  prev_kwargs = {}
  cache_status = " (not found)"

if obj and must_update(prev_kwargs, kwargs):
  obj = None
  cache_status = " (new kwargs required)"
elif obj and outdated(obj):
  obj = None
  cache_status = " (stale)"

if not obj:
  fetch_start = time.perf_counter()
  obj = get_attr(self, *args, **kwargs)
  fetch_elapsed = time.perf_counter() - fetch_start
  obj._fetched_at = datetime.now()
  attr_cache[obj.id] = (obj, kwargs)
  logger.info(f"Cache miss{cache_status}: {attr_name} id={id} fetched in "
              f"{fetch_elapsed:.2f}s")
else:
  logger.info(f"Cache hit: {attr_name} id={id}")

return obj
@

We log cache hits and misses to measure caching effectiveness.
Cache hits should be nearly instantaneous (microseconds), as we're just
returning a cached object.
Cache misses require API calls to Canvas, which typically take hundreds of
milliseconds to seconds depending on network latency and server response time.
This timing data helps identify whether caching is providing the expected
performance benefits.

We already saw [[must_update]] above.
That one focuses on the keyword arguments.
We'll return to the [[outdated]] function later, that one determines from the
object itself if it needs updating.

The [[get_*]] methods all take either an ID or an object as argument.
If it's an object, then we can get the ID from that object.
<<let [[id]] be the ID of the object to fetch>>=
try:
  obj = args[0]
  id = obj.id
except IndexError:
  raise TypeError(f"{singular_name}() missing 1 required positional "
                  f"argument: 'id'")
except AttributeError:
  if isinstance(obj, int):
    id = obj
  else:
    raise TypeError(f"{singular_name}() argument 1 must be int or "
                    f"Canvas object, not {type(obj)}")
@

Now, let's turn to the plural method.
We have the obvious case that we must fetch all if we haven't fetched all 
before.
But after this it gets more interesting.
If we have fetched all, we must check if the previous [[kwargs]] are a superset
of the new [[kwargs]].
If not, we must fetch all again with the extended [[kwargs]].
However, even if we can reuse the cache, we must check each object if it
requires updating.
For instance, if it's a submission and the grade is not P, then we must update
it.

The key logic is that we have two distinct paths:
\begin{enumerate}
\item \textbf{Bulk fetch path} ([[if not attr_all_fetched]]): When cache is
  empty or stale, we do one bulk API call to fetch all objects and populate the
  cache.
  We then yield all objects without individual staleness checks.
  This is efficient because all objects are fresh from the bulk fetch.
\item \textbf{Cached path} ([[else]]): When cache is already populated and
  fresh, we yield objects from cache, checking each for staleness.
  If an object is outdated (e.g., submission without passing grade older than 5
  minutes), we fetch that individual object.
  This avoids refetching everything when only a few items need updating.
\end{enumerate}
The [[else]] is critical to avoid the double-yield bug: without it, we would
yield all objects once from the bulk fetch, then yield them all again while
checking for staleness, causing N individual API calls for outdated items
immediately after a bulk fetch.
<<return cached result or fetch from [[get_attrs]]>>=
attr_cache = getattr(self, f"{attr_name}_cache")
attr_all_fetched = getattr(self, f"{attr_name}_all_fetched")

if attr_all_fetched:
  for _, prev_kwargs in attr_cache.values():
    if must_update(prev_kwargs, kwargs):
      attr_all_fetched = None
      break

if not attr_all_fetched:
  <<fetch all and populate [[attr_cache]]>>
  <<yield all objects in [[attr_cache]]>>
else:
  <<yield all objects in [[attr_cache]], update individual entries if needed>>
@

To fetch all again, we must merge the keyword arguments.
This is done with the [[merge_kwargs]] function.
Then we must add each object to the cache.

We separately time the API call versus processing to identify bottlenecks.
The API call time represents network latency and Canvas server response time,
while processing time represents local cache updates and object manipulation.
This helps distinguish between network bottlenecks and local processing
overhead.
<<fetch all and populate [[attr_cache]]>>=
import time
bulk_start = time.perf_counter()

union_kwargs = merge_kwargs(
  [kwargs for _, kwargs in attr_cache.values()] + [kwargs])

api_start = time.perf_counter()
fetched_objs = list(get_attrs(self, *args, **union_kwargs))
api_elapsed = time.perf_counter() - api_start

process_start = time.perf_counter()
for obj in fetched_objs:
  old_obj = attr_cache.get(obj.id, None)
  <<update [[obj]] with cache from [[old_obj]]>>
  obj._fetched_at = datetime.now()
  attr_cache[obj.id] = (obj, union_kwargs)
process_elapsed = time.perf_counter() - process_start

setattr(self, f"{attr_name}_all_fetched", datetime.now())
attr_all_fetched = getattr(self, f"{attr_name}_all_fetched")

bulk_elapsed = time.perf_counter() - bulk_start
logger.info(f"Bulk refresh: {len(fetched_objs)} {attr_name}s in "
            f"{bulk_elapsed:.2f}s (API: {api_elapsed:.2f}s, "
            f"processing: {process_elapsed:.2f}s)")
@

Finally, when we want to return all the objects, we will yield them instead.
It's more efficient to use a generator in this case, as that would make us
update when (if) needed.
When we update an object, we should use the same keyword arguments as specified
in the cache---not the requested ones---because these will, at this point, be
the largest possible set of keyword arguments.

We log individual refresh timing to show the cost of updating stale cache
entries without a full bulk fetch.
This provides contrast with bulk refresh performance: individual refreshes make
one API call per outdated object, while bulk refresh makes a single API call
for all objects.
The timing data reveals when the bulk refresh threshold optimization is
beneficial.
<<yield all objects in [[attr_cache]], update individual entries if needed>>=
for obj, obj_kwargs in attr_cache.values():
  if outdated(obj):
    refresh_start = time.perf_counter()
    obj = get_attr(self, obj.id, **obj_kwargs)
    refresh_elapsed = time.perf_counter() - refresh_start
    logger.info(f"Individual refresh: {attr_name} id={obj.id} in "
                f"{refresh_elapsed:.2f}s")
  yield obj
<<yield all objects in [[attr_cache]]>>=
for obj, _ in attr_cache.values():
  yield obj
@

This completes the [[CacheGetMethods]] class decorator, which we add to the module's
functions.
<<functions>>=
<<general class decorator for caching get methods>>
@

\subsection{Copy cache to new object}

When we update all objects, we want to keep the caches of every old object.
Otherwise, whenever we update all objects we lose all caches further down the 
hierarchy.
When we construct a new object, we want to copy the cache from the old object.

We can do this by finding all cache-related attributes and copying them.
The problem is that we don't know what attributes an object has caches for.
So we'll do some pattern matching on the attribute names.
We'll look for attributes that end with [[_cache]] and [[_all_fetched]].

Note that we use [[cache_attr_name]] as the loop variable, not [[attr_name]].
This is critical to avoid shadowing the closure variable [[attr_name]] that was
captured in line~371.
If we used [[attr_name]] as the loop variable, Python would treat it as a local
variable throughout the entire [[new_get_attrs]] function, causing an
[[UnboundLocalError]] when we try to access it before the loop (in
lines~494--495).
<<update [[obj]] with cache from [[old_obj]]>>=
for cache_attr_name in dir(old_obj):
  if cache_attr_name.endswith("_cache") or cache_attr_name.endswith("_all_fetched"):
    setattr(obj, cache_attr_name, getattr(old_obj, cache_attr_name))
@

\subsection{Testing outdated objects}

Now we'll deal with the [[outdated]] function.
We simply test different attributes of the object.
If we find values indicating that we want to refresh it, we return [[True]].
<<function [[outdated]] to test if an object is outdated>>=
def outdated(obj):
  """Returns True if the object obj is outdated"""
  <<test if [[obj]] is outdated, return True if so>>
  return False
@

The obvious case is the grade on a submission.
Submissions with passing grades that can't be improved (A, P, P+, complete) are
cached forever.
Submissions without passing grades are cached with a 5-minute TTL, allowing
students to submit work and see updated grades relatively quickly without
overwhelming the Canvas API.
<<test if [[obj]] is outdated, return True if so>>=
try:
  if obj.grade not in NOREFRESH_GRADES:
    <<check if submission TTL has expired>>
except AttributeError:
  pass
@

For submissions without passing grades, we check if the 5-minute TTL has
expired.
If the object has no [[_fetched_at]] attribute, we treat it as outdated to
ensure it gets refreshed.
<<check if submission TTL has expired>>=
try:
  if datetime.now() - obj._fetched_at > timedelta(minutes=5):
    return True
except AttributeError:
  return True
@

Instinctively, we might want to use all passing grades as the grades that don't 
need refreshing.
However, even if a student has gotten a B, they might want to improve their 
grade.
So the only grades that we should use are passing grades that can't be 
improved.
<<constants>>=
NOREFRESH_GRADES = ["A", "P", "P+", "complete"]

# Threshold for bulk refresh optimization
# Can be overridden via environment variables
BULK_REFRESH_THRESHOLD = float(os.environ.get('CANVASLMS_BULK_THRESHOLD', '0.2'))
BULK_REFRESH_MIN_COUNT = int(os.environ.get('CANVASLMS_BULK_MIN_COUNT', '3'))
@

Another thing that we can do is to periodically reset the [[_all_fetched]] 
flag.
If we set it to [[None]], then we'll fetch all objects again.
So we can check this value to see if a sufficient amount of time has passed for 
us to reset it.
Note, however, that for this case we shouldn't return [[True]] or [[False]], 
since it's not the object itself that is outdated.
We don't need to refetch this object, just for it to refetch its children.
<<test if [[obj]] is outdated, return True if so>>=
for attr_name in dir(obj):
  <<reset [[_all_fetched]] if necessary>>
@

We want to periodically reset the [[_all_fetched]] date so that we periodically 
try to refetch all data.
This interval will be different for different attributes.
But we add a default of 7 days.
<<reset [[_all_fetched]] if necessary>>=
<<if statements for resetting [[_all_fetched]] for various attributes>>
elif attr_name.endswith("_all_fetched"):
  if not getattr(obj, attr_name):
    continue
  elif datetime.now() - getattr(obj, attr_name) > timedelta(days=7):
    setattr(obj, attr_name, None)
@

We don't want them to refresh at the same time, so we'll need to have the 
intervals coprime.
The default 7 is a prime, so everything will be coprime with it.

Students can be added every now and then.
So it will be useful to refetch them quite often.
For instance, they're added at the beginning of the course, might fail to
register and get removed, then readded when they're registered.
Also, when a student reregister for the course, that might happen at any time.
So a rather short interval is useful.
<<if statements for resetting [[_all_fetched]] for various attributes>>=
if attr_name == "user_all_fetched":
  if not getattr(obj, attr_name):
    continue
  elif datetime.now() - getattr(obj, attr_name) > timedelta(days=2):
    setattr(obj, attr_name, None)
@

Groups and group categories are more dynamic than assignment structure but less
dynamic than user enrollments.
Students are organized into groups for collaborative work, and group membership
can change during the course (though less frequently than enrollment changes).
We use 5 days as a compromise between freshness and API efficiency.
The prime number 5 ensures cache refreshes don't align with other attributes.
<<if statements for resetting [[_all_fetched]] for various attributes>>=
elif attr_name in ["group_all_fetched", "group_category_all_fetched"]:
  if not getattr(obj, attr_name):
    continue
  elif datetime.now() - getattr(obj, attr_name) > timedelta(days=5):
    setattr(obj, attr_name, None)
@

The [[outdated]] function is used by the caching decorator to check if cached
objects need refreshing.
<<functions>>=
<<function [[outdated]] to test if an object is outdated>>
@


\subsection{Caching courses}

The list of courses changes whenever new courses are created that we have
access to.
This usually happens well in advance, so we could have a rather long
time-to-live on the courses cache.

We use the [[CacheGetMethods]] decorator to add caching to the [[Canvas]] class
for its [[get_course]] and [[get_courses]] methods.
The decorator takes the attribute name [[course]] as argument.
<<functions>>=
def make_canvas_courses_cacheable():
  import canvasapi.canvas
  canvasapi.canvas.Canvas = CacheGetMethods("course")(canvasapi.canvas.Canvas)
@

Note that we don't need a TTLCache here, since the [[outdated]] function will
periodically reset the [[_all_fetched]] flag to trigger a refetch.


\subsection{Caching course contents}

Each course has assignments and users (students, teachers, TAs).
These change relatively infrequently:
Assignments are typically created at the beginning of the course and rarely
change after that.
Users are added at the beginning and occasionally during the course.

We use [[CacheGetMethods]] to cache both [[assignment]] and [[user]] attributes
of the [[Course]] class.
<<functions>>=
def make_course_contents_cacheable():
  import canvasapi.course
  canvasapi.course.Course = CacheGetMethods("assignment")(
    canvasapi.course.Course)
  canvasapi.course.Course = CacheGetMethods("user")(
    canvasapi.course.Course)
@


\subsection{Caching course structure}

Courses have structural elements that change very rarely after the course is
set up: assignment groups, modules, group categories, and groups.
These are typically created at course design time and remain stable throughout
the course.

Assignment groups organize assignments into categories (e.g., "Labs",
"Assignments", "Exams").
They're used frequently for filtering but almost never change after setup.
The Canvas API only provides [[get_assignment_groups()]] (plural), not a
singular [[get_assignment_group()]] method.
We use [[CacheGetMethods]] with [[include_singular=False]] to cache only the
plural method.
<<functions>>=
def make_course_assignment_groups_cacheable():
  import canvasapi.course
  canvasapi.course.Course = CacheGetMethods("assignment_group",
    include_singular=False)(canvasapi.course.Course)
@

Modules represent the learning structure of a course.
They're used for filtering assignments and rarely change after initial setup.
Similar to assignment groups, the Canvas API only provides [[get_modules()]]
(plural), so we cache with [[include_singular=False]].
<<functions>>=
def make_course_modules_cacheable():
  import canvasapi.course
  canvasapi.course.Course = CacheGetMethods("module",
    include_singular=False)(canvasapi.course.Course)
@

Group categories and groups organize students for collaborative work.
They're set up at course start or when group projects begin and change
infrequently.
The Canvas API only provides plural methods ([[get_group_categories()]],
[[get_groups()]]), not singular methods.
Note that groups can be fetched from both [[Course]] and [[GroupCategory]]
objects, so we cache on both.
For [[group_category]], the plural is irregular (``categories'' not
``categorys''), so we specify [[plural_name="group_categories"]].
<<functions>>=
def make_course_groups_cacheable():
  import canvasapi.course
  import canvasapi.group
  canvasapi.course.Course = CacheGetMethods("group_category",
    include_singular=False, plural_name="group_categories")(
    canvasapi.course.Course)
  canvasapi.course.Course = CacheGetMethods("group",
    include_singular=False)(canvasapi.course.Course)
  canvasapi.group.GroupCategory = CacheGetMethods("group",
    include_singular=False)(canvasapi.group.GroupCategory)
@


\subsection{Lazy-loading submissions}

The current caching approach has an inefficiency: when we call
[[get_submissions()]], we immediately check if each cached submission needs
updating and refresh it if necessary.
However, in many workflows we filter the submission list to access only a few
submissions (e.g., filtering by user or assignment).
This means we waste API calls refreshing submissions we never actually use.

\subsubsection{Contrasting eager and lazy loading}

Consider this usage pattern:
\begin{minted}{python}
submissions = assignment.get_submissions()
filtered = [s for s in submissions if s.user_id == target_user]
print(filtered[0].grade)
\end{minted}

With \emph{eager loading} (the old approach):
\begin{enumerate}
\item [[get_submissions()]] returns 50 cached submissions
\item We check each submission: is it stale? Does it need refresh?
\item We make 50 API calls to refresh non-final grades
\item We filter to 1 submission
\item We access the grade (already fresh)
\end{enumerate}

With \emph{lazy loading} (the new approach):
\begin{enumerate}
\item [[get_submissions()]] returns 50 wrapped submissions (no API calls)
\item We filter to 1 submission (wrapper objects pass through filters)
\item We access the grade attribute
\item The wrapper detects access to mutable attribute, checks staleness
\item We make 1 API call to refresh only the accessed submission
\end{enumerate}

The performance improvement is dramatic when filtering: \textbf{1 API call instead
of 50}.

\subsubsection{Design: LazySubmission wrapper}

We implement lazy loading using a transparent wrapper class that intercepts
attribute access.
The wrapper stores the original submission and assignment reference, allowing it
to refresh itself when needed.

Key design decisions:
\begin{description}
\item[Mutable attributes] Only certain attributes change over time: grade,
  grading dates, rubrics, submission data, comments, attachments, and submission
  body.
  We track these and trigger refresh only when they're accessed.
\item[Final grade policy] Submissions with final grades (A, P, P+, complete)
  are never refreshed, maintaining the existing [[NOREFRESH_GRADES]] policy.
\item[Time-based staleness] We use the existing [[outdated()]] function
  with 5-minute TTL for non-final grades.
\item[Preserved includes] The wrapper stores the original [[include]]
  parameters from [[get_submissions()]] to ensure refreshes request the
  same data.
\item[Cache updates] When a wrapper refreshes, it updates its internal state.
  Since the cache holds references to wrapper objects, the cache automatically
  reflects the update.
\end{description}

\subsubsection{Implementation: LazySubmission class}

The [[LazySubmission]] class wraps a submission object and intercepts
attribute access using [[__getattribute__]].

<<LazySubmission class>>=
class LazySubmission:
  """
  Transparent wrapper for Submission objects that lazily refreshes on access.

  This wrapper intercepts access to mutable attributes (grade, timestamps,
  rubrics, etc.) and refreshes the submission from Canvas if:
  - The submission is stale (based on TTL)
  - The grade is not a final grade (not in NOREFRESH_GRADES)

  Immutable attributes (user_id, assignment_id, etc.) are accessed directly
  without triggering a refresh.
  """

  # Attributes that can change and should trigger refresh when accessed
  MUTABLE_ATTRS = {
    'grade', 'graded_at', 'submitted_at', 'rubric_assessment',
    'submission_data', 'submission_history', 'submission_comments',
    'attachments', 'body', 'grade_matches_current_submission'
  }

  def __init__(self, submission, assignment, includes, original_get_submission):
    """
    Initialize lazy submission wrapper.

    Parameters:
      submission: The Submission object to wrap
      assignment: The Assignment object this submission belongs to
      includes: List of include parameters used when fetching
      original_get_submission: Unwrapped get_submission method for refreshing
    """
    # Use object.__setattr__ to avoid triggering our custom __setattr__
    object.__setattr__(self, '_submission', submission)
    object.__setattr__(self, '_assignment', assignment)
    object.__setattr__(self, '_includes', includes)
    object.__setattr__(self, '_original_get_submission', original_get_submission)
    object.__setattr__(self, '_last_refresh', datetime.now())

  def _needs_refresh(self):
    """Check if the wrapped submission needs refreshing."""
    submission = object.__getattribute__(self, '_submission')
    last_refresh = object.__getattribute__(self, '_last_refresh')

    # Never refresh final grades
    if submission.grade in NOREFRESH_GRADES:
      return False

    # Check time-based staleness (5-minute TTL for non-final grades)
    if datetime.now() - last_refresh > timedelta(minutes=5):
      return True

    return False

  def would_need_refresh(self):
    """
    Check if submission would need refresh WITHOUT triggering it.

    Used by pre-check phase to count stale submissions before deciding
    whether to do bulk refresh.

    Returns:
        bool: True if submission would refresh on next access
    """
    return self._needs_refresh()

  def _refresh(self):
    """Refresh the wrapped submission from Canvas."""
    assignment = object.__getattribute__(self, '_assignment')
    submission = object.__getattribute__(self, '_submission')
    includes = object.__getattribute__(self, '_includes')
    original_get_submission = object.__getattribute__(self, '_original_get_submission')

    # Call original method directly (bypasses decorator, no double-wrapping)
    fresh_submission = original_get_submission(
      assignment,
      submission.user_id,
      include=list(includes)
    )

    # Update internal state (cache automatically updated via reference)
    object.__setattr__(self, '_submission', fresh_submission)
    object.__setattr__(self, '_last_refresh', datetime.now())

  def __getattribute__(self, name):
    """
    Intercept attribute access to trigger refresh when needed.

    Logic:
    1. Internal attributes (_*) and methods: access directly
    2. Mutable attributes: check staleness, refresh if needed, then delegate
    3. All other attributes: delegate to wrapped submission
    """
    # Always access internal attributes directly to avoid recursion
    if name.startswith('_') or name in ['MUTABLE_ATTRS']:
      return object.__getattribute__(self, name)

    # Check if this is a mutable attribute that might need refresh
    if name in LazySubmission.MUTABLE_ATTRS:
      if object.__getattribute__(self, '_needs_refresh')():
        object.__getattribute__(self, '_refresh')()

    # Delegate to wrapped submission
    submission = object.__getattribute__(self, '_submission')
    return getattr(submission, name)

  def __setattr__(self, name, value):
    """
    Intercept attribute setting to handle both wrapper and submission attrs.

    Internal attributes (_*) are set on the wrapper.
    All other attributes are set on the wrapped submission.
    """
    if name.startswith('_'):
      object.__setattr__(self, name, value)
    else:
      submission = object.__getattribute__(self, '_submission')
      setattr(submission, name, value)

  def __getattr__(self, name):
    """Fallback for attributes not found on wrapper (delegates to submission)."""
    submission = object.__getattribute__(self, '_submission')
    return getattr(submission, name)

  def __repr__(self):
    """Delegate repr to wrapped submission."""
    submission = object.__getattribute__(self, '_submission')
    return repr(submission)

  def __str__(self):
    """Delegate str to wrapped submission."""
    submission = object.__getattribute__(self, '_submission')
    return str(submission)

  <<pickle support for LazySubmission>>
@

\subsubsection{Pickle support for cache persistence}

The [[LazySubmission]] wrapper must support pickling to enable cache
persistence.
When the [[Canvas]] object is saved to disk (see cache.nw), Python's
[[pickle]] module serializes the entire object hierarchy, including all
cached submissions.

\paragraph{The pickling challenge}

Python's [[pickle]] cannot serialize certain objects, including:
\begin{itemize}
\item Function and method references (unless they're module-level functions)
\item Lambda functions
\item Generator objects
\item Objects with circular references to unpicklable objects
\end{itemize}

Our [[LazySubmission]] stores [[_original_get_submission]], which is
a reference to the original (unwrapped) [[get_submission]] method.
This is a \emph{bound method} when called, but we store the \emph{unbound
function} from the decorator's closure.
Method objects are not picklable by default.

\paragraph{The solution: reconstruct from closure}

We implement [[__getstate__]] and [[__setstate__]] to handle
pickling:
\begin{enumerate}
\item \textbf{Before pickling} ([[__getstate__]]): Exclude the
  unpicklable [[_original_get_submission]] from the state dictionary.
  Save only the submission, assignment reference, includes list, and last
  refresh timestamp.
\item \textbf{After unpickling} ([[__setstate__]]): Reconstruct
  [[_original_get_submission]] by searching the decorator's closure.
  The decorated method stores the original in its [[__closure__]]
  attribute, which we can access via
  [[assignment.__class__.get_submission]].
\end{enumerate}

\paragraph{Why closure search works}

When the [[cache_submissions]] decorator wraps
[[Assignment.get_submission]], it captures the original method in a
closure variable (line~1072 in the decorator):
\begin{minted}{python}
original_get_submission = cls.get_submission
\end{minted}

This variable is referenced by the [[new_get_submission]] function,
making it part of the function's closure.
After unpickling, we can access this closure via the class's decorated method.

\paragraph{Limitations and failure modes}

This approach has several limitations:
\begin{description}
\item[Version compatibility] If the decorator implementation changes between
  pickle and unpickle (e.g., code upgrade), the closure structure may differ,
  causing reconstruction to fail.
\item[Closure ambiguity] If multiple callable objects exist in the closure with
  \enquote{get\_submission} in their name, we might retrieve the wrong one.
  The heuristic checks for callables with matching names.
\item[Graceful degradation] If reconstruction fails, we set
  [[_original_get_submission]] to [[None]].
  The wrapper remains functional for read-only access (attributes pass through
  to the wrapped submission), but calling [[_refresh()]] will fail.
  This is acceptable because:
  \begin{itemize}
  \item Cache files are typically invalidated on code changes
  \item The user can clear the cache manually if issues arise
  \item Better to have stale data than a complete crash
  \end{itemize}
\end{description}

\paragraph{Alternative approaches considered}

We considered but rejected these alternatives:
\begin{description}
\item[Storing method name as string] Reconstruct via
  [[getattr(assignment, 'get_submission')]].
  Problem: This retrieves the \emph{decorated} method, not the original,
  causing infinite recursion or double-wrapping.
\item[Not using a wrapper] Store a \enquote{needs refresh} flag on the
  submission itself.
  Problem: Mutating Canvas API objects is fragile and breaks the lazy-loading
  design.
\item[Store assignment ID, re-fetch assignment] Reconstruct the entire context
  from IDs.
  Problem: Defeats the purpose of caching; would require API calls just to
  restore cache state.
\end{description}

Now we implement the pickle support:
<<pickle support for LazySubmission>>=
def __getstate__(self):
  """
  Prepare object for pickling.

  We can't pickle method references, so we exclude _original_get_submission.
  It will be restored from the decorator's closure during unpickling.
  """
  state = {
    '_submission': object.__getattribute__(self, '_submission'),
    '_assignment': object.__getattribute__(self, '_assignment'),
    '_includes': object.__getattribute__(self, '_includes'),
    '_last_refresh': object.__getattribute__(self, '_last_refresh'),
  }
  return state

def __setstate__(self, state):
  """
  Restore object after unpickling.

  Restore all attributes from state, and reconstruct _original_get_submission
  by searching the decorator's closure for the original method.
  """
  for key, value in state.items():
    object.__setattr__(self, key, value)

  # Restore the original method from the assignment's decorated method
  # The decorated method has the original in its closure
  assignment = state['_assignment']
  decorated_method = assignment.__class__.get_submission

  # The original method is captured in the decorator's closure
  # We can access it via the closure of the decorated method
  # The decorated method's __closure__ contains the original method
  try:
    # Find the original_get_submission in the closure
    for cell in decorated_method.__closure__:
      try:
        obj = cell.cell_contents
        # Check if it's a callable with the right name
        if callable(obj) and hasattr(obj, '__name__') and \
           'get_submission' in obj.__name__:
          object.__setattr__(self, '_original_get_submission', obj)
          break
      except (AttributeError, ValueError):
        continue
  except (AttributeError, TypeError):
    # If we can't find it in closure, fall back to None
    # This means refresh won't work, but at least unpickling succeeds
    object.__setattr__(self, '_original_get_submission', None)
@

This class is used by the caching decorator below.
<<functions>>=
<<LazySubmission class>>
@

\subsubsection{Threshold-based bulk refresh optimization}

While lazy loading solves the problem of wasted refreshes when filtering
submissions, it introduces a new inefficiency in the opposite scenario: when
workflows access \emph{all} submissions' mutable attributes, lazy loading
triggers individual API calls for each stale submission.

\paragraph{The performance bottleneck}

Consider the [[-U]] (ungraded) option in [[submissions.nw]]:
\begin{minted}{python}
for submission in submissions:
    if is_ungraded(submission):
        yield submission
\end{minted}

The [[is_ungraded()]] function checks [[submitted_at]],
[[graded_at]], and [[grade_matches_current_submission]]---all mutable
attributes.
With 50 submissions where 40 are stale:
\begin{enumerate}
\item Access [[submission.submitted_at]] → triggers refresh check
\item Submission is stale → makes API call to refresh
\item Repeat for all 50 submissions
\item Result: 40 individual API calls
\end{enumerate}

This is \textbf{40× slower} than fetching all submissions in one bulk request.

\paragraph{The solution: pre-check and bulk refresh}

We add an optional pre-check phase that counts how many submissions would need
refresh before accessing them. If the count exceeds a threshold, we trigger one
bulk fetch instead of many individual refreshes.

The optimization is opt-in via a [[check_bulk_refresh]] parameter to
[[get_submissions()]]:
\begin{enumerate}
\item Count stale submissions using [[would_need_refresh()]] (non-mutating check)
\item Calculate threshold: [[max(total × 0.2, 3)]] (20\% with minimum 3)
\item If [[stale_count ≥ threshold]], perform bulk refresh (1 API call)
\item Update all [[LazySubmission]] wrappers in-place with fresh data
\item Return refreshed cache so subsequent attribute access doesn't trigger refreshes
\end{enumerate}

\paragraph{When to enable bulk refresh}

The optimization is beneficial when workflows will access mutable attributes on
\emph{most} submissions:
\begin{description}
\item[[{-U}]] (ungraded option)] Always enabled---checks all submissions'
  grading state
\item[Unfiltered listings] Enabled when no [[--user]], [[--category]], or
  [[--group]] filters are applied
\item[Specific user filtering] Disabled---only a few submissions will be
  accessed
\end{description}

\paragraph{Threshold configuration}

The threshold is configurable via environment variables:
\begin{description}
\item[[CANVASLMS_BULK_THRESHOLD]] Fraction of stale submissions needed (default:
  0.2 = 20\%)
\item[[CANVASLMS_BULK_MIN_COUNT]] Minimum number of stale submissions required
  (default: 3)
\end{description}

Examples:
\begin{itemize}
\item Small assignment (10 submissions): threshold = [[max(2, 3) = 3]] (30\%)
\item Medium assignment (50 submissions): threshold = 10 (20\%)
\item Large assignment (200 submissions): threshold = 40 (20\%)
\end{itemize}

\paragraph{Trade-offs}

\emph{Advantages:}
\begin{itemize}
\item Dramatic reduction in API calls for [[-U]] scenarios (40 calls → 1 call)
\item Faster execution when many submissions are accessed
\item Respects final grade policy ([[NOREFRESH_GRADES]] never counted as stale)
\item Updates cache in-place (preserves object identity and references)
\end{itemize}

\emph{Disadvantages:}
\begin{itemize}
\item Pre-check adds overhead (one pass through cache to count staleness)
\item May refresh submissions that won't ultimately be accessed (if filtering
  happens after refresh)
\item Bulk refresh fetches all includes, increasing response size slightly
\end{itemize}

The threshold (20\% minimum 3) balances these trade-offs: we only bulk refresh
when it's likely to save significant API calls.

\paragraph{Design rationale}

Why implement in the decorator, not in [[LazySubmission]]?
\begin{itemize}
\item The decorator has access to the entire cache (all submissions)
\item [[LazySubmission]] only knows about individual submissions
\item Threshold calculation requires global view of staleness
\item Bulk refresh requires coordinating multiple wrappers
\end{itemize}

Why opt-in instead of automatic?
\begin{itemize}
\item Filtering workflows benefit from lazy loading (don't want bulk refresh)
\item [[-U]] and unfiltered listings benefit from bulk refresh
\item Call site knows the access pattern, decorator doesn't
\item Opt-in ensures we only bulk refresh when beneficial
\end{itemize}

\subsection{Caching submissions}

For the results, we can construct something more specific.
Results that we're interested in are submissions.
We can only get submission from an assignment object.
We can either get all submissions, or one specific submission if we specify the
user.

Canvas's [[get_submissions()]] method accepts a [[bucket]] parameter to filter
submissions server-side (e.g., [[bucket="ungraded"]]).
However, this caching implementation does not track the [[bucket]] parameter in
the cache key.

This is intentional: tracking [[bucket]] would require separate caches for
different bucket values, complicating the cache hierarchy and potentially
causing inconsistencies.
Instead, client code (particularly [[submissions.nw]] and [[assignments.nw]])
performs local filtering on the complete submission list.

This design ensures:
\begin{itemize}
\item Cache keys are unambiguous (based only on user ID).
\item Lazy loading works correctly (submissions refresh based on staleness, not
bucket parameter).
\item Cache hierarchy is preserved (course caches contain assignment caches
contain submission caches).
\end{itemize}

See \cref{SubmissionsFiltering} in [[submissions.nw]] for the local filtering
implementation.

Now we combine the caching mechanism with lazy loading.
The cache stores \texttt{LazySubmission} wrappers instead of raw submissions.
This means submissions are only refreshed when their attributes are actually
accessed, not when the list is returned.
<<functions>>=
def make_assignment_submissions_cacheable():
  def cache_submissions(cls):
    """Class decorator for cacheable get_submission, get_submissions methods"""
    <<decorator body for caching assignment submissions>>
    return cls

  canvasapi.assignment.Assignment = \
    cache_submissions(canvasapi.assignment.Assignment)
@

Then we can write the decorator as follows.
We need to add a cache attribute in the constructor, so we must decorate the
constructor.
We must also store the original [[get_submission]] method before decorating it,
because [[LazySubmission]] needs to call it directly to avoid double-wrapping.
Then we must decorate both [[get_submission]] and [[get_submissions]].
<<decorator body for caching assignment submissions>>=
old_constructor = cls.__init__

@functools.wraps(cls.__init__)
def new_init(*args, **kwargs):
  <<extend class constructor for decorators>>
  old_constructor(*args, **kwargs)

cls.__init__ = new_init

# Store original method before wrapping (needed by LazySubmission)
original_get_submission = cls.get_submission
get_submission = cls.get_submission

@functools.wraps(cls.get_submission)
def new_get_submission(self, user, **kwargs):
  <<return submission of user>>

cls.get_submission = new_get_submission

get_submissions = cls.get_submissions

@functools.wraps(cls.get_submissions)
def new_get_submissions(self, **kwargs):
  <<return a list of all submissions>>

cls.get_submissions = new_get_submissions
@

We decorate the method to get a specific submission, [[get_submission]].
With lazy loading, we wrap the submission in [[LazySubmission]] and cache the
wrapper.
The wrapper will handle staleness checking and refresh only when attributes are
accessed.
For this, we first need a cache attribute.
<<extend class constructor for decorators>>=
args[0].__cache = {}
@

Now we can treat how we get an individual submission.
The logic is simplified compared to the eager approach: we only fetch if not in
cache or if new includes are requested.
We no longer check [[NOREFRESH_GRADES]] here---that's handled by
[[LazySubmission._needs_refresh()]].
<<return submission of user>>=
if isinstance(user, User):
  uid = user.id
elif isinstance(user, int):
  uid = user
else:
  raise TypeError(f"user must be User or int")

if "include" in kwargs:
  to_include = set(kwargs["include"])
else:
  to_include = set()

# Check cache
if uid in self.__cache:
  lazy_submission = self.__cache[uid]
  # Get current includes from the wrapper
  current_includes = set(lazy_submission._includes)

  # If new includes are requested, merge and refetch
  if not to_include.issubset(current_includes):
    to_include |= current_includes
    raw_submission = get_submission(self, user, include=list(to_include))
    lazy_submission = LazySubmission(raw_submission, self, to_include,
                                     original_get_submission)
    self.__cache[uid] = lazy_submission

  return lazy_submission
else:
  # Not in cache, fetch and wrap
  raw_submission = get_submission(self, user, include=list(to_include))
  lazy_submission = LazySubmission(raw_submission, self, to_include,
                                   original_get_submission)
  self.__cache[uid] = lazy_submission
  return lazy_submission
@

Now we can deal with [[get_submissions]].
As we might call [[get_submission]] before any [[get_submissions]], we cannot
rely on the cache as a check.
We introduce a new attribute, initialized to [[None]] to indicate that no bulk
fetch has been performed yet.
This follows the same pattern as [[CacheGetMethods]], which uses [[None]] for
\enquote{not fetched} and [[datetime.now()]] for \enquote{last fetch time}.
<<extend class constructor for decorators>>=
args[0].__all_fetched = None
@ Now we can check if this is set or not.
When we fetch, we want to include any data that was previously included.

With lazy loading, we simplify the logic significantly.
We no longer check each submission for staleness here---the [[LazySubmission]]
wrapper handles that when attributes are accessed.
We only refetch all if we haven't fetched before, or if new includes are
requested that aren't in the cache.
<<return a list of all submissions>>=
if "include" in kwargs:
  to_include = set(kwargs["include"])
else:
  to_include = set()

# Extract check_bulk_refresh parameter (default: False)
check_bulk_refresh = kwargs.get("check_bulk_refresh", False)

# Check if we need to refetch due to new includes
need_refetch = False
if self.__all_fetched:
  # Check if any cached submission doesn't have the requested includes
  for lazy_submission in self.__cache.values():
    cached_includes = set(lazy_submission._includes)
    if not to_include.issubset(cached_includes):
      need_refetch = True
      to_include |= cached_includes
      break

if not self.__all_fetched or need_refetch:
  # If refetching, merge all previously requested includes
  if need_refetch:
    for lazy_submission in self.__cache.values():
      to_include |= set(lazy_submission._includes)

  # Fetch all and wrap with LazySubmission
  for raw_submission in get_submissions(self, include=list(to_include)):
    lazy_submission = LazySubmission(raw_submission, self, to_include,
                                     original_get_submission)
    self.__cache[raw_submission.user_id] = lazy_submission

  self.__all_fetched = datetime.now()
elif check_bulk_refresh:
  # Cache exists, includes satisfied, AND caller requested bulk refresh check
  <<check staleness threshold and bulk refresh if needed>>

# Return list of LazySubmission wrappers (no staleness checks here!)
return list(self.__cache.values())
@

The bulk refresh pre-check logic counts how many submissions would need refresh
and triggers a bulk fetch if the count exceeds the threshold.
<<check staleness threshold and bulk refresh if needed>>=
stale_count = 0
total_count = len(self.__cache)

if total_count > 0:
  # Count stale submissions (non-mutating check)
  for lazy_submission in self.__cache.values():
    if lazy_submission.would_need_refresh():
      stale_count += 1

  # Calculate threshold
  if isinstance(BULK_REFRESH_THRESHOLD, float):
    threshold = max(int(total_count * BULK_REFRESH_THRESHOLD),
                    BULK_REFRESH_MIN_COUNT)
  else:
    threshold = BULK_REFRESH_THRESHOLD

  # If threshold exceeded, perform bulk refresh
  if stale_count >= threshold:
    logger.info(f"Bulk refresh: {stale_count}/{total_count} submissions stale, "
                f"threshold={threshold}")

    # Merge all includes from cached submissions
    all_includes = set()
    for lazy_submission in self.__cache.values():
      all_includes |= set(lazy_submission._includes)
    all_includes |= to_include

    # Perform bulk refresh (1 API call)
    for raw_submission in get_submissions(self, include=list(all_includes)):
      if raw_submission.user_id in self.__cache:
        # Update existing wrapper in-place
        lazy_submission = self.__cache[raw_submission.user_id]
        object.__setattr__(lazy_submission, '_submission', raw_submission)
        object.__setattr__(lazy_submission, '_last_refresh', datetime.now())
        object.__setattr__(lazy_submission, '_includes', all_includes)
      else:
        # New submission appeared since cache was created
        lazy_submission = LazySubmission(raw_submission, self, all_includes,
                                       original_get_submission)
        self.__cache[raw_submission.user_id] = lazy_submission

    self.__all_fetched = datetime.now()

