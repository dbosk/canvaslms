\chapter{Caching submission attachments}

When working with submission attachments, we want to avoid downloading the same
file multiple times.
This is particularly important when computing diffs between submission
versions: if a student submits version N+1, we already have the attachments
from versions N, N-1, \ldots cached locally.
Computing the new diff then only requires downloading version N+1.

\section{Design decisions}

We face several design choices for caching attachment files:

\subsection{Where to store cached files}

We use a platform-specific cache directory provided by the [[appdirs]] package.
This follows the conventions of the host operating system:
\begin{itemize}
\item Linux: [[~/.cache/canvaslms/attachments/]]
\item macOS: [[~/Library/Caches/canvaslms/attachments/]]
\item Windows: [[C:\Users\<user>\AppData\Local\canvaslms\attachments\]]
\end{itemize}

This is superior to storing files in the pickle cache because:
\begin{itemize}
\item Pickle files remain small (metadata only)
\item Users can manage cache size and location using OS tools
\item Cache cleanup doesn't affect submission metadata
\end{itemize}

\subsection{Content-addressed storage}

We use \emph{content-addressed storage}: files are identified by the SHA-256
hash of their content, not by Canvas attachment ID or filename.
This provides perfect deduplication---if multiple students submit the same
file, or if a student resubmits an unchanged file, we store it only once.

The storage structure is:
\begin{description}
\item[Content files] [[<cache_dir>/attachments/<sha256>.dat]]
\item[Metadata index] [[<cache_dir>/attachments/index.json]] mapping attachment
  IDs to hashes
\end{description}

The index maps Canvas attachment IDs to cached file metadata:
\begin{minted}{python}
{
  "12345": {
    "hash": "a1b2c3...",
    "filename": "report.pdf",
    "size": 102400,
    "content_type": "application/pdf",
    "last_access": "2025-11-24T10:30:00"
  }
}
\end{minted}

\subsection{When attachments become stale}

Attachments for a particular submission version are immutable---once submitted,
they never change.
Therefore, we cache attachment files forever (no time-to-live expiration).

The only cleanup is \emph{age-based}: we remove files that haven't been
accessed in a configurable number of days (default: 90 days).
This prevents unbounded cache growth while keeping frequently-used files.

\section{Implementation}

We implement cache operations with comprehensive logging to help users understand
cache performance.
Following the pattern established in [[cache.nw]], we log cache operations at
INFO level so users can see cache hits/misses, deduplication, and timing
information with the [[-v]] flag.

This makes cache behavior transparent and helps diagnose performance issues:
users can see when the cache saves time by reusing existing files, and when
new downloads are needed.

<<attachment_cache.py>>=
"""Content-addressed cache for Canvas submission attachments"""

import appdirs
import hashlib
import json
import logging
import pathlib
import shutil
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)

<<cache directory initialization>>
<<cache operations>>
@

\subsection{Cache directory initialization}

We use [[appdirs]] to get the platform-specific cache directory, and create the
necessary subdirectories on first use.

<<cache directory initialization>>=
CACHE_DIR = pathlib.Path(appdirs.user_cache_dir("canvaslms")) / "attachments"
INDEX_FILE = CACHE_DIR / "index.json"

def _ensure_cache_dir():
    """Create cache directory if it doesn't exist"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)

def _load_index() -> Dict[str, Dict[str, Any]]:
    """Load the metadata index from disk"""
    _ensure_cache_dir()
    if INDEX_FILE.exists():
        with open(INDEX_FILE, 'r') as f:
            return json.load(f)
    return {}

def _save_index(index: Dict[str, Dict[str, Any]]):
    """Save the metadata index to disk"""
    _ensure_cache_dir()
    with open(INDEX_FILE, 'w') as f:
        json.dump(index, f, indent=2)
@

\subsection{Computing content hashes}

We use SHA-256 to compute content hashes because it provides:
\begin{itemize}
\item Strong collision resistance (extremely unlikely two different files hash
  to the same value)
\item Reasonable performance for typical attachment sizes
\item Wide availability in Python's standard library
\end{itemize}

<<cache operations>>=
def _compute_hash(file_path: pathlib.Path) -> str:
    """Compute SHA-256 hash of file content"""
    sha256 = hashlib.sha256()
    with open(file_path, 'rb') as f:
        while chunk := f.read(8192):
            sha256.update(chunk)
    return sha256.hexdigest()
@

\subsection{Retrieving cached attachments}

When we need an attachment, we first check if it's in the cache.
If found, we update the [[last_access]] timestamp to prevent premature cleanup,
and return the path to the cached file.

We log cache hits and misses at INFO level (consistent with [[cache.nw]]) so
users can see cache effectiveness with the [[-v]] flag.
This helps users understand when cached files are being reused, and when new
downloads are required.

<<cache operations>>=
def get_cached_attachment(attachment_id: int) -> Optional[pathlib.Path]:
    """
    Get cached file path for attachment, or None if not cached.

    Updates last_access timestamp on cache hit.
    """
    index = _load_index()
    attachment_key = str(attachment_id)

    if attachment_key not in index:
        logger.info(f"Attachment cache miss: {attachment_id}")
        return None

    entry = index[attachment_key]
    file_hash = entry["hash"]
    cached_file = CACHE_DIR / f"{file_hash}.dat"

    if not cached_file.exists():
        # Index entry exists but file missing - remove stale entry
        logger.info(f"Attachment cache: stale entry removed: {attachment_id}")
        del index[attachment_key]
        _save_index(index)
        return None

    # Update last access time
    entry["last_access"] = datetime.now().isoformat()
    _save_index(index)

    logger.info(f"Attachment cache hit: {attachment_id} "
                f"({entry['filename']}, {entry['size'] / 1024:.1f}KB)")

    return cached_file
@

\subsection{Caching new attachments}

When we download a new attachment, we compute its hash, store the file by hash,
and update the index.
If a file with the same hash already exists (deduplication), we reuse it.

We log timing information for hash computation and file operations, following
the pattern from [[cache.nw]] where pickle/encrypt/decrypt timing is logged.
This helps users understand the cost of caching operations and see when
deduplication saves disk space.

Hash computation is I/O intensive (reading entire file), so we log its timing
at INFO level.
When deduplication occurs (file already cached), we log this at INFO level
since it represents a performance benefit worth highlighting.

<<cache operations>>=
def cache_attachment(attachment_id: int, file_path: pathlib.Path,
                     metadata: Dict[str, Any]):
    """
    Cache an attachment file using content-addressed storage.

    Args:
        attachment_id: Canvas attachment ID
        file_path: Path to the downloaded file
        metadata: Dictionary with 'filename', 'size', 'content_type'
    """
    _ensure_cache_dir()

    # Compute content hash with timing
    hash_start = time.perf_counter()
    file_hash = _compute_hash(file_path)
    hash_elapsed = time.perf_counter() - hash_start

    file_size = file_path.stat().st_size
    logger.info(f"Attachment {attachment_id}: hash computed "
                f"({hash_elapsed:.2f}s, {file_size / 1024:.1f}KB)")

    cached_file = CACHE_DIR / f"{file_hash}.dat"

    # Copy file to cache if not already present (deduplication)
    if not cached_file.exists():
        copy_start = time.perf_counter()
        shutil.copy2(file_path, cached_file)
        copy_elapsed = time.perf_counter() - copy_start
        logger.info(f"Attachment {attachment_id}: file cached "
                    f"(copy: {copy_elapsed:.2f}s)")
    else:
        logger.info(f"Attachment {attachment_id}: content already cached (dedup)")

    # Update index
    index = _load_index()
    index[str(attachment_id)] = {
        "hash": file_hash,
        "filename": metadata.get("filename", "unknown"),
        "size": metadata.get("size", 0),
        "content_type": metadata.get("content_type", "application/octet-stream"),
        "last_access": datetime.now().isoformat()
    }
    _save_index(index)
@

\subsection{Cache cleanup}

We provide a cleanup function that removes files not accessed within a
specified time window.
This allows users to manage cache size manually via the [[canvaslms cache
clear-attachments]] command.

The cleanup process:
\begin{enumerate}
\item Load the index and identify attachments not accessed recently
\item Remove their index entries
\item Find all [[.dat]] files in the cache directory
\item Build a set of hashes still referenced by the index
\item Remove [[.dat]] files whose hashes aren't referenced (orphaned files)
\item Report statistics about removed files and space freed
\end{enumerate}

We log cleanup operations at INFO level to provide visibility into cache
maintenance, following the pattern from [[cache.nw]]'s [[clear_command]].
The start message helps users understand when cleanup begins, and the final
statistics show the impact of the operation.

<<cache operations>>=
def cleanup_old_attachments(max_age_days: int = 90) -> Dict[str, Any]:
    """
    Remove cached attachments not accessed in max_age_days.

    Returns:
        Dictionary with 'files_removed' and 'bytes_freed' statistics
    """
    _ensure_cache_dir()
    index = _load_index()
    cutoff_date = datetime.now() - timedelta(days=max_age_days)

    logger.info(f"Attachment cache cleanup: removing files not accessed in "
                f"{max_age_days} days")

    # Find attachments to remove from index
    to_remove = []
    for attachment_id, entry in index.items():
        last_access = datetime.fromisoformat(entry["last_access"])
        if last_access < cutoff_date:
            to_remove.append(attachment_id)

    # Remove old entries from index
    for attachment_id in to_remove:
        del index[attachment_id]
    _save_index(index)

    # Build set of hashes still referenced
    referenced_hashes = {entry["hash"] for entry in index.values()}

    # Find and remove orphaned .dat files
    files_removed = 0
    bytes_freed = 0

    for dat_file in CACHE_DIR.glob("*.dat"):
        file_hash = dat_file.stem  # filename without extension
        if file_hash not in referenced_hashes:
            file_size = dat_file.stat().st_size
            dat_file.unlink()
            files_removed += 1
            bytes_freed += file_size

    logger.info(f"Attachment cache cleanup: {files_removed} files removed, "
                f"{bytes_freed / (1024 * 1024):.1f}MB freed")

    return {
        "files_removed": files_removed,
        "bytes_freed": bytes_freed
    }
@
