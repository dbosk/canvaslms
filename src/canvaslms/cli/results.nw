\chapter{The \texttt{results} command}
\label{results-command}

This chapter provides the subcommand [[results]], which lists the results of a 
course.
The purpose of the listing is to export results from Canvas.

We want to export two types of results.
The first is grades for the students.
We want to turn the assignments in Canvas into grades that can be exported.
The format of the listing is compatible with the \texttt{ladok3} 
package\footnote{%
  URL: \url{https://github.com/dbosk/ladok3}
}.

The second is a listing of all assignments that prevents a student from getting 
a grade.
This is useful for reminding students to finish their missing assignments.

We'll take a general approach and provide an option to switch between these two 
cases.


\section{The [[results]] subcommand and its options}

We outline the module:
<<[[results.py]]>>=
import canvaslms.cli
from canvaslms.cli import assignments, courses, modules, submissions, users
import canvaslms.cli.utils
import canvaslms.hacks.canvasapi

import argparse
import csv
import canvasapi.submission
import datetime as dt
import importlib
import importlib.machinery
import importlib.util
import os
import pathlib
import pkgutil
import re
import sys

<<constants>>
<<functions>>

def add_command(subp):
  """Adds the results command to argparse parser subp"""
  <<add results command to subp>>
@

We add the subparser for [[results]].
The command requires two arguments: course and assignment.
We also want the option to filter on users.
We can add these by using [[add_assignment_option]], however, we don't need the 
ungraded flag as we want to export results (\ie graded material).
Also, we can just add the [[add_user_or_group_option]] to be able to filter on
users or groups.

The epilog contains a formatted list of available grading modules.
We use [[RawDescriptionHelpFormatter]] to preserve the whitespace formatting in
the epilog while still allowing argument help text to wrap normally.

However, we must be careful about how we construct the epilog string.
When [[black]] formats the generated Python, triple-quoted strings passed as
arguments get indented on continuation lines.
Since [[RawDescriptionHelpFormatter]] preserves this whitespace literally, we'd
end up with oddly indented prose text in the help output.

To avoid this, we build the epilog via the [[format_results_epilog]] function,
which joins individual lines without introducing unwanted indentation.
<<add results command to subp>>=
results_parser = subp.add_parser("results",
    help="Lists results of a course",
    description="""<<results command description>>""",
    epilog=format_results_epilog(),
    formatter_class=argparse.RawDescriptionHelpFormatter)
results_parser.set_defaults(func=results_command)
assignments.add_assignment_option(results_parser, ungraded=False)
users.add_user_or_group_option(results_parser)
<<add option to include Fs>>
<<add option for custom summary module>>
<<add option for missing assignments>>
@

Let's summarize what we want to do.
<<results command description>>=
Lists results of a course for export, for instance to the `ladok report` 
command. Output format, CSV:

  <course code> <component code> <student ID> <grade> <grade date> <graders ...>

Can also export a list of missing assignment results (--missing option) that 
prevent the student from getting a grade. Output format, CSV:

  <course code> <component code> <student ID> <missing assignment> <reason>

The reason can be "not submitted" or "not graded".
@ We will cover the option for and loading of the custom summary module later,
in \cref{custom-summary-modules}.
We will also cover the [[format_grading_modules_help]] function, which
dynamically discovers available modules in the [[canvaslms.grades]] package.

Now, that [[results_command]] function must take three arguments: [[config]], 
[[canvas]] and [[args]].
However, unlike the other commands, we don't want to do the processing for the 
assignment options using [[process_assignment_option]].
We want to handle that ourselves, because we want slightly different handling.
<<functions>>=
def results_command(config, canvas, args):
  <<get results and print them>>
@

Now we'd simply like to print the results.
The user provides us with a set of courses and a set of assignments or 
assignment groups in those courses.
If the user provides assignment groups, we will automatically summarize the 
results of all assignments in the assignment group.

We will create a list of results, where each result is a tuple (actually a 
list, since the length might vary).
These tuples will then be printed in CSV format to standard output.
<<get results and print them>>=
output = csv.writer(sys.stdout, delimiter=args.delimiter)

if args.assignment_group != "":
  results = summarize_assignment_groups(canvas, args)
elif hasattr(args, 'module') and args.module != "":
  results = summarize_modules(canvas, args)
else:
  results = summarize_assignments(canvas, args)

# Convert to list and check if empty
results = list(results)
if not results:
  raise canvaslms.cli.EmptyListError("No results found matching the criteria")

for result in results:
  output.writerow(result)
@


\section{Filtering grades in output}

We also want to let the user choose to not include Fs (or other grades) in the 
output.
By default, we ignore F and Fx grades.
If the user supplies [[-F]], we include all grades.
If the user supplies [[-F regex]], we use regex to filter grades.
<<add option to include Fs>>=
results_parser.add_argument("-F", "--filter-grades",
  required=False, action="store", nargs="?",
  const=ALL_GRADES_REGEX,
  default=PASSING_REGEX,
  help=f"Filter grades. By default we only output "
       f"A--Es and Ps ({PASSING_REGEX}. "
       f"If you want to include Fs ({ALL_GRADES_REGEX}), use this option. "
       f"You can also supply an optional regex to this option "
       f"to filter grades based on that.")
<<constants>>=
PASSING_REGEX= r"^([A-EP]|complete)$"
ALL_GRADES_REGEX= r"^([A-FP]x?|(in)?complete)$"
@

To make filtering easy, we provide a helper function.
<<functions>>=
def filter_grade(grade, regex):
  """
  Returns True if the grade matches the regex.
  """
  return re.search(regex, grade)
@


\section{Summarizing assignment results}

In this case, we want to have one assignment per row in the output.
We want to output course, assignment, student ID, grade, submission date and 
those who participated in the grading.

We first get the list of courses.
We do this to then get the list of all users in all courses.
We need these to get the integration ID, that can be used for LADOK for 
example.

Then we get the list of assignments in all courses.
We get the submissions for each assignment.
These submissions are filtered by user.
We do this because this attaches a [[user]] attribute to each submissions with 
the details of each user.
This gives a trivial [[yield]] statement at the end.
<<functions>>=
def summarize_assignments(canvas, args):
  """
  Turn submissions into results:
  - canvas is a Canvas object,
  - args is the command-line arguments, as parsed by argparse.
  """

  <<create [[submissions_list]]>>

  for submission in submissions_list:
    if submission.grade is not None:
      if filter_grade(submission.grade, args.filter_grades):
        yield [
          submission.assignment.course.course_code,
          submission.assignment.name,
          submission.user.integration_id,
          submission.grade,
          round_to_day(submission.submitted_at or submission.graded_at),
          *all_graders(submission)
        ]
@

To create the list of submissions, [[submissions_list]], we have to do the 
following.
First need to list the courses.
For each course we need to get all the users (students).
Then, for each course, we also need all the assignments.
When we have the assignments, we can get the submissions.
Fortunately, we can use the filtering functions provided by the [[courses]], 
[[assignments]] and [[submissions]] modules.
They will parse the CLI arguments and generate the lists.
<<create [[submissions_list]]>>=
assignments_list = assignments.process_assignment_option(canvas, args)
users_list = users.process_user_or_group_option(canvas, args)

submissions_list = submissions.filter_submissions(
  submissions.list_submissions(assignments_list,
                               include=["submission_history"]),
  users_list)
@

\section{Fixing the dates}

We want the grade date to be a date, not the timestamp supplied by Canvas.
For instance, LADOK wants dates, not timestamps.
<<functions>>=
def round_to_day(timestamp):
  """
  Takes a Canvas timestamp and returns the corresponding datetime.date object.
  """
  return dt.date.fromisoformat(timestamp.split("T")[0])
@

\section{Getting all graders for a submission}

We need all graders who participated in the grading, meaning also those who 
previously graded (since the last grader might just complement it).
<<functions>>=
def all_graders(submission):
  """
  Returns a list of everyone who participated in the grading of the submission. 
  I.e. also those who graded previous submissions, when submission history is 
  available.
  """
  graders = []

  for prev_submission in submission.submission_history:
    <<turn [[prev_submission]] into [[Submission]] object>>
    <<append [[prev_submission]]'s grader to [[graders]]>>

  return graders
@

To make the code easier, we'll turn the [[submission_history]] data into 
[[Submission]] objects.
We also want to keep the added [[.assignment]] attribute, since we'll use it 
later.
<<turn [[prev_submission]] into [[Submission]] object>>=
prev_submission = canvasapi.submission.Submission(
  submission._requester, prev_submission)
prev_submission.assignment = submission.assignment
@

Now, we'd like to extract the grader.
We'll get the grader's Canvas user ID, so we'll need to resolve it to an actual 
user.
Fortunately, we can use the [[resolve_grader]] function from the 
[[submissions]] module to do all the work.
<<append [[prev_submission]]'s grader to [[graders]]>>=
grader = submissions.resolve_grader(prev_submission)
if grader:
  graders.append(grader)
@


\section{Summarizing assignment group results}

In this case, we want to have one assignment group per row in the output.
We want to output course, assignment group, student ID, summarized grade based 
on all assignments in the group and the latest submission date.

Unlike the previous case, here we must maintain the structure of which 
assignments belong to which assignment group so that we can check easily that a 
user has passed all assignments in the group.
<<functions>>=
def summarize_assignment_groups(canvas, args):
  """
  Summarize assignment groups into a single grade:
  - canvas is a Canvas object,
  - args is the command-line arguments, as parsed by argparse.
  """

  courses_list = courses.process_course_option(canvas, args)
  all_assignments = list(assignments.process_assignment_option(canvas, args))
  users_list = set(users.process_user_or_group_option(canvas, args))

  for course in courses_list:
    ag_list = assignments.filter_assignment_groups(
      course, args.assignment_group)

    for assignment_group in ag_list:
      assignments_list = list(assignments.filter_assignments_by_group(
        assignment_group, all_assignments))
      if args.missing:
        <<produce a list of missing assignments>>
      else:
        <<produce a list of grades>>
@

\subsection{Producing a list of grades}

Let's start with the case where we want to produce a list of grades.
We simply call the [[summary.summarize_group]] function with the assignments 
and users and process the results.
<<produce a list of grades>>=
<<load the correct summary module as [[summary]]>>
for user, grade, grade_date, *graders in summary.summarize_group(
  assignments_list, users_list):
    <<check if we should skip based on [[grade]]>>
    yield [
      course.course_code,
      assignment_group.name,
      user.integration_id,
      grade,
      grade_date,
      *graders
    ]
@ We will now cover the [[summarize_group]] function in the [[summary]] module.

Similarly, we want to summarize modules.
This is very similar to assignment groups, but we work with modules instead.
<<functions>>=
def summarize_modules(canvas, args):
  """
  Summarize modules into a single grade:
  - canvas is a Canvas object,
  - args is the command-line arguments, as parsed by argparse.
  """

  courses_list = courses.process_course_option(canvas, args)
  all_assignments = list(assignments.process_assignment_option(canvas, args))
  users_list = set(users.process_user_or_group_option(canvas, args))

  for course in courses_list:
    module_list = modules.filter_modules(course, args.module)

    for module in module_list:
      assignments_list = list(modules.filter_assignments_by_module(
        module, all_assignments))
      if args.missing:
        <<produce a list of missing assignments for module>>
      else:
        <<produce a list of grades for module>>
@

\subsection{Producing grades and missing assignments for modules}

@ We also need to handle the module case for grades.
<<produce a list of grades for module>>=
<<load the correct summary module as [[summary]]>>
for user, grade, grade_date, *graders in summary.summarize_group(
  assignments_list, users_list):
    <<check if we should skip based on [[grade]]>>
    yield [
      course.course_code,
      module.name,
      user.integration_id,
      grade,
      grade_date,
      *graders
    ]

@ And for missing assignments in modules.
<<produce a list of missing assignments for module>>=
<<load the correct missing module as [[missing]]>>
<<let [[missing_results]] be the result of [[missing.missing_assignments]]>>
for user, assignment, reason in missing_results:
  yield [
    course.course_code,
    module.name,
    user.login_id,
    assignment.name,
    reason
  ]
@

If a student hasn't done anything, the grade and date will be [[None]].
There is no point in including this in the result.
Similarly, this is a good point to do the filtering of grades.
<<check if we should skip based on [[grade]]>>=
if grade is None or grade_date is None \
    or not filter_grade(grade, args.filter_grades):
  continue
@

\subsection{Discovering available grading modules}
\label{grading-module-discovery}

When users install [[canvaslms]] via pipx, the package resides in an isolated
virtual environment.
This means the traditional approach of using [[pydoc3 canvaslms.grades]] to
discover available modules no longer works---pydoc searches the system Python
path, not pipx's isolated environment.

To solve this, we dynamically discover available modules at runtime using
[[pkgutil.iter_modules]].
This works regardless of installation method because we introspect from within
the package itself.
If a module fails to import (for example, due to missing dependencies), we
simply skip it rather than failing the entire discovery process.

We provide two helper functions: one to list the modules with their
descriptions, and one to format them for display in help text.
<<functions>>=
def list_grading_modules():
  """
  Discover available grading modules in canvaslms.grades package.
  Returns a list of (module_name, description) tuples.
  """
  import canvaslms.grades

  modules = []
  # These are internal modules, not user-facing grading strategies
  excluded = {"__init__", "grades"}

  for importer, modname, ispkg in pkgutil.iter_modules(canvaslms.grades.__path__):
    if modname in excluded:
      continue
    try:
      module = importlib.import_module(f"canvaslms.grades.{modname}")
      doc = module.__doc__
      if doc:
        # Take only the first line of the docstring
        first_line = doc.strip().split('\n')[0]
      else:
        first_line = "(no description)"
      modules.append((modname, first_line))
    except ImportError:
      continue

  return sorted(modules)

def format_grading_modules_help():
  """Format the list of grading modules for help text display."""
  modules = list_grading_modules()
  if not modules:
    return "  (no modules found)"

  lines = []
  for name, desc in modules:
    lines.append(f"  canvaslms.grades.{name}: {desc}")
  return "\n\n".join(lines)

def format_results_epilog():
  """
  Build the epilog for results command help.

  We construct the epilog by joining lines explicitly rather than using a
  triple-quoted string. This avoids indentation issues when black formats
  the generated Python code---triple-quoted strings in function arguments
  get continuation-line indentation that RawDescriptionHelpFormatter
  would preserve literally.
  """
  parts = [
    "If you specify an assignment group, the results of the assignments in that",
    "group will be summarized. You can supply your own function for summarizing",
    "grades through the -S option.",
    "",
    "Available grading modules:",
    format_grading_modules_help(),
    "",
    "You can also provide a path to your own Python file with a summarize_group",
    "function."
  ]
  return "\n".join(parts)
@

The [[list_grading_modules]] function uses [[pkgutil.iter_modules]] to iterate
over all submodules in the [[canvaslms.grades]] package.
We exclude [[__init__]] (the package initializer) and [[grades]] (an internal
module) since these are not user-facing grading strategies.

For each module, we import it and extract the first line of its docstring as a
brief description.
This gives users immediate visibility into what each module does, right in the
help output.

The [[format_grading_modules_help]] function joins module entries with blank
lines between them (double newline) for visual separation in the help output.
This makes the list easier to scan when module descriptions vary in length.

The [[format_results_epilog]] function builds the complete epilog by joining
individual lines.
This approach avoids the pitfall where [[black]] indents continuation lines in
triple-quoted strings, which [[RawDescriptionHelpFormatter]] would preserve
literally, resulting in oddly indented prose in the help output.


\subsection{Loading a custom summary module}
\label{custom-summary-modules}

Different teachers have different policies for merging several assignments into
one grade.
We now want to provide a way to override the default function.
<<summary module option doc>>=
Name of Python module or file to load with a custom summarization function.
Default: `{default_summary_module}`. Available modules: """ + \
", ".join(m[0] for m in list_grading_modules()) + """. \
Or provide a path to your own Python file.
<<add option for custom summary module>>=
default_summary_module = "canvaslms.grades.conjunctavg"
results_parser.add_argument("-S", "--summary-module",
  required=False, default=default_summary_module,
  help=f"""<<summary module option doc>>""")
@

Now, let's load the module into the identifier [[summary]] for the above code.
This is a very dangerous construction.
An attacker can potentially load their own module and have it execute when 
reporting grades.
For instance, a malicious module could change grades, \eg always set 
A's.

Now to the loader, we first try to load a system module, then we look for a 
module in the current working directory.
We provide a helper function to do this.
<<functions>>=
def load_module(module_name):
  """
  Load a module from the file system or a built-in module.
  """
  try:
    return importlib.import_module(module_name)
  except ModuleNotFoundError:
    module_path = pathlib.Path.cwd() / module_name
    module = module_path.stem

    loader = importlib.machinery.SourceFileLoader(
      module, str(module_path))
    spec = importlib.util.spec_from_loader(module, loader)
    module_obj = importlib.util.module_from_spec(spec)
    loader.exec_module(module_obj)
    return module_obj
<<load the correct summary module as [[summary]]>>=
try:
  summary = load_module(args.summary_module)
except Exception as err:
  canvaslms.cli.err(1, f"Error loading summary module "
    f"'{args.summary_module}': {err}")
@

The available summary functions and the default one can be found in 
\cref{summary-modules}.

\subsection{Producing a list of missing assignments}

Now we want to look at the missing option.
If the user supplies this option, we want to produce a list of missing 
assignments.
Similarly to summarizing a group, we also want to use different modules to 
produce the missing assignments.
We'll use an option missing which takes an optional name of such a module.
<<add option for missing assignments>>=
<<define [[default_missing_module]]>>
results_parser.add_argument("--missing",
  required=False, nargs="?",
  const=default_missing_module, default=None,
  help="Produce a list of missing assignments instead of grades. "
       "You can supply a custom module to this option, the module must "
       "contain a "
       "function `missing_assignments(assignments_list, users_list). "
       <<missing module behaviour>>
       "This option only has effect when working with assignment groups.")
@

This lets us load the module and use it to produce the missing assignments, in 
a similar fashion as above.
<<load the correct missing module as [[missing]]>>=
if args.missing:
  try:
    missing = load_module(args.missing)
  except Exception as err:
    canvaslms.cli.err(1, f"Error loading missing module "
      f"'{args.missing}': {err}")
@

Now, to the main part of the problem.
We simply load the module and call the [[missing_assignments]] function.
It should return a list of tuples, where each tuple is a user, an assignment 
and a reason why the assignment is missing.
For instance, the reason could be \enquote{not submitted} or \enquote{not 
graded} or \enquote{failed}.

We output the user's login ID instead of the integration ID, since the login ID 
can be used to contact the student (which is probably what we want this data 
for).
<<produce a list of missing assignments>>=
<<load the correct missing module as [[missing]]>>
<<let [[missing_results]] be the result of [[missing.missing_assignments]]>>
for user, assignment, reason in missing_results:
  yield [
    course.course_code,
    assignment_group.name,
    user.login_id,
    assignment.name,
    reason
  ]
@

\subsubsection{The default missing module}

We'll now cover a default function for the missing assignments.
We'll put it in the same module as the [[results]] CLI command, not in a 
separate module.
<<functions>>=
def missing_assignments(assignments_list, users_list,
                        <<optional [[missing_assignments]] args>>):
  """
  Returns tuples of missing assignments.

  <<doc [[missing_assignments]]>>
  """
  for user in users_list:
    for assignment in assignments_list:
      <<skip if [[assignment]] is optional>>
      <<if [[assignment]] is missing for [[user]], yield it>>
<<define [[default_missing_module]]>>=
default_missing_module = "canvaslms.cli.results"
@

We'll add [[<<optional [[missing_assignments]] args>>]] to the function to make 
it accept useful arguments to modify its behaviour.
This way, when someone needs a specific function for their course, they can 
just write a function that modifies the default arguments to this function.

Let's outline what we want this function to do.
The default module checks if all things are graded or submitted.
<<missing module behaviour>>=
"The default module checks if all things are graded or submitted. "
<<doc [[missing_assignments]]>>=
For each assignment that a student is not done with, we yield a tuple of the
user, the assignment and the reason why the assignment is missing.

The reason can be "not submitted" or "not graded" or "not a passing grade".

The only reason to use a different module is if you have optional assignments.
We only want to remind the students of the things they need to pass the course.
We don't want to make it sound like an optional assignment is mandatory.
@

This gives us something like this.
<<if [[assignment]] is missing for [[user]], yield it>>=
try:
  submission = assignment.get_submission(user)
except canvasapi.exceptions.ResourceDoesNotExist:
  continue

if submission is None:
  yield user, assignment, "not submitted"
elif submission.grade is None:
  if hasattr(submission, 'submitted_at') and submission.submitted_at:
    yield user, assignment, \
          f"submitted on {canvaslms.cli.utils.format_local_time(submission.submitted_at)}, but not graded"
  else:
    yield user, assignment, "not done"
elif not filter_grade(submission.grade, passing_regex):
  if hasattr(submission, 'submitted_at') and submission.submitted_at and \
        hasattr(submission, 'graded_at') and submission.graded_at and \
        submission.submitted_at > submission.graded_at:
    yield user, assignment, \
          f"not a passing grade ({submission.grade}), resubmission not graded"
  else:
    yield user, assignment, \
          f"not a passing grade ({submission.grade})"
@

Now, we need that [[passing_regex]], so we can add it to the optional 
arguments, with a default value (same as above).
We add the most common grading scales.
But we also add number scores, which can be used for mandatory surveys and the 
like.
<<optional [[missing_assignments]] args>>=
passing_regex=PASSING_REGEX,
@

Next, if we want to be able to skip optional assignments, we can add an 
optional argument for that.
<<optional [[missing_assignments]] args>>=
optional_assignments = None,
@

This allows us to make the call to the function as follows.
We check if it's the default function or not, if it is we can pass additional 
arguments from the CLI arguments.
<<let [[missing_results]] be the result of [[missing.missing_assignments]]>>=
if missing.missing_assignments == missing_assignments:
  missing_results = missing.missing_assignments(
                                  assignments_list, users_list,
                                  passing_regex=args.filter_grades,
                                  optional_assignments=args.optional_assignments
                                )
else:
  missing_results = missing.missing_assignments(
    assignments_list, users_list)
@

All that is missing now is the optional assignments argument for the parser.
<<add option for missing assignments>>=
results_parser.add_argument("-O", "--optional-assignments",
  required=False, nargs="+",
  default=[<<default optional assignments>>],
  help="List of regexes matching optional assignments. The default missing "
       "assignments will treat matching assignments as optional.")
@

We let the default be assignments prefixed with \enquote{Optional:}.
<<default optional assignments>>=
"^Optional:",
@

Finally, we can do the skipping too.
<<skip if [[assignment]] is optional>>=
if optional_assignments:
  if any(re.search(optional, assignment.name)
         for optional in optional_assignments):
    continue
@
