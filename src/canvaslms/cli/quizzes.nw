\chapter{The \texttt{quizzes} command}
\label{quizzes-command}
\chapterprecis{%
  This chapter is authored by GitHub Copilot.
  It has been minimally reviewed and revised by Daniel Bosk.%
}

This chapter provides the subcommand [[quizzes]], which provides access to
Canvas quiz and survey data.

The [[quizzes]] command has two subcommands:
\begin{itemize}
\item [[list]] lists all quizzes (including Classic Quizzes, New Quizzes, and surveys) in a course
\item [[analyse]] summarizes quiz or survey evaluation data
\end{itemize}

The [[analyse]] subcommand supports two modes of operation:
\begin{enumerate}
\item Fetch quiz/survey data directly from Canvas by specifying the quiz.
  This works reliably for Classic Quizzes. For New Quizzes (Quizzes.Next),
  the implementation uses the documented New Quiz Reports API, but this may
  not be available on all Canvas instances or may require specific permissions.
\item Read and analyze a CSV file downloaded from Canvas.
  This is the most reliable method for both Classic and New Quizzes.
\end{enumerate}

\textbf{Note on New Quizzes:} The New Quiz Reports API was recently introduced
(January 2025) and is documented at
\url{https://documentation.instructure.com/doc/api/new_quizzes_reports.html}.
If you encounter ``Not Found'' errors when analyzing New Quizzes via the API,
use the CSV export method instead:
\begin{enumerate}
\item Navigate to the New Quiz in Canvas
\item Click the three-dot menu (\ldots) in the upper right
\item Select \enquote{Student Analysis}
\item Download the CSV file
\item Run: [[canvaslms quizzes analyse --csv <file.csv>]]
\end{enumerate}

For analysis, the command provides:
\begin{itemize}
\item Statistical summaries for quantitative (multiple choice, rating) data
\item Individual responses for qualitative (free text) data
\item AI-generated summaries of qualitative data using the [[llm]] package
\end{itemize}


\section{Usage Examples}

\subsection{Analyzing a CSV file}

The most common usage is to download the Student Analysis Report CSV from 
Canvas and analyze it:

\begin{minted}{bash}
canvaslms quizzes analyse --csv survey_results.csv
\end{minted}

To download the CSV from Canvas:
\begin{enumerate}
\item Navigate to your quiz/survey in Canvas
\item Click the three-dot menu (\ldots) in the upper right
\item Select \enquote{Student Analysis}
\item Download the CSV file
\item Run the command above with the downloaded file
\end{enumerate}

\subsection{AI Summaries}

The command uses the [[llm]] package for AI-generated summaries of qualitative 
responses. This feature is enabled by default, but can be disabled with the 
[[--no-ai]] option. Before using AI summaries, you need to configure [[llm]] with your 
API keys:

\begin{minted}{bash}
# Configure OpenAI
llm keys set openai

# Or configure another provider (e.g., Anthropic)
llm keys set anthropic

# Test the configuration
llm "Hello, world!"
\end{minted}

Once configured, the [[quizzes analyse]] command will automatically generate summaries 
for all qualitative (free-text) responses. To disable AI summaries, use the [[--no-ai]] 
option:

\begin{minted}{bash}
# Analyze without AI summaries
canvaslms quizzes analyse --csv survey_results.csv --no-ai
\end{minted}

\subsection{Output Format}

The command produces two types of output:

\paragraph{Quantitative Summary} For questions with limited response options 
(multiple choice, ratings, etc.), the command shows:
\begin{itemize}
\item Total number of responses
\item For numeric data: mean, median, standard deviation, min, and max
\item For categorical data: frequency distribution with percentages
\item For multi-select questions (comma-separated): total selections and 
  individual option counts
\end{itemize}

\paragraph{Qualitative Summary} For free-text questions, the command shows:
\begin{itemize}
\item All individual responses (numbered)
\item An AI-generated summary highlighting main themes, concerns, suggestions, 
  and overall sentiment
\end{itemize}

\paragraph{Output Formats} The command supports two output formats:
\begin{itemize}
\item \textbf{Markdown} (default): Rendered beautifully using the [[rich]] 
  package with styled headers and formatting
\item \textbf{LaTeX}: Raw LaTeX output ready to be compiled into a PDF document
\end{itemize}

To select the output format:
\begin{minted}{bash}
# Markdown output (default)
canvaslms quizzes analyse --csv survey.csv

# LaTeX output
canvaslms quizzes analyse --csv survey.csv --format latex > report.tex
pdflatex report.tex
\end{minted}


\section{Module outline}

We outline the module:
<<quizzes.py>>=
import argparse
import csv
import json
import os
import re
import statistics
import sys
from collections import defaultdict, Counter
from typing import Dict, List, Any

import canvaslms.cli
import canvaslms.cli.courses as courses
import canvaslms.cli.assignments as assignments
import canvaslms.cli.utils
from rich.console import Console
from rich.markdown import Markdown

<<functions>>

def add_command(subp):
  """Adds the quizzes command with subcommands to argparse parser subp"""
  <<add quizzes command to subp>>

def add_list_command(subp):
  """Adds the quizzes list subcommand to argparse parser subp"""
  <<add quizzes list command to subp>>

def add_analyse_command(subp):
  """Adds the quizzes analyse subcommand to argparse parser subp"""
  <<add quizzes analyse command to subp>>
@


\section{The [[quizzes]] command and its subcommands}

We add the parent [[quizzes]] parser with subcommands.
The [[quizzes analyse]] subcommand can either work with a CSV file or fetch data from Canvas.
<<add quizzes command to subp>>=
quizzes_parser = subp.add_parser("quizzes",
    help="Quiz-related commands",
    description="Quiz-related commands for Canvas LMS")

quizzes_subp = quizzes_parser.add_subparsers(
    title="quizzes subcommands",
    dest="quizzes_command",
    required=True)

add_list_command(quizzes_subp)
add_analyse_command(quizzes_subp)
@


\section{The [[quizzes analyse]] subcommand and its options}

We add the subparser for [[quizzes analyse]].
The command can either work with a CSV file or fetch data from Canvas.
<<add quizzes analyse command to subp>>=
analyse_parser = subp.add_parser("analyse",
    help="Summarize quiz/survey evaluation data",
    description="""Summarizes Canvas quiz or survey evaluation data.
    
Can either fetch quiz data from Canvas or analyze a downloaded CSV file.
Provides statistical summaries for quantitative data and AI-generated 
summaries for qualitative (free text) responses.""")

analyse_parser.set_defaults(func=analyse_command)

<<add quizzes options>>
@


\section{The [[quizzes list]] subcommand and its options}

We add the subparser for [[quizzes list]].
This command lists all quizzes (both Classic Quizzes and New Quizzes) in a course.
<<add quizzes list command to subp>>=
list_parser = subp.add_parser("list",
    help="List all quizzes in a course",
    description="""Lists all quizzes (including Classic Quizzes, New Quizzes, and surveys)
in a course. Output in CSV format with quiz ID, title, type, and whether it's published.""")

list_parser.set_defaults(func=list_command)

try:
  courses.add_course_option(list_parser, required=True)
except argparse.ArgumentError:
  pass
@

We need options for:
\begin{itemize}
\item CSV file input
\item Course selection (for Canvas API mode)
\item Assignment/quiz selection (for Canvas API mode)
\item Output format selection (markdown or LaTeX)
\end{itemize}
<<add quizzes options>>=
analyse_parser.add_argument("--csv", "-f",
    help="Path to CSV file downloaded from Canvas",
    type=str)

analyse_parser.add_argument("--format", "-F",
    help="Output format: markdown (default) or latex",
    choices=["markdown", "latex"],
    default="markdown")

# Check if llm package is available
try:
  import llm
  HAS_LLM = True
except ImportError:
  HAS_LLM = False

if HAS_LLM:
  analyse_parser.add_argument("--ai", 
      dest="ai",
      action="store_true",
      default=False,
      help="Enable AI-generated summaries. These use the `llm` package "
           "on PyPI and require configuration. Particularly you need to "
           "configure a default model and set up API keys. "
           "See https://pypi.org/project/llm/ for details.")

analyse_parser.add_argument("--no-ai",
    dest="ai",
    action="store_false",
    default=True,
    help="Disable AI-generated summaries" \
        + ("" if HAS_LLM \
           else " (--ai option not available: install with "
                  "'pipx install canvaslms[llm]' to enable AI summaries)"))

try:
  courses.add_course_option(analyse_parser, required=False)
except argparse.ArgumentError:
  pass

try:
  assignments.add_assignment_option(analyse_parser, 
      ungraded=False, required=False)
except argparse.ArgumentError:
  pass
@


\section{Processing the [[quizzes list]] command}

The [[quizzes list]] command lists all quizzes in a course, including both
new quizzes (assignments) and classic quizzes (accessed via [[get_quizzes()]]).
<<functions>>=
def list_command(config, canvas, args):
  """Lists all quizzes in a course"""
  <<process quizzes list command>>
@

<<process quizzes list command>>=
# Get the course list
course_list = courses.process_course_option(canvas, args)

if not course_list:
  canvaslms.cli.err(1, "No course found matching criteria")

for course in course_list:
  <<list all quizzes in course>>
@

\subsection{Listing all quizzes}

We need to list both New Quizzes (from assignments) and Classic Quizzes.
The output format follows the same pattern as [[assignments list]]:
course code, title, type, published status, due date.

<<list all quizzes in course>>=
# Keep track of quiz IDs we've already listed to avoid duplicates
listed_quiz_ids = set()

# List Classic Quizzes (including surveys)
# These are retrieved via get_quizzes() and have quiz_type attribute
try:
  quizzes = course.get_quizzes()
  for quiz in quizzes:
    quiz_type = getattr(quiz, 'quiz_type', 'quiz')
    published = "Published" if getattr(quiz, 'published', False) else "Unpublished"
    due_date = canvaslms.cli.utils.format_local_time(getattr(quiz, 'due_at', None))
    print(f"{course.course_code}\t{quiz.title}\t{quiz_type}\t{published}\t{due_date}")
    listed_quiz_ids.add(quiz.id)
except Exception as e:
  canvaslms.cli.warn(f"Could not fetch classic quizzes: {e}")

# List New Quizzes (Quizzes.Next)
# These are retrieved via get_new_quizzes() API
try:
  new_quizzes = course.get_new_quizzes()
  for quiz in new_quizzes:
    # Skip if we've already listed this quiz (shouldn't happen, but just in case)
    if quiz.id in listed_quiz_ids:
      continue
    published = "Published" if getattr(quiz, 'published', False) else "Unpublished"
    due_date = canvaslms.cli.utils.format_local_time(getattr(quiz, 'due_at', None))
    print(f"{course.course_code}\t{quiz.title}\tnew_quiz\t{published}\t{due_date}")
    listed_quiz_ids.add(quiz.id)
except Exception as e:
  # New Quizzes API might not be available on all Canvas instances
  canvaslms.cli.warn(f"Could not fetch New Quizzes: {e}")
@


\section{Processing the [[quizzes analyse]] command}

The main command processing determines whether to use CSV file or Canvas API.
<<functions>>=
def analyse_command(config, canvas, args):
  """Analyzes quiz or survey data from CSV file or Canvas"""
  <<process quizzes command>>
@

<<process quizzes command>>=
if args.csv:
  <<process CSV file>>
else:
  <<fetch from Canvas and process>>
@


\section{CSV file processing}

When processing a CSV file, we need to:
\begin{enumerate}
\item Parse the CSV structure
\item Identify question columns
\item Separate quantitative and qualitative questions
\item Generate summaries
\end{enumerate}
<<process CSV file>>=
if not os.path.exists(args.csv):
  canvaslms.cli.err(1, f"CSV file not found: {args.csv}")

try:
  with open(args.csv, 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    rows = list(reader)
    
    if not rows:
      canvaslms.cli.err(1, "CSV file is empty")
    
    <<parse and summarize CSV data>>
except Exception as e:
  canvaslms.cli.err(1, f"Error processing CSV: {e}")
@


\section{Parsing and summarizing CSV data}

The Canvas CSV format has a complex structure where questions appear as column 
headers with their IDs.
We need to identify which columns are questions and which are metadata.
<<parse and summarize CSV data>>=
<<categorize and summarize quiz data>>
@

\subsection{Categorize and summarize quiz data}

This chunk processes quiz data rows and generates summaries.
It's used both for CSV file processing and Canvas API data.
<<categorize and summarize quiz data>>=
# Initialize output buffer
output_buffer = []

# Get all column names
columns = list(rows[0].keys())

# Identify question columns (they contain question IDs like "588913:")
question_columns = []
for col in columns:
  if re.match(r'^\d+:', col):
    question_columns.append(col)

if not question_columns:
  canvaslms.cli.err(1, "No question columns found in CSV")

# Categorize questions
quantitative_questions = []
qualitative_questions = []

for qcol in question_columns:
  # Check if the column contains mostly numeric/categorical responses
  sample_responses = [row[qcol] for row in rows if row[qcol]]
  
  if is_quantitative(sample_responses):
    quantitative_questions.append(qcol)
  else:
    qualitative_questions.append(qcol)

<<summarize quantitative data>>
<<summarize qualitative data>>
<<render output>>
@


\section{Helper functions}

\subsection{Detecting New Quizzes}

New Quizzes (also known as Quizzes.Next) are NewQuiz objects retrieved via
[[get_new_quizzes()]], while Classic Quizzes are Quiz objects from
[[get_quizzes()]]. We detect them by checking the class type.
<<functions>>=
def is_new_quiz(quiz):
  """Determine if a quiz object is a New Quiz (Quizzes.Next)"""
  # Check if it's a NewQuiz object (from get_new_quizzes())
  # vs a Quiz object (from get_quizzes())
  return quiz.__class__.__name__ == 'NewQuiz'
@


\subsection{Progress polling}

Both Classic Quizzes and New Quizzes return Progress objects when generating
reports. This helper function polls a progress object until completion.
<<functions>>=
def poll_progress(progress_obj, max_attempts=30, sleep_interval=2):
  """
  Poll a progress object until it completes.

  Args:
    progress_obj: A Progress object or report object with progress attribute
    max_attempts: Maximum number of polling attempts
    sleep_interval: Seconds to wait between polls

  Returns:
    The final progress/report object, or None if max attempts reached
  """
  import time

  for attempt in range(max_attempts):
    # Check different ways the progress might indicate completion
    is_completed = False

    if hasattr(progress_obj, 'query'):
      # It's a Progress object - refresh it
      progress_obj.query()
      if hasattr(progress_obj, 'workflow_state'):
        is_completed = progress_obj.workflow_state == 'completed'

    # For quiz reports with embedded progress
    if hasattr(progress_obj, 'progress'):
      if hasattr(progress_obj.progress, 'workflow_state'):
        is_completed = progress_obj.progress.workflow_state == 'completed'
      elif isinstance(progress_obj.progress, dict):
        is_completed = progress_obj.progress.get('workflow_state') == 'completed'
    elif hasattr(progress_obj, 'workflow_state'):
      is_completed = progress_obj.workflow_state == 'completed'

    if is_completed:
      return progress_obj

    if attempt < max_attempts - 1:
      time.sleep(sleep_interval)

  return None
@


\subsection{CSV report downloading}

Both Classic and New Quizzes provide CSV reports via a file URL. This helper
downloads and decodes the CSV with proper UTF-8 encoding.
<<functions>>=
def download_csv_report(file_url):
  """
  Download a CSV report from Canvas and return a CSV reader.

  Args:
    file_url: URL to the CSV file

  Returns:
    csv.DictReader object with the CSV data
  """
  import requests
  import io

  response = requests.get(file_url)
  response.raise_for_status()

  # Explicitly decode as UTF-8 to handle international characters
  csv_data = response.content.decode('utf-8')
  return csv.DictReader(io.StringIO(csv_data))
@


\subsection{Creating New Quiz reports}

New Quizzes use a different API endpoint for report generation. We need to
make a direct API call using the Canvas [[_requester]] object.
<<functions>>=
def create_new_quiz_report(course, assignment_id, requester):
  """
  Create a student analysis report for a New Quiz.

  Args:
    course: Course object
    assignment_id: The assignment ID of the New Quiz
    requester: Canvas _requester object for making API calls

  Returns:
    Progress object for polling
  """
  import canvasapi.progress

  # Build the API endpoint
  endpoint = f"courses/{course.id}/quizzes/{assignment_id}/reports"

  # Make the POST request with form parameters
  # Note: New Quiz API expects form-encoded parameters
  response = requester.request(
    'POST',
    f'/api/quiz/v1/{endpoint}',
    **{
      'quiz_report[report_type]': 'student_analysis',
      'quiz_report[format]': 'csv'
    }
  )

  # The response is a Progress object
  return canvasapi.progress.Progress(requester, response.json())
@


\subsection{Determining question type}

We need to determine if a question is quantitative or qualitative based on the 
responses.
For questions where Canvas lists comma-separated options (like multi-select 
questions), we need to detect when substrings reoccur systematically.
<<functions>>=
def extract_comma_separated_options(responses: List[str]) -> List[str]:
  """Extract individual options from comma-separated responses"""
  all_options = []
  for resp in responses:
    if ',' in resp:
      # Split by comma and clean up
      options = [opt.strip() for opt in resp.split(',')]
      all_options.extend(options)
    else:
      all_options.append(resp)
  return all_options

def is_quantitative(responses: List[str]) -> bool:
  """Determine if responses are quantitative (numbers/categories) or qualitative (free text)"""
  if not responses:
    return False
  
  # Check if responses contain comma-separated lists that suggest multi-select
  # If many responses have commas and we see repeated substrings, it's quantitative
  comma_count = sum(1 for r in responses if ',' in r)
  if comma_count > len(responses) * 0.3:  # More than 30% have commas
    # Extract all individual options
    all_options = extract_comma_separated_options(responses)
    unique_options = set(all_options)
    
    # If we have a reasonable number of unique options that repeat, it's quantitative
    if len(unique_options) <= 20 and len(all_options) > len(unique_options):
      return True
  
  # If responses are very short or repetitive, likely quantitative
  # Check for common patterns
  unique_responses = set(responses)
  
  # If all or most unique responses appear only once (no repetition), 
  # it's likely qualitative (free text where each answer is unique)
  response_counts = Counter(responses)
  responses_with_one_occurrence = sum(1 for count in response_counts.values() if count == 1)
  
  # If 90% or more of unique responses appear only once, likely qualitative
  if len(unique_responses) >= 5 and responses_with_one_occurrence >= len(unique_responses) * 0.9:
    # Also check for substring overlaps to be sure
    has_overlap = False
    unique_list = list(unique_responses)
    for i, resp1 in enumerate(unique_list):
      for resp2 in unique_list[i+1:]:
        # Check if there's significant substring overlap
        if len(resp1) > 10 and len(resp2) > 10:
          shorter = resp1 if len(resp1) < len(resp2) else resp2
          longer = resp2 if len(resp1) < len(resp2) else resp1
          if shorter.lower() in longer.lower():
            has_overlap = True
            break
      if has_overlap:
        break
    
    # If no overlap and most responses are unique, it's qualitative
    if not has_overlap:
      return False
  
  # If there are very few unique responses relative to total, it's likely quantitative
  if len(unique_responses) <= 10 and len(responses) > 3:
    return True
  
  # Check if responses are numeric
  numeric_count = 0
  for resp in responses:
    try:
      float(resp)
      numeric_count += 1
    except (ValueError, TypeError):
      pass
  
  if numeric_count > len(responses) * 0.5:
    return True
  
  # Check average response length - short responses suggest quantitative
  avg_length = sum(len(str(r)) for r in responses) / len(responses)
  if avg_length < 30:
    return True
  
  return False
@


\subsection{Rendering output}

We need to render the output buffer in the appropriate format.
For markdown, we use the [[rich]] package to render it nicely.
When stdout is a terminal, we use a pager (similar to submissions).
When piped to a file, we output plain markdown without the pager.
For LaTeX, we just output the raw LaTeX without a pager (always goes to a file).
<<render output>>=
output_text = ''.join(output_buffer)

if args.format == "markdown":
  # Use rich to render markdown
  console = Console()
  md = Markdown(output_text)
  
  if sys.stdout.isatty():
    # Output to terminal with pager
    pager = ""
    if "MANPAGER" in os.environ:
      pager = os.environ["MANPAGER"]
    elif "PAGER" in os.environ:
      pager = os.environ["PAGER"]
    
    styles = False
    if "less" in pager and ("-R" in pager or "-r" in pager):
      styles = True
    
    with console.pager(styles=styles):
      console.print(md)
  else:
    # Piped to file, output plain markdown
    print(output_text)
else:  # latex
  # Output raw LaTeX (no pager, always goes to file)
  print(output_text)
@


\subsection{Summarizing quantitative data}

For quantitative data, we compute statistics and output in the requested format.
<<summarize quantitative data>>=
if quantitative_questions:
  <<output quantitative header>>
  
  for qcol in quantitative_questions:
    <<output question header>>
    
    responses = [row[qcol] for row in rows if row[qcol] and row[qcol].strip()]
    
    if not responses:
      <<output no responses>>
      continue
    
    <<compute and output statistics>>
@

We need to handle both markdown and LaTeX output formats.
<<output quantitative header>>=
if args.format == "markdown":
  output_buffer.append("\n# Quantitative Summary\n")
else:  # latex
  output_buffer.append("\\section{Quantitative Summary}\n\n")
@

<<output question header>>=
if args.format == "markdown":
  output_buffer.append(f"\n## {qcol}\n")
else:  # latex
  # Escape LaTeX special characters
  question_escaped = qcol.replace('_', '\\_').replace('%', '\\%').replace('&', '\\&')
  output_buffer.append(f"\\subsection{{{question_escaped}}}\n\n")
@

<<output no responses>>=
if args.format == "markdown":
  output_buffer.append("*No responses*\n")
else:  # latex
  output_buffer.append("\\textit{No responses}\n\n")
@

We compute different statistics depending on the type of data.
For comma-separated responses, we need to split them and count individual options.
<<compute and output statistics>>=
# Check if responses contain comma-separated lists
has_commas = sum(1 for r in responses if ',' in r) > len(responses) * 0.3

if has_commas:
  # Extract and count individual options
  all_options = extract_comma_separated_options(responses)
  freq = Counter(all_options)
  
  if args.format == "markdown":
    output_buffer.append(f"**Total responses:** {len(responses)}  \n")
    output_buffer.append(f"**Total selections:** {len(all_options)}  \n")
    output_buffer.append("\n**Option distribution:**\n\n")
    for value, count in freq.most_common():
      percentage = (count / len(responses)) * 100
      value_display = value.replace('\n', ' ')[:70]
      output_buffer.append(f"- {value_display}: {count} ({percentage:.1f}%)\n")
  else:  # latex
    output_buffer.append(f"Total responses: {len(responses)}\\\\\n")
    output_buffer.append(f"Total selections: {len(all_options)}\\\\\n\n")
    output_buffer.append("\\textbf{Option distribution:}\n\\begin{itemize}\n")
    for value, count in freq.most_common():
      percentage = (count / len(responses)) * 100
      value_escaped = value.replace('_', '\\_').replace('%', '\\%').replace('&', '\\&')
      output_buffer.append(f"  \\item {value_escaped}: {count} ({percentage:.1f}\\%)\n")
    output_buffer.append("\\end{itemize}\n\n")
else:
  # Try to parse as numbers first
  numeric_values = []
  for resp in responses:
    try:
      numeric_values.append(float(resp))
    except (ValueError, TypeError):
      pass

  if numeric_values and len(numeric_values) >= len(responses) * 0.5:
    # Numeric data
    if args.format == "markdown":
      output_buffer.append(f"**Total responses:** {len(numeric_values)}  \n")
      output_buffer.append(f"**Mean:** {statistics.mean(numeric_values):.2f}  \n")
      output_buffer.append(f"**Median:** {statistics.median(numeric_values):.2f}  \n")
      if len(numeric_values) > 1:
        output_buffer.append(f"**Std Dev:** {statistics.stdev(numeric_values):.2f}  \n")
      output_buffer.append(f"**Min:** {min(numeric_values):.2f}  \n")
      output_buffer.append(f"**Max:** {max(numeric_values):.2f}  \n")
    else:  # latex
      output_buffer.append(f"Total responses: {len(numeric_values)}\\\\\n")
      output_buffer.append(f"Mean: {statistics.mean(numeric_values):.2f}\\\\\n")
      output_buffer.append(f"Median: {statistics.median(numeric_values):.2f}\\\\\n")
      if len(numeric_values) > 1:
        output_buffer.append(f"Standard deviation: {statistics.stdev(numeric_values):.2f}\\\\\n")
      output_buffer.append(f"Min: {min(numeric_values):.2f}\\\\\n")
      output_buffer.append(f"Max: {max(numeric_values):.2f}\\\\\n\n")
  else:
    # Categorical data - show frequency distribution
    freq = Counter(responses)
    if args.format == "markdown":
      output_buffer.append(f"**Total responses:** {len(responses)}  \n")
      output_buffer.append("\n**Response distribution:**\n\n")
      for value, count in freq.most_common():
        percentage = (count / len(responses)) * 100
        value_display = value.replace('\n', ' ')[:70]
        output_buffer.append(f"- {value_display}: {count} ({percentage:.1f}%)\n")
    else:  # latex
      output_buffer.append(f"Total responses: {len(responses)}\\\\\n\n")
      output_buffer.append("\\textbf{Response distribution:}\n\\begin{itemize}\n")
      for value, count in freq.most_common():
        percentage = (count / len(responses)) * 100
        value_escaped = value.replace('\n', ' ').replace('_', '\\_').replace('%', '\\%').replace('&', '\\&')[:70]
        output_buffer.append(f"  \\item {value_escaped}: {count} ({percentage:.1f}\\%)\n")
      output_buffer.append("\\end{itemize}\n\n")
@


\subsection{Summarizing qualitative data}

For qualitative data, we show all responses and generate an AI summary.
<<summarize qualitative data>>=
if qualitative_questions:
  <<output qualitative header>>
  
  for qcol in qualitative_questions:
    <<output question header>>
    
    responses = [row[qcol] for row in rows 
                 if row[qcol] and row[qcol].strip()]
    
    if not responses:
      <<output no responses>>
      continue
    
    <<output individual responses>>
    
    <<generate and output AI summary>>
@

<<output qualitative header>>=
if args.format == "markdown":
  output_buffer.append("\n# Qualitative Summary\n")
else:  # latex
  output_buffer.append("\\section{Qualitative Summary}\n\n")
@

<<output individual responses>>=
if args.format == "markdown":
  output_buffer.append(f"\n**Individual Responses ({len(responses)} total):**\n\n")
  for i, resp in enumerate(responses, 1):
    output_buffer.append(f"{i}. {resp}\n\n")
else:  # latex
  output_buffer.append(f"\\textbf{{Individual Responses ({len(responses)} total):}}\n\n")
  output_buffer.append("\\begin{enumerate}\n")
  for i, resp in enumerate(responses, 1):
    resp_escaped = resp.replace('_', '\\_').replace('%', '\\%').replace('&', '\\&').replace('\n', ' ')
    output_buffer.append(f"  \\item {resp_escaped}\n")
  output_buffer.append("\\end{enumerate}\n\n")
@


\section{AI summary generation}

We use the [[llm]] package to generate summaries of qualitative responses.
The AI summary generation can be disabled with the [[--no-ai]] option.
<<generate and output AI summary>>=
if args.ai:
  if args.format == "markdown":
    output_buffer.append("\n**AI-Generated Summary:**\n\n")
  else:  # latex
    output_buffer.append("\\textbf{AI-Generated Summary:}\n\n")

  try:
    import llm
    
    # Prepare the prompt based on output format
    if args.format == "latex":
      prompt = f"""Please analyze the following survey responses and provide a concise summary of the main themes, concerns, and suggestions mentioned by respondents.

Format your response in LaTeX. Use LaTeX formatting such as \\textbf{{}} for bold, \\textit{{}} for italics, and \\begin{{itemize}} for lists. Do not include section headers (like \\section or \\subsection).

Question: {qcol}

Responses:
"""
    else:  # markdown
      prompt = f"""Please analyze the following survey responses and provide a concise summary of the main themes, concerns, and suggestions mentioned by respondents.

Format your response in markdown. Use markdown formatting such as **bold**, *italics*, and bullet points.

Question: {qcol}

Responses:
"""
    
    for i, resp in enumerate(responses, 1):
      prompt += f"\n{i}. {resp}"
    
    prompt += "\n\nProvide a summary highlighting:\n1. Main themes\n2. Common concerns or issues\n3. Suggestions for improvement\n4. Overall sentiment"
    
    # Get default model and generate summary
    model = llm.get_model()
    response = model.prompt(prompt)
    summary_text = response.text()
    
    if args.format == "markdown":
      output_buffer.append(f"{summary_text}\n\n")
    else:  # latex
      # For LaTeX format, the AI already generated LaTeX, so don't escape
      output_buffer.append(f"{summary_text}\n\n")
    
  except ImportError:
    error_msg = "The 'llm' package is not installed. Install it with: pip install llm"
    if args.format == "markdown":
      output_buffer.append(f"*{error_msg}*\n\n")
    else:  # latex
      output_buffer.append(f"\\textit{{{error_msg}}}\n\n")
  except Exception as e:
    error_msg = f"Error generating AI summary: {e}\nMake sure llm is configured with: llm keys set <provider>"
    if args.format == "markdown":
      output_buffer.append(f"*{error_msg}*\n\n")
    else:  # latex
      output_buffer.append(f"\\textit{{{error_msg}}}\n\n")
@


\section{Fetching from Canvas}

For Canvas API mode, we need to fetch quiz data and process it similarly.
We need to search for quizzes in two places:
\begin{enumerate}
\item Assignments with [[is_quiz_assignment]] attribute (New Quizzes/Quizzes.Next)
\item Classic quizzes (accessed via [[get_quizzes()]])
\end{enumerate}

Both quiz types are handled by generating a student analysis report, polling
until complete, and downloading the resulting CSV for processing.
<<fetch from Canvas and process>>=
# Ensure we have course and assignment specified
if not hasattr(args, 'course') or not args.course:
  canvaslms.cli.err(1, "Course must be specified when not using CSV file")

try:
  # Get the course list
  course_list = courses.process_course_option(canvas, args)
  
  if not course_list:
    canvaslms.cli.err(1, "No course found matching criteria")
  
  for course in course_list:
    # Search for quiz in two places, preferring Classic Quizzes (more reliable API)
    quiz = None

    # First, try to find in Classic Quizzes (accessed via get_quizzes())
    # These have reliable API support for report generation
    <<search for classic quiz>>

    # If not found in Classic Quizzes, try New Quizzes via get_new_quizzes()
    if quiz is None:
      <<search for new quiz>>
    
    if quiz is None:
      canvaslms.cli.warn(f"No quiz/survey found matching criteria in {course.course_code}")
      continue

    <<fetch quiz submissions from Canvas>>
    <<process Canvas quiz data>>
  
except canvaslms.cli.EmptyListError as e:
  canvaslms.cli.err(1, str(e))
except Exception as e:
  canvaslms.cli.err(1, f"Error fetching from Canvas: {e}")
@

\subsection{Searching for classic quizzes}

Classic quizzes (including anonymous surveys) are not accessible through the
assignments API. We need to search for them using [[get_quizzes()]].
<<search for classic quiz>>=
# Get all quizzes in the course
quizzes = course.get_quizzes()

# Build regex pattern from assignment argument
if hasattr(args, 'assignment') and args.assignment:
  pattern = re.compile(args.assignment, re.IGNORECASE)

  for q in quizzes:
    # Match against quiz title or ID
    if pattern.search(q.title) or pattern.search(str(q.id)):
      quiz = q
      break
@

\subsection{Searching for New Quizzes}

New Quizzes are retrieved via [[get_new_quizzes()]] API method.
<<search for new quiz>>=
try:
  new_quizzes = course.get_new_quizzes()

  # Build regex pattern from assignment argument
  if hasattr(args, 'assignment') and args.assignment:
    pattern = re.compile(args.assignment, re.IGNORECASE)

    for q in new_quizzes:
      # Match against quiz title or ID
      if pattern.search(q.title) or pattern.search(str(q.id)):
        quiz = q
        break
except Exception as e:
  # New Quizzes API might not be available
  canvaslms.cli.warn(f"Could not search New Quizzes: {e}")
@

We need to fetch quiz submissions and extract the data to analyze.
For classic quizzes, we can use [[get_quiz_report]] to get the student analysis report.
<<fetch quiz submissions from Canvas>>=
# Check if this is a Classic Quiz or New Quiz and handle accordingly
try:
  if is_new_quiz(quiz):
    <<fetch New Quiz report>>
  else:
    <<fetch Classic Quiz report>>
except AttributeError:
  # Quiz doesn't support the report API
  print(f"Note: Canvas API quiz data processing is limited for this quiz type.")
  print(f"For best results, export the Student Analysis Report as CSV and use --csv option.")
  print(f"\nTo export: Go to Quiz -> ... menu -> Student Analysis")
  print(f"Download the CSV and run: canvaslms quizzes analyse --csv <file.csv>")
  sys.exit(0)
except Exception as e:
  canvaslms.cli.warn(f"Could not fetch quiz report via API: {e}")
  print(f"\nNote: For best results, export the Student Analysis Report as CSV manually.")
  print(f"To export: Go to Quiz -> ... menu -> Student Analysis")
  print(f"Download the CSV and run: canvaslms quizzes analyse --csv <file.csv>")
  sys.exit(1)
@

\subsection{Fetching Classic Quiz reports}

Classic quizzes use the [[create_report()]] method which returns a report
object. We poll until complete, then download the CSV.
<<fetch Classic Quiz report>>=
# Try to create a student analysis report (works for surveys, graded_surveys, etc.)
try:
  report = quiz.create_report(report_type='student_analysis', includes_all_versions=True)
except AttributeError:
  # Quiz doesn't support create_report, fall back to manual export
  print(f"Note: Canvas API quiz data processing is limited for this quiz type.")
  print(f"For best results, export the Student Analysis Report as CSV and use --csv option.")
  print(f"\nTo export: Go to Quiz -> ... menu -> Student Analysis")
  print(f"Download the CSV and run: canvaslms quizzes analyse --csv <file.csv>")
  sys.exit(0)

# Poll until the report is ready
# We need to refresh the report object to check progress
for attempt in range(30):
  report = quiz.get_quiz_report(report.id)
  final_report = poll_progress(report, max_attempts=1)
  if final_report:
    report = final_report
    break

# Check if we have the file URL
file_url = None
if hasattr(report, 'file'):
  if hasattr(report.file, 'url'):
    file_url = report.file.url
  elif isinstance(report.file, dict):
    file_url = report.file.get('url')

if file_url:
  # Download and parse the CSV data
  reader = download_csv_report(file_url)
  # Process the CSV data
  <<process quiz CSV data from API>>
else:
  canvaslms.cli.err(1, "Report file URL not available")
@

\subsection{Fetching New Quiz reports}

New Quizzes use a different API endpoint. We call the helper to create the
report, which returns a Progress object directly.

\textbf{Known Issue:} The New Quiz Reports API was recently introduced in
January 2025. Testing shows that the API may return ``Not Found'' errors on
some Canvas instances, possibly due to:
\begin{itemize}
\item The API not being enabled on all Canvas instances
\item Missing permissions or feature flags
\item New Quizzes with no student submissions
\end{itemize}

The implementation is correct according to the Canvas API documentation. If
the API call fails, users should use the CSV export method as described in
the introduction.
<<fetch New Quiz report>>=
# Create the report using the New Quiz API
# Note: Canvas uses name-mangled private attribute __requester
progress = create_new_quiz_report(course, quiz.id, canvas._Canvas__requester)

# Poll until the report is ready
progress = poll_progress(progress)

if not progress:
  canvaslms.cli.err(1, "Report generation timed out")

# Extract the file URL from the progress results
file_url = None
if hasattr(progress, 'results'):
  if isinstance(progress.results, dict):
    file_url = progress.results.get('url')

if not file_url:
  canvaslms.cli.err(1, "Report file URL not available from New Quiz report")

# Download and parse the CSV data
reader = download_csv_report(file_url)
# Process the CSV data
<<process quiz CSV data from API>>
@

<<process quiz CSV data from API>>=
# Convert the CSV reader data into the same format as our file processing
rows = list(reader)
if not rows:
  canvaslms.cli.err(1, "No data in quiz report")

# Process it the same way as CSV file processing
<<categorize and summarize quiz data>>
@

<<process Canvas quiz data>>=
# This section intentionally left empty for future implementation
@
