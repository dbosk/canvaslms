\chapter{The \texttt{quizzes} command}
\label{quizzes-command}

This chapter provides the subcommand [[quizzes]], which summarizes Canvas quiz 
or survey evaluation data.

The command supports two modes of operation:
\begin{enumerate}
\item Fetch quiz/survey data directly from Canvas by specifying the quiz.
\item Read and analyze a CSV file downloaded from Canvas.
\end{enumerate}

The command provides:
\begin{itemize}
\item Statistical summaries for quantitative (multiple choice, rating) data
\item Individual responses for qualitative (free text) data
\item AI-generated summaries of qualitative data using the [[llm]] package
\end{itemize}


\section{Usage Examples}

\subsection{Analyzing a CSV file}

The most common usage is to download the Student Analysis Report CSV from 
Canvas and analyze it:

\begin{minted}{bash}
canvaslms quizzes --csv survey_results.csv
\end{minted}

To download the CSV from Canvas:
\begin{enumerate}
\item Navigate to your quiz/survey in Canvas
\item Click the three-dot menu (\ldots) in the upper right
\item Select \enquote{Student Analysis}
\item Download the CSV file
\item Run the command above with the downloaded file
\end{enumerate}

\subsection{AI Summaries}

The command uses the [[llm]] package for AI-generated summaries of qualitative 
responses. Before using this feature, you need to configure [[llm]] with your 
API keys:

\begin{minted}{bash}
# Configure OpenAI
llm keys set openai

# Or configure another provider (e.g., Anthropic)
llm keys set anthropic

# Test the configuration
llm "Hello, world!"
\end{minted}

Once configured, the [[quizzes]] command will automatically generate summaries 
for all qualitative (free-text) responses.

\subsection{Output Format}

The command produces two types of output:

\paragraph{Quantitative Summary} For questions with limited response options 
(multiple choice, ratings, etc.), the command shows:
\begin{itemize}
\item Total number of responses
\item For numeric data: mean, median, standard deviation, min, and max
\item For categorical data: frequency distribution with percentages
\item For multi-select questions (comma-separated): total selections and 
  individual option counts
\end{itemize}

\paragraph{Qualitative Summary} For free-text questions, the command shows:
\begin{itemize}
\item All individual responses (numbered)
\item An AI-generated summary highlighting main themes, concerns, suggestions, 
  and overall sentiment
\end{itemize}

\paragraph{Output Formats} The command supports two output formats:
\begin{itemize}
\item \textbf{Markdown} (default): Rendered beautifully using the [[rich]] 
  package with styled headers and formatting
\item \textbf{LaTeX}: Raw LaTeX output ready to be compiled into a PDF document
\end{itemize}

To select the output format:
\begin{minted}{bash}
# Markdown output (default)
canvaslms quizzes --csv survey.csv

# LaTeX output
canvaslms quizzes --csv survey.csv --format latex > report.tex
pdflatex report.tex
\end{minted}


\section{Module outline}

We outline the module:
<<quizzes.py>>=
import argparse
import csv
import json
import os
import re
import statistics
import sys
from collections import defaultdict, Counter
from typing import Dict, List, Any

import canvaslms.cli
import canvaslms.cli.courses as courses
import canvaslms.cli.assignments as assignments
from rich.console import Console
from rich.markdown import Markdown

<<helper functions>>

def add_command(subp):
  """Adds the quizzes command to argparse parser subp"""
  <<add quizzes command to subp>>

def quizzes_command(config, canvas, args):
  """Main command handler for quizzes"""
  <<process quizzes command>>
@


\section{The [[quizzes]] subcommand and its options}

We add the subparser for [[quizzes]].
The command can either work with a CSV file or fetch data from Canvas.
<<add quizzes command to subp>>=
quizzes_parser = subp.add_parser("quizzes",
    help="Summarize quiz/survey evaluation data",
    description="""Summarizes Canvas quiz or survey evaluation data.
    
Can either fetch quiz data from Canvas or analyze a downloaded CSV file.
Provides statistical summaries for quantitative data and AI-generated 
summaries for qualitative (free text) responses.""")

quizzes_parser.set_defaults(func=quizzes_command)

<<add quizzes options>>
@

We need options for:
\begin{itemize}
\item CSV file input
\item Course selection (for Canvas API mode)
\item Assignment/quiz selection (for Canvas API mode)
\item Output format selection (markdown or LaTeX)
\end{itemize}
<<add quizzes options>>=
quizzes_parser.add_argument("--csv", "-f",
    help="Path to CSV file downloaded from Canvas",
    type=str)

quizzes_parser.add_argument("--format", "-F",
    help="Output format: markdown (default) or latex",
    choices=["markdown", "latex"],
    default="markdown")

try:
  courses.add_course_option(quizzes_parser, required=False)
except argparse.ArgumentError:
  pass

try:
  assignments.add_assignment_option(quizzes_parser, 
      ungraded=False, required=False)
except argparse.ArgumentError:
  pass
@


\section{Processing the command}

The main command processing determines whether to use CSV file or Canvas API.
<<process quizzes command>>=
if args.csv:
  <<process CSV file>>
else:
  <<fetch from Canvas and process>>
@


\section{CSV file processing}

When processing a CSV file, we need to:
\begin{enumerate}
\item Parse the CSV structure
\item Identify question columns
\item Separate quantitative and qualitative questions
\item Generate summaries
\end{enumerate}
<<process CSV file>>=
if not os.path.exists(args.csv):
  canvaslms.cli.err(1, f"CSV file not found: {args.csv}")

try:
  with open(args.csv, 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    rows = list(reader)
    
    if not rows:
      canvaslms.cli.err(1, "CSV file is empty")
    
    <<parse and summarize CSV data>>
except Exception as e:
  canvaslms.cli.err(1, f"Error processing CSV: {e}")
@


\section{Parsing and summarizing CSV data}

The Canvas CSV format has a complex structure where questions appear as column 
headers with their IDs.
We need to identify which columns are questions and which are metadata.
<<parse and summarize CSV data>>=
# Initialize output buffer
output_buffer = []

# Get all column names
columns = list(rows[0].keys())

# Identify question columns (they contain question IDs like "588913:")
question_columns = []
for col in columns:
  if re.match(r'^\d+:', col):
    question_columns.append(col)

if not question_columns:
  canvaslms.cli.err(1, "No question columns found in CSV")

# Categorize questions
quantitative_questions = []
qualitative_questions = []

for qcol in question_columns:
  # Check if the column contains mostly numeric/categorical responses
  sample_responses = [row[qcol] for row in rows if row[qcol]]
  
  if is_quantitative(sample_responses):
    quantitative_questions.append(qcol)
  else:
    qualitative_questions.append(qcol)

<<summarize quantitative data>>
<<summarize qualitative data>>
<<render output>>
@


\section{Helper functions}

\subsection{Determining question type}

We need to determine if a question is quantitative or qualitative based on the 
responses.
For questions where Canvas lists comma-separated options (like multi-select 
questions), we need to detect when substrings reoccur systematically.
<<helper functions>>=
def extract_comma_separated_options(responses: List[str]) -> List[str]:
  """Extract individual options from comma-separated responses"""
  all_options = []
  for resp in responses:
    if ',' in resp:
      # Split by comma and clean up
      options = [opt.strip() for opt in resp.split(',')]
      all_options.extend(options)
    else:
      all_options.append(resp)
  return all_options

def is_quantitative(responses: List[str]) -> bool:
  """Determine if responses are quantitative (numbers/categories) or qualitative (free text)"""
  if not responses:
    return False
  
  # Check if responses contain comma-separated lists that suggest multi-select
  # If many responses have commas and we see repeated substrings, it's quantitative
  comma_count = sum(1 for r in responses if ',' in r)
  if comma_count > len(responses) * 0.3:  # More than 30% have commas
    # Extract all individual options
    all_options = extract_comma_separated_options(responses)
    unique_options = set(all_options)
    
    # If we have a reasonable number of unique options that repeat, it's quantitative
    if len(unique_options) <= 20 and len(all_options) > len(unique_options):
      return True
  
  # If responses are very short or repetitive, likely quantitative
  # Check for common patterns
  unique_responses = set(responses)
  
  # If there are very few unique responses relative to total, it's likely quantitative
  if len(unique_responses) <= 10 and len(responses) > 3:
    return True
  
  # Check if responses are numeric
  numeric_count = 0
  for resp in responses:
    try:
      float(resp)
      numeric_count += 1
    except (ValueError, TypeError):
      pass
  
  if numeric_count > len(responses) * 0.5:
    return True
  
  # Check average response length - short responses suggest quantitative
  avg_length = sum(len(str(r)) for r in responses) / len(responses)
  if avg_length < 30:
    return True
  
  return False
@


\subsection{Rendering output}

We need to render the output buffer in the appropriate format.
For markdown, we use the [[rich]] package to render it nicely.
For LaTeX, we just output the raw LaTeX.
<<render output>>=
output_text = ''.join(output_buffer)

if args.format == "markdown":
  # Use rich to render markdown
  console = Console()
  md = Markdown(output_text)
  console.print(md)
else:  # latex
  # Output raw LaTeX
  print(output_text)
@


\subsection{Summarizing quantitative data}

For quantitative data, we compute statistics and output in the requested format.
<<summarize quantitative data>>=
if quantitative_questions:
  <<output quantitative header>>
  
  for qcol in quantitative_questions:
    <<output question header>>
    
    responses = [row[qcol] for row in rows if row[qcol] and row[qcol].strip()]
    
    if not responses:
      <<output no responses>>
      continue
    
    <<compute and output statistics>>
@

We need to handle both markdown and LaTeX output formats.
<<output quantitative header>>=
if args.format == "markdown":
  output_buffer.append("\n# Quantitative Summary\n")
else:  # latex
  output_buffer.append("\\section{Quantitative Summary}\n\n")
@

<<output question header>>=
if args.format == "markdown":
  output_buffer.append(f"\n## {qcol}\n")
else:  # latex
  # Escape LaTeX special characters
  question_escaped = qcol.replace('_', '\\_').replace('%', '\\%').replace('&', '\\&')
  output_buffer.append(f"\\subsection{{{question_escaped}}}\n\n")
@

<<output no responses>>=
if args.format == "markdown":
  output_buffer.append("*No responses*\n")
else:  # latex
  output_buffer.append("\\textit{No responses}\n\n")
@

We compute different statistics depending on the type of data.
For comma-separated responses, we need to split them and count individual options.
<<compute and output statistics>>=
# Check if responses contain comma-separated lists
has_commas = sum(1 for r in responses if ',' in r) > len(responses) * 0.3

if has_commas:
  # Extract and count individual options
  all_options = extract_comma_separated_options(responses)
  freq = Counter(all_options)
  
  if args.format == "markdown":
    output_buffer.append(f"**Total responses:** {len(responses)}  \n")
    output_buffer.append(f"**Total selections:** {len(all_options)}  \n")
    output_buffer.append("\n**Option distribution:**\n\n")
    for value, count in freq.most_common():
      percentage = (count / len(responses)) * 100
      value_display = value.replace('\n', ' ')[:70]
      output_buffer.append(f"- {value_display}: {count} ({percentage:.1f}%)\n")
  else:  # latex
    output_buffer.append(f"Total responses: {len(responses)}\\\\\n")
    output_buffer.append(f"Total selections: {len(all_options)}\\\\\n\n")
    output_buffer.append("\\textbf{Option distribution:}\n\\begin{itemize}\n")
    for value, count in freq.most_common():
      percentage = (count / len(responses)) * 100
      value_escaped = value.replace('_', '\\_').replace('%', '\\%').replace('&', '\\&')
      output_buffer.append(f"  \\item {value_escaped}: {count} ({percentage:.1f}\\%)\n")
    output_buffer.append("\\end{itemize}\n\n")
else:
  # Try to parse as numbers first
  numeric_values = []
  for resp in responses:
    try:
      numeric_values.append(float(resp))
    except (ValueError, TypeError):
      pass

  if numeric_values and len(numeric_values) >= len(responses) * 0.5:
    # Numeric data
    if args.format == "markdown":
      output_buffer.append(f"**Total responses:** {len(numeric_values)}  \n")
      output_buffer.append(f"**Mean:** {statistics.mean(numeric_values):.2f}  \n")
      output_buffer.append(f"**Median:** {statistics.median(numeric_values):.2f}  \n")
      if len(numeric_values) > 1:
        output_buffer.append(f"**Std Dev:** {statistics.stdev(numeric_values):.2f}  \n")
      output_buffer.append(f"**Min:** {min(numeric_values):.2f}  \n")
      output_buffer.append(f"**Max:** {max(numeric_values):.2f}  \n")
    else:  # latex
      output_buffer.append(f"Total responses: {len(numeric_values)}\\\\\n")
      output_buffer.append(f"Mean: {statistics.mean(numeric_values):.2f}\\\\\n")
      output_buffer.append(f"Median: {statistics.median(numeric_values):.2f}\\\\\n")
      if len(numeric_values) > 1:
        output_buffer.append(f"Standard deviation: {statistics.stdev(numeric_values):.2f}\\\\\n")
      output_buffer.append(f"Min: {min(numeric_values):.2f}\\\\\n")
      output_buffer.append(f"Max: {max(numeric_values):.2f}\\\\\n\n")
  else:
    # Categorical data - show frequency distribution
    freq = Counter(responses)
    if args.format == "markdown":
      output_buffer.append(f"**Total responses:** {len(responses)}  \n")
      output_buffer.append("\n**Response distribution:**\n\n")
      for value, count in freq.most_common():
        percentage = (count / len(responses)) * 100
        value_display = value.replace('\n', ' ')[:70]
        output_buffer.append(f"- {value_display}: {count} ({percentage:.1f}%)\n")
    else:  # latex
      output_buffer.append(f"Total responses: {len(responses)}\\\\\n\n")
      output_buffer.append("\\textbf{Response distribution:}\n\\begin{itemize}\n")
      for value, count in freq.most_common():
        percentage = (count / len(responses)) * 100
        value_escaped = value.replace('\n', ' ').replace('_', '\\_').replace('%', '\\%').replace('&', '\\&')[:70]
        output_buffer.append(f"  \\item {value_escaped}: {count} ({percentage:.1f}\\%)\n")
      output_buffer.append("\\end{itemize}\n\n")
@


\subsection{Summarizing qualitative data}

For qualitative data, we show all responses and generate an AI summary.
<<summarize qualitative data>>=
if qualitative_questions:
  <<output qualitative header>>
  
  for qcol in qualitative_questions:
    <<output question header>>
    
    responses = [row[qcol] for row in rows 
                 if row[qcol] and row[qcol].strip()]
    
    if not responses:
      <<output no responses>>
      continue
    
    <<output individual responses>>
    
    <<generate and output AI summary>>
@

<<output qualitative header>>=
if args.format == "markdown":
  output_buffer.append("\n# Qualitative Summary\n")
else:  # latex
  output_buffer.append("\\section{Qualitative Summary}\n\n")
@

<<output individual responses>>=
if args.format == "markdown":
  output_buffer.append(f"\n**Individual Responses ({len(responses)} total):**\n\n")
  for i, resp in enumerate(responses, 1):
    output_buffer.append(f"{i}. {resp}\n\n")
else:  # latex
  output_buffer.append(f"\\textbf{{Individual Responses ({len(responses)} total):}}\n\n")
  output_buffer.append("\\begin{enumerate}\n")
  for i, resp in enumerate(responses, 1):
    resp_escaped = resp.replace('_', '\\_').replace('%', '\\%').replace('&', '\\&').replace('\n', ' ')
    output_buffer.append(f"  \\item {resp_escaped}\n")
  output_buffer.append("\\end{enumerate}\n\n")
@


\section{AI summary generation}

We use the [[llm]] package to generate summaries of qualitative responses.
<<generate and output AI summary>>=
if args.format == "markdown":
  output_buffer.append("\n**AI-Generated Summary:**\n\n")
else:  # latex
  output_buffer.append("\\textbf{AI-Generated Summary:}\n\n")

try:
  import llm
  
  # Prepare the prompt based on output format
  if args.format == "latex":
    prompt = f"""Please analyze the following survey responses and provide a concise summary of the main themes, concerns, and suggestions mentioned by respondents.

Format your response in LaTeX. Use LaTeX formatting such as \\textbf{{}} for bold, \\textit{{}} for italics, and \\begin{{itemize}} for lists. Do not include section headers (like \\section or \\subsection).

Question: {qcol}

Responses:
"""
  else:  # markdown
    prompt = f"""Please analyze the following survey responses and provide a concise summary of the main themes, concerns, and suggestions mentioned by respondents.

Format your response in markdown. Use markdown formatting such as **bold**, *italics*, and bullet points.

Question: {qcol}

Responses:
"""
  
  for i, resp in enumerate(responses, 1):
    prompt += f"\n{i}. {resp}"
  
  prompt += "\n\nProvide a summary highlighting:\n1. Main themes\n2. Common concerns or issues\n3. Suggestions for improvement\n4. Overall sentiment"
  
  # Get default model and generate summary
  model = llm.get_model()
  response = model.prompt(prompt)
  summary_text = response.text()
  
  if args.format == "markdown":
    output_buffer.append(f"{summary_text}\n\n")
  else:  # latex
    # For LaTeX format, the AI already generated LaTeX, so don't escape
    output_buffer.append(f"{summary_text}\n\n")
  
except ImportError:
  error_msg = "The 'llm' package is not installed. Install it with: pip install llm"
  if args.format == "markdown":
    output_buffer.append(f"*{error_msg}*\n\n")
  else:  # latex
    output_buffer.append(f"\\textit{{{error_msg}}}\n\n")
except Exception as e:
  error_msg = f"Error generating AI summary: {e}\nMake sure llm is configured with: llm keys set <provider>"
  if args.format == "markdown":
    output_buffer.append(f"*{error_msg}*\n\n")
  else:  # latex
    output_buffer.append(f"\\textit{{{error_msg}}}\n\n")
@


\section{Fetching from Canvas}

For Canvas API mode, we need to fetch quiz data and process it similarly.
<<fetch from Canvas and process>>=
# Ensure we have course and assignment specified
if not hasattr(args, 'course') or not args.course:
  canvaslms.cli.err(1, "Course must be specified when not using CSV file")

try:
  # Get the quiz/assignment
  assignment_list = assignments.process_assignment_option(canvas, args)
  
  if not assignment_list:
    canvaslms.cli.err(1, "No quiz/survey found matching criteria")
  
  if len(assignment_list) > 1:
    canvaslms.cli.warn("Multiple assignments found, using first one")
  
  quiz = assignment_list[0]
  
  <<fetch quiz submissions from Canvas>>
  <<process Canvas quiz data>>
  
except canvaslms.cli.EmptyListError as e:
  canvaslms.cli.err(1, str(e))
except Exception as e:
  canvaslms.cli.err(1, f"Error fetching from Canvas: {e}")
@

For now, we'll implement a basic version that suggests using CSV export.
<<fetch quiz submissions from Canvas>>=
print(f"Note: Canvas API quiz data processing is limited.")
print(f"For best results, export the Student Analysis Report as CSV and use --csv option.")
print(f"\nTo export: Go to Quiz -> ... menu -> Student Analysis")
print(f"Download the CSV and run: canvaslms quizzes --csv <file.csv>")
sys.exit(0)
@

<<process Canvas quiz data>>=
# This section intentionally left empty for future implementation
@
