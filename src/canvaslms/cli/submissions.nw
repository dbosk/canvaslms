\chapter{The submissions related commands}

This chapter provides the subcommands [[submissions]], which lists the 
submissions of a given assignment, and the [[submission]] command, which 
handles an individual submission.

We outline the module:
<<submissions.py>>=
import canvasapi.file
import canvasapi.submission

import canvaslms.cli.assignments as assignments
import canvaslms.cli.users as users
import canvaslms.hacks.canvasapi

import argparse
import csv
import json
import os
import pathlib
import subprocess
import pypandoc
import re
import rich.console
import rich.markdown
import sys
import tempfile
import urllib.request

<<functions>>

def add_command(subp):
  """Adds the submissions and submission commands to argparse parser subp"""
  add_submissions_command(subp)
  add_submission_command(subp)

def add_submissions_command(subp):
  """Adds submissions command to argparse parser subp"""
  <<add submissions command to subp>>

def add_submission_command(subp):
  """Adds submission command to argparse parser subp"""
  <<add submission command to subp>>
@

\section{The [[submissions]] subcommand and its options}

We add the subparser for [[submissions]].
<<add submissions command to subp>>=
submissions_parser = subp.add_parser("submissions",
    help="Lists submissions of an assignment",
    description="Lists submissions of assignment(s). Output format: "
      "<course code> <assignment name> <user> <grade> "
      "<submission date> <grade date>")
submissions_parser.set_defaults(func=submissions_command)
assignments.add_assignment_option(submissions_parser)
add_submission_options(submissions_parser)
<<add options for printing>>
@ Now, that [[submissions_command]] function must take three arguments: 
[[config]], [[canvas]] and [[args]].
It must also do the processing for the assignment options using 
[[process_assignment_option]].
<<functions>>=
def submissions_command(config, canvas, args):
  assignment_list = assignments.process_assignment_option(canvas, args)
  <<get submissions and print them>>
@

\subsection{Get and print the list of submissions}

We then simply call the appropriate list-submissions function with the 
[[assignments_list]] object as a parameter.
If any of the group options are set, we filter out the submissions to only 
include submissions by the users in the groups.
Then we will print the most useful attributes of a submission, which the 
[[format_submission_short]] will return preformatted for us.
<<get submissions and print them>>=
if args.ungraded:
  submissions = list_ungraded_submissions(assignment_list)
else:
  submissions = list_submissions(assignment_list)

if args.user or args.category or args.group:
  user_list = users.process_user_or_group_option(canvas, args)
  submissions = filter_submissions(submissions, user_list)

output = csv.writer(sys.stdout, delimiter=args.delimiter)

for submission in submissions:
  if args.login_id:
    output.writerow(format_submission_short_unique(submission))
  else:
    output.writerow(format_submission_short(submission))
@

Now, we must add that option [[args.login_id]].
<<add options for printing>>=
submissions_parser.add_argument("-l", "--login-id",
  help="Print login ID instead of name.",
  default=False, action="store_true")
@


\section{Computing the SpeedGrader URL}

Sometimes we want to compute the URL for SpeedGrader for a submission.
The [[Submission]] objects come with an attribute [[preview_url]].
We want to turn that one into the SpeedGrader URL.
The [[preview_url]] looks like this:
\begin{minted}{text}
https://canvas.kth.se/courses/26930/assignments/173825/submissions/108849?
  preview=1&version=5
\end{minted}
It should be turned into this:
\begin{minted}{text}
https://canvas.kth.se/courses/26930/gradebook/speed_grader?
  assignment_id=173825&student_id=108849
\end{minted}
We use Python's regex abilities to rewrite the URL.
<<functions>>=
def speedgrader(submission):
  """Returns the SpeedGrader URL of the submission"""
  speedgrader_url = submission.preview_url

  speedgrader_url = re.sub("assignments/",
    "gradebook/speed_grader?assignment_id=",
    speedgrader_url)

  speedgrader_url = re.sub("/submissions/",
    "&student_id=",
    speedgrader_url)

  speedgrader_url = re.sub(r"\?preview.*$", "", speedgrader_url)

  return speedgrader_url
@


\section{The [[submission]] subcommand and its options}

Here we provide a subcommand [[submission]] which deals with an individual 
submission.
<<submission command description>>=
Prints data about matching submissions, including submission and grading time, 
and any text-based attachments.

<<add submission command to subp>>=
submission_parser = subp.add_parser("submission",
  help="Prints information about a submission",
  description="""
<<submission command description>>
""")
submission_parser.set_defaults(func=submission_command)
add_submission_options(submission_parser)
<<add options for printing a submission>>
<<add submission command options>>
@ We also need the corresponding function.
For now, we only print the most relevant data of a submission.
<<functions>>=
def submission_command(config, canvas, args):
  submission_list = process_submission_options(canvas, args)
  <<get and print the submission data>>
@

Then we can fetch the submission, format it to markdown ([[format_submission]]) 
and then print it.
We use the [[rich]] package to print it.
This prints the markdown output of [[format_submission]] nicer in the terminal.
It also adds syntax highlighting for the source code attachments.
However, we need to adapt the use of styles to the pager to be used.
If stdout is not a terminal, we don't use [[rich]], then we simply print the 
raw markdown.

However, we might want to write all data to the output directory.
In that case, we don't print anything to stdout.

Finally, if we use a pager, we'll first open the directory with the files, so 
that the user can explore the files while reading the output (containing the 
grading and comments).
That way, the user can try to run the Python code too, if necessary.
<<get and print the submission data>>=
console = rich.console.Console()
<<create [[tmpdir]]>>
for submission in submission_list:
  <<let [[subdir]] be the subdir for [[submission]]'s files>>
  output = format_submission(submission,
                             history=args.history,
                             tmpdir=tmpdir/subdir)

  if args.output_dir:
    <<write [[output]] to file in [[tmpdir/subdir]]>>
  elif sys.stdout.isatty():
    <<check if we should use styles>>
    <<open [[tmpdir/subdir]] for the user>>
    with console.pager(styles=styles):
      console.print(rich.markdown.Markdown(output,
                                           code_theme="manni"))
  else:
    print(output)
@ Note that we use the theme [[manni]] for the code, as this works well in both 
dark and light terminals.

\subsection{Optional history}

Now, let's turn to that [[args.history]] argument.
We want to exclude it sometimes, for instance, when we want to get to the 
comments only.
So we default to off, since it's only occasionally that we want to see the 
history.
<<add options for printing a submission>>=
submission_parser.add_argument("-H", "--history", action="store_true",
  help="Include submission history.")
@

\subsection{Specifying an output directory}

Next we have the [[args.output_dir]] argument.
If we specify the [[output_dir]] we want to write the output to files and not 
have it printed to stdout.
<<add options for printing a submission>>=
submission_parser.add_argument("-o", "--output-dir",
  required=False, default=None,
  help="Write output to files in directory the given directory.")
@

If the user specified an output directory, we will not create a temporary 
directory, but rather let [[tmpdir]] be the output directory.
<<create [[tmpdir]]>>=
if args.output_dir:
  tmpdir = pathlib.Path(args.output_dir)
else:
  tmpdir = pathlib.Path(tempfile.mkdtemp())
@

Finally, we can then write the output to a file in the output directory.
We need to structure the files in some way.
We have a list of submissions for various students.
The submissions can be submissions for various assignments (in assignment 
groups) in various courses.
The two most interesting options are:
\begin{enumerate}
\item to group by student, that is the student is the top level directory, 
[[student/course/assignment]]; or
\item to group by course, that is we get [[course/assignment/student]].
\end{enumerate}
This affects both
[[<<write [[output]] to file in [[args.output_dir]]>>]]
and
[[<<[[tmpdir]] for [[format_submission]]>>]].

We'll introduce an option that lets us choose between these two options.
<<add submission command options>>=
submission_parser.add_argument("--sort-order", required=False,
  choices=["student", "course"], default="student",
  help="Determines the order in which directories are created "
       "in `output_dir`. `student` results in `student/course/assignment` "
       "and `course` results in `course/assignment/student`. "
       "Default: %(default)s")
<<let [[subdir]] be the subdir for [[submission]]'s files>>=
if args.sort_order == "student":
  subdir = f"{submission.user.login_id}" \
           f"/{submission.assignment.course.course_code}" \
           f"/{submission.assignment.name}"
else:
  subdir = f"{submission.assignment.course.course_code}" \
           f"/{submission.assignment.name}" \
           f"/{submission.user.login_id}"

(tmpdir / subdir).mkdir(parents=True, exist_ok=True)
<<write [[output]] to file in [[tmpdir/subdir]]>>=
with open(tmpdir/subdir/"metadata.md", "w") as f:
  f.write(output)
@

\subsection{Opening the directory with the files}

We want to open the directory with the files if we use a pager.
However, we provide this as an option.
<<add submission command options>>=
submission_parser.add_argument("--open", required=False, default=False,
  action="store_true",
  help="Open the directory with the files in the default file manager.")
<<open [[tmpdir/subdir]] for the user>>=
if args.open:
  subprocess.run(["open", tmpdir/subdir])
@


\subsection{Check if we should use styles}

By default, [[rich.console.Console]] uses the [[pydoc.pager]], which uses the 
system pager (as determined by environment variables etc.).
The default usually can't handle colours, so [[rich]] doesn't use colours when 
paging.
We want to check if [[less -r]] or [[less -R]] is set as the pager, in that 
case we can use styles.
<<check if we should use styles>>=
pager = ""
if "MANPAGER" in os.environ:
  pager = os.environ["MANPAGER"]
elif "PAGER" in os.environ:
  pager = os.environ["PAGER"]

styles = False
if "less" in pager and ("-R" in pager or "-r" in pager):
  styles = True
<<submission command description>>=
Uses MANPAGER or PAGER environment variables for the pager to page output. If 
the `-r` or `-R` flag is passed to `less`, it uses colours in the output.

@


\section{Selecting a submission on the command line}%
\label{submission-options}

We now provide a function to set up the command-line options to select a 
particular submission along with a function to process those options.
For this we need
\begin{itemize}
\item an assignment,
\item a user or group,
\item to know if we aim for all or just ungraded submissions.
\end{itemize}
We add the [[required]] parameter to specify if we want to have required 
arguments, \eg for the [[grade]] command.
<<functions>>=
def add_submission_options(parser, required=False):
  try:
    assignments.add_assignment_option(parser, required=required)
  except argparse.ArgumentError:
    pass

  try:
    users.add_user_or_group_option(parser, required=required)
  except argparse.ArgumentError:
    pass

  submissions_parser = parser.add_argument_group("filter submissions")
  try: # to protect from this option already existing in add_assignment_option
    submissions_parser.add_argument("-U", "--ungraded", action="store_true",
      help="Only ungraded submissions.")
  except argparse.ArgumentError:
    pass

def process_submission_options(canvas, args):
  assignment_list = assignments.process_assignment_option(canvas, args)
  user_list = users.process_user_or_group_option(canvas, args)

  if args.ungraded:
    submissions = list_ungraded_submissions(assignment_list,
      include=["submission_history", "submission_comments", 
      "rubric_assessment"])
  else:
    submissions = list_submissions(assignment_list,
      include=["submission_history", "submission_comments", 
      "rubric_assessment"])

  return list(filter_submissions(submissions, user_list))
@


\section{Producing a list of submissions}%
\label{list-submissions-function}

We provide the following functions:
\begin{itemize}
  \item [[list_submissions]], which returns all submissions;
  \item [[list_ungraded_submissions]], which returns all ungraded submissions.
\end{itemize}
We return the submissions for a list of assignments, since we can match several 
assignments with a regular expression (using [[filter_assignments]]).
<<functions>>=
def list_submissions(assignments, include=["submission_comments"]):
  for assignment in assignments:
    submissions = assignment.get_submissions(include=include)
    for submission in submissions:
      submission.assignment = assignment
      yield submission

def list_ungraded_submissions(assignments, include=["submisson_comments"]):
  for assignment in assignments:
    submissions = assignment.get_submissions(bucket="ungraded",
      include=include)
    for submission in submissions:
      if submission.submitted_at and (submission.graded_at is None or
          not submission.grade_matches_current_submission):
        submission.assignment = assignment
        yield submission
@


\section{Filtering a list of submissions}

We provide the function [[filter_submissions]] which filters out a subset of 
submissions from a list.

For each submission we check if it's user is in the desired user set.
(Note that since the list~[[user_list]], might contain duplicates, so we turn 
it into a set first.)
Once we've found the user, we don't need to search further, so we can break 
after the yield.
<<functions>>=
def filter_submissions(submission_list, user_list):
  user_list = set(user_list)

  for submission in submission_list:
    for user in user_list:
      if submission.user_id == user.id:
        submission.user = user
        yield submission
        break
@


\section{Printing a submission}

We provide two functions to print a submission.
One to print a short summary (row in CSV data) and one to print the submission 
data for rendering.
The first is to get an overview of all submissions, the latter to look into the 
details of only one submission.

We'll format the submission in short format.
The most useful data is the identifier, the grade and the date of grading.
<<functions>>=
def format_submission_short(submission):
  return [
    submission.assignment.course.course_code,
    submission.assignment.name,
    submission.user.name,
    submission.grade, submission.submitted_at, submission.graded_at
  ]
@

Sometimes we want the short format to contain a unique identifier (such as 
[[login_id]]) instead of the name.
<<functions>>=
def format_submission_short_unique(submission):
  <<get uid from submission's user object>>

  return [
    submission.assignment.course.course_code,
    submission.assignment.name,
    uid,
    submission.grade, submission.submitted_at, submission.graded_at
  ]
@

However, we note that sometimes the student doesn't have a [[login_id]] 
attribute, so we can use their [[integration_id]] or [[sis_user_id]] instead 
for uniqueness.
See \cref{UserUniqueID} for details.
<<get uid from submission's user object>>=
uid = users.get_uid(submission.user)
@

We provide the function [[format_submission]] to nicely format a submission.
It prints metadata, downloads any text attachments to include in the output.
We also output all the submission history at the end.
We no longer need the [[canvas]] object to resolve course, assignment and user 
IDs, instead, we add these as attributes when fetching the objects.
So [[submission.assignment]] is the assignment it came from, we don't need to 
resolve the assignment from the assignmend ID.
<<functions>>=
def format_submission(submission, history=False,
                      <<[[format_submission]] args>>):
  """
  Formats submission for printing to stdout. Returns a string.

  If history is True, also include submission history.
  """
  student = submission.assignment.course.get_user(submission.user_id)

  formatted_submission = ""

  <<add metadata to output>>
  <<add rubric assessment to output>>
  <<add comments to output>>
  <<add body to output>>
  <<add quiz answers to output>>
  <<add attachments to output>>
  if history:
    <<add submission history to output>>

  return formatted_submission
@

\subsection{Some helper functions}

We want to format some sections.
<<functions>>=
def format_section(title, body):
  return f"\n# {title}\n\n{body}\n\n"
@

\subsection{Metadata}

To format the metadata section, we simply pass the right strings to the section 
formatting function.
<<add metadata to output>>=
formatted_submission += format_section(
  "Metadata",
  f"{submission.assignment.course.course_code} > {submission.assignment.name}"
  f"\n\n"
  f" - Student: {student.name} "
    f"({student.login_id or None}, {submission.user_id})\n"
  f" - Submission ID: {submission.id}\n"
  f" - Submitted (graded): {submission.submitted_at} "
    f"({submission.graded_at})\n"
  f" - Grade: {submission.grade} ({submission.score})\n"
  f" - Graded by: {resolve_grader(submission)}\n"
  f" - SpeedGrader: {speedgrader(submission)}")
@

Now to resolve the grader, we need to look up a user ID.
Fortunately, we can do that through the course that is included as part of the 
assignment, as part of the submission.
(We add this manually in [[list_submissions]].)
The grader ID is negative if it was graded automatically, \eg by a quiz or LTI 
integration.
If negative, it's either the quiz ID or LTI tool ID.
(Negate to get the ID.)
<<functions>>=
def resolve_grader(submission):
  """
  Returns a user object if the submission was graded by a human.
  Otherwise returns None if ungraded or a descriptive string.
  """
  try:
    if submission.grader_id is None:
      return None
  except AttributeError:
    return None
    
  if submission.grader_id < 0:
    return "autograded"
  return submission.assignment.course.get_user(submission.grader_id)
@

\subsection{Rubric data}

<<add rubric assessment to output>>=
try:
  if submission.rubric_assessment:
    formatted_submission += format_section(
      "Rubric assessment",
      format_rubric(submission))
except AttributeError:
  pass
@

\subsection{General comments}

<<add comments to output>>=
try:
  if submission.submission_comments:
    body = ""
    for comment in submission.submission_comments:
      body += f"{comment['author_name']} ({comment['created_at']}):\n\n"
      body += comment["comment"] + "\n\n"
    formatted_submission += format_section("Comments", body)
except AttributeError:
  pass
@

\subsection{Body}

<<add body to output>>=
try:
  if submission.body:
    formatted_submission += format_section("Body", submission.body)
except AttributeError:
  pass
@

\subsection{Quiz answers}

<<add quiz answers to output>>=
try:
  if submission.submission_data:
    formatted_submission += format_section(
      "Quiz answers",
      json.dumps(submission.submission_data, indent=2))
except AttributeError:
  pass
@

\subsection{Attachments}

For the attachment, we want to add it to the output.
In the case of Python code we want to have it as a Markdown code block.
If it's a text file, we want to have it as part of the plain Markdown.
We will try to convert the attachment to Markdown using [[pypandoc]].
<<add attachments to output>>=
try:
  <<attachment iteration variables>>
  for attachment in submission.attachments:
    <<let [[contents]] be the converted [[attachment]]>>
    formatted_submission += format_section(attachment.filename,
                                            contents)
except AttributeError:
  pass
@

Let's look at the conversion.
If it's a text-based format, we want to include it as a Markdown code block.
Otherwise, we'll try to convert it to Markdown using [[pypandoc]].
If the latter fails, we want to add a pointer to the file in the output, so 
that the user can open it in an external viewer.

In fact, having a copy of all the files locally is useful.
We need to download it anyways, so we might just as well put a copy in a local 
temporary directory too.
We'll let the caller specify the directory to use, so we can use the same 
directory for all attachments and potentially all users.
<<[[format_submission]] args>>=
tmpdir = None,
<<attachment iteration variables>>=
tmpdir = pathlib.Path(tmpdir or tempfile.mkdtemp())
@

We'll use [[tmpdir]] as the temporary directory to store the files when we try 
to convert them.
<<functions>>=
def convert_to_md(attachment: canvasapi.file.File,
                  tmpdir: pathlib.Path) -> str:
  """
  Converts `attachment` to Markdown. Returns the Markdown string.

  Store a file version in `tmpdir`.
  """
  <<download [[attachment]] to [[outfile]] in [[tmpdir]]>>
  content_type = getattr(attachment, "content-type")
  <<if [[content_type]] is text, just use [[outfile]] contents>>
  <<else convert [[outfile]] using [[pypandoc]]>>
<<let [[contents]] be the converted [[attachment]]>>=
contents = convert_to_md(attachment, tmpdir)
@

The download is simple.
<<download [[attachment]] to [[outfile]] in [[tmpdir]]>>=
outfile = tmpdir / attachment.filename
attachment.download(outfile)
@

If the content type is text, we can just decode it and use it in a Markdown 
code block with a suitable (Markdown) content type.
This means that we can set what type of data the code block contains.
We compute this text from the content type of the attachment.
For instance, Python source code is [[text/x-python]].
We remove the [[text/]] prefix and check if there is any [[x-]] prefix left, in 
which case we remove that as well.

Now, we want to change the content type to the format expected by Markdown to 
do proper syntax highlighting.
This requires some ugly processing since one [[.py]] file might have content 
type [[text/x-python]] and another [[.py]] file might have 
[[text/python-script]].
<<functions>>=
def text_to_md(content_type):
  """
  Takes a text-based content type, returns Markdown code block type.
  Raises ValueError if not possible.
  """
  if content_type.startswith("text/"):
    content_type = content_type[len("text/"):]
  else:
    raise ValueError(f"Not text-based content type: {content_type}")

  if content_type.startswith("x-"):
    content_type = content_type[2:]
  if content_type == "python-script":
    content_type = "python"

  return content_type
@

This leaves us with the following.
The advantage of reading the content from the file is that Python will solve 
the encoding for us.
<<if [[content_type]] is text, just use [[outfile]] contents>>=
try:
  md_type = text_to_md(content_type)
  with open(outfile, "r") as f:
    contents = f.read()
  return f"```{md_type}\n{contents}\n```"
except ValueError:
  pass
@

If the content type is not text, we use [[pypandoc]] to convert it to Markdown.
Here we'll use Pandoc's ability to infer the file type on its own.
This means we'll have to download the attachment as a file in a temporary 
location and let Pandoc convert the file to Markdown.
<<else convert [[outfile]] using [[pypandoc]]>>=
try:
  return pypandoc.convert_file(outfile, "markdown")
except Exception as err:
  return f"Pandoc cannot convert this file. " \
         f"The file is located at\n\n  {outfile}\n\n"
@


\subsection{Submission history}

<<add submission history to output>>=
try:
  if submission.submission_history:
    for prev_submission in submission.submission_history:
      prev_submission = canvasapi.submission.Submission(
        submission._requester, prev_submission)
      prev_submission.assignment = submission.assignment

      formatted_submission += "\n\n" + format_submission(prev_submission)
except AttributeError:
  pass
@


\section{Formatting rubrics}

For assignments that use rubrics, we want to format those rubrics so that we 
can read the results instead of just the cumulated grade.
<<functions>>=
def format_rubric(submission):
  """Format the rubric assessment of the `submission` in readable form."""

  result = ""

  for crit_id, rating_data in submission.rubric_assessment.items():
    criterion = get_criterion(crit_id, submission.assignment.rubric)
    rating = get_rating(rating_data["rating_id"], criterion)

    <<add [[criterion]] description and [[rating]] to [[result]]>>

    if rating_data["comments"]:
      result += f"Comments: {rating_data['comments']}\n"

    result += "\n"

  return result.strip()
@

Sometimes Canvas is missing some data,
for instance, an individual rating.
So we add it only if it exists.
<<add [[criterion]] description and [[rating]] to [[result]]>>=
result += f"{criterion['description']}: "
if rating:
  result += f"{rating['description']} ({rating['points']})"
else:
  result += "-"
result += "\n"
@

We can get the rating of a rubric from the rubric assessment.
We can get this data from [[submission.rubric_assessment]] and it looks like 
this:
\begin{minted}{python}
{'_7957': {'rating_id': '_6397', 'comments': '', 'points': 1.0},
 '_1100': {'rating_id': '_8950', 'comments': '', 'points': 1.0}}
\end{minted}

We get the rubric with the assignment.
So we can get it through [[submission.assignment.rubric]] and it looks like 
this:
\begin{minted}{python}
[{'id': '_7957', 'points': 1.0, 'description': 'Uppfyller kraven i lydelsen', 
'long_description': '', 'criterion_use_range': False, 'ratings': [{'id': 
'_6397', 'points': 1.0, 'description': 'OK', 'long_description': ''}, {'id': 
'_7836', 'points': 0.0, 'description': 'Påpekande', 'long_description': ''}]},
{'id': '_1100', 'points': 1.0, 'description': 'Kan redogöra för alla detaljer', 
'long_description': '', 'criterion_use_range': False, 'ratings': [{'id': 
'_8950', 'points': 1.0, 'description': 'OK', 'long_description': ''}, {'id': 
'_4428', 'points': 0.0, 'description': 'Påpekande', 'long_description': ''}]}]
\end{minted}
It's essentially a list of criterions.
We want to extract a criterion by ID from the rubric.
<<functions>>=
def get_criterion(criterion_id, rubric):
  """Returns criterion with ID `criterion_id` from rubric `rubric`"""
  for criterion in rubric:
    if criterion["id"] == criterion_id:
      return criterion

  return None
@ And in exactly the same fashion we want to extract a rating from the 
criterion.
<<functions>>=
def get_rating(rating_id, criterion):
  """Returns rating with ID `rating_id` from rubric criterion `criterion`"""
  for rating in criterion["ratings"]:
    if rating["id"] == rating_id:
      return rating

  return None
@

